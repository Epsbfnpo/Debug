import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import numpy as np
import random
import time
import logging
import datetime
import sys
from tqdm import tqdm

import algorithms
from utils.args import get_args, setup_cfg
from utils.misc import init_log, LossCounter, get_scheduler, update_writer
from utils.validate import algorithm_validate
from dataset.data_manager import get_dataset


def debug_log(msg, rank):
    timestamp = datetime.datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}][Rank {rank}] {msg}", flush=True)


def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True


def save_checkpoint(path, algorithm, optimizer, scheduler, epoch, best_performance):
    if hasattr(algorithm.network, 'module'):
        model_state = algorithm.network.module.state_dict()
    else:
        model_state = algorithm.network.state_dict()

    state = {
        'epoch': epoch,
        'model_state': model_state,
        'optimizer_state': optimizer.state_dict(),
        'scheduler_state': scheduler.state_dict(),
        'best_performance': best_performance
    }
    torch.save(state, path)


def load_checkpoint(path, algorithm, optimizer, scheduler):
    print(f"Loading checkpoint from {path}...")
    checkpoint = torch.load(path, map_location='cpu')

    if hasattr(algorithm.network, 'module'):
        algorithm.network.module.load_state_dict(checkpoint['model_state'])
    else:
        algorithm.network.load_state_dict(checkpoint['model_state'])

    optimizer.load_state_dict(checkpoint['optimizer_state'])
    scheduler.load_state_dict(checkpoint['scheduler_state'])

    start_epoch = checkpoint['epoch'] + 1
    best_performance = checkpoint.get('best_performance', 0.0)

    return start_epoch, best_performance


def main():
    start_time = time.time()
    args = get_args()

    # --- 1. DDP åˆå§‹åŒ– ---
    if 'LOCAL_RANK' in os.environ:
        args.local_rank = int(os.environ['LOCAL_RANK'])

    if args.local_rank != -1:
        torch.cuda.set_device(args.local_rank)
        dist.init_process_group(backend='nccl')
        args.device = torch.device('cuda', args.local_rank)
        is_distributed = True
    else:
        args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        is_distributed = False

    # --- 2. é…ç½®ä¸è·¯å¾„ ---
    cfg = setup_cfg(args)
    root_output = os.path.abspath(cfg.OUT_DIR)
    log_path = os.path.join(root_output, cfg.OUTPUT_PATH)

    if not os.path.exists(log_path):
        os.makedirs(log_path, exist_ok=True)

    # --- 3. æ—¥å¿—åˆå§‹åŒ– ---
    writer = None
    if args.local_rank in [-1, 0]:
        writer = init_log(args, cfg, log_path, 0, [0, 0, 0])
        logging.info(f"Distributed: {is_distributed}, Rank: {args.local_rank}")
        print(f"[INFO] Log Path: {log_path}")

    set_seed(cfg.SEED)

    latest_ckpt_path = os.path.join(log_path, 'latest_model.pth')
    final_ckpt_path = os.path.join(log_path, 'final_model.pth')

    if os.path.exists(final_ckpt_path):
        if args.local_rank in [-1, 0]:
            print(f"âœ… Found {final_ckpt_path}. Training already completed.")
        if is_distributed:
            dist.barrier()
            dist.destroy_process_group()
        sys.exit(0)

    # --- 4. æ•°æ®ä¸æ¨¡å‹ ---
    debug_log("Loading datasets...", args.local_rank)
    # dataset_size åªæœ‰ rank 0 ç”¨æ¥æ‰“å°ï¼Œå…¶ä»–æ— æ‰€è°“
    train_loader, val_loader, test_loader, dataset_size, train_sampler = get_dataset(args, cfg)

    algorithm_class = algorithms.get_algorithm_class(cfg.ALGORITHM)
    algorithm = algorithm_class(cfg.DATASET.NUM_CLASSES, cfg)
    algorithm.network = algorithm.network.to(args.device)

    if hasattr(algorithm, 'classifier'):
        algorithm.classifier = algorithm.classifier.to(args.device)
    if hasattr(algorithm, 'swad_algorithm'):
        algorithm.swad_algorithm = algorithm.swad_algorithm.to(args.device)

    if is_distributed:
        algorithm.network = nn.SyncBatchNorm.convert_sync_batchnorm(algorithm.network)
        algorithm.network = DDP(algorithm.network, device_ids=[args.local_rank], output_device=args.local_rank)

    optimizer = algorithm.optimizer
    scheduler = get_scheduler(optimizer, cfg.EPOCHS)

    start_epoch = 1
    best_performance = 0.0

    # --- [Resume Check] ---
    if os.path.exists(latest_ckpt_path):
        debug_log(f"Found {latest_ckpt_path}. Resuming training...", args.local_rank)
        try:
            start_epoch, best_performance = load_checkpoint(latest_ckpt_path, algorithm, optimizer, scheduler)
            debug_log(f"Resumed from Epoch {start_epoch}.", args.local_rank)
        except Exception as e:
            debug_log(f"Error loading checkpoint: {e}. Starting from scratch.", args.local_rank)

    # --- 5. è®­ç»ƒå¾ªç¯ ---
    iterator = tqdm(range(start_epoch - 1, cfg.EPOCHS), disable=(args.local_rank not in [-1, 0]),
                    initial=start_epoch - 1, total=cfg.EPOCHS)

    for i in iterator:
        epoch = i + 1

        if is_distributed and train_sampler is not None:
            train_sampler.set_epoch(i)

        loss_avg = LossCounter()
        algorithm.train()

        # Train Loop
        for image, mask, label, domain, img_index in train_loader:
            image = image.to(args.device)
            mask = mask.to(args.device)
            label = label.to(args.device).long()
            domain = domain.to(args.device).long()

            minibatch = [image, mask, label, domain]
            loss_dict_iter = algorithm.update(minibatch)
            loss_avg.update(loss_dict_iter['loss'])

        if hasattr(algorithm, 'update_epoch'):
            algorithm.update_epoch(epoch)

        if args.local_rank in [-1, 0]:
            update_writer(writer, epoch, scheduler, loss_avg)

        scheduler.step()

        # Save Latest
        if args.local_rank in [-1, 0]:
            save_checkpoint(latest_ckpt_path, algorithm, optimizer, scheduler, epoch, best_performance)

        # =========================================================
        # [ä¿®æ”¹ç‚¹] éªŒè¯é˜¶æ®µï¼š4å¡å…¨å¼€
        # =========================================================
        if epoch % cfg.VAL_EPOCH == 0:
            if args.local_rank in [-1, 0]:
                logging.info(f"Epoch {epoch} Validation...")

            # [å…³é”®] ç§»é™¤ if rank==0 çš„é™åˆ¶ï¼Œæ‰€æœ‰ rank éƒ½å¿…é¡»è¿›å…¥ validate
            # å†…éƒ¨ä¼šè‡ªåŠ¨ gather æ•°æ®ï¼Œåªæœ‰ rank 0 ä¼šè¿”å›æœ‰æ•ˆçš„ val_auc
            val_auc, _ = algorithm_validate(algorithm, val_loader, writer, epoch, 'val')

            # åªæœ‰ Rank 0 è´Ÿè´£ä¿å­˜æœ€ä½³æ¨¡å‹
            if args.local_rank in [-1, 0]:
                if val_auc > best_performance:
                    best_performance = val_auc
                    logging.info(f"â­ï¸ New Best Model! Val AUC: {val_auc:.4f}")
                    algorithm.save_model(log_path)

            # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥ï¼Œé˜²æ­¢ Rank 0 åœ¨ä¿å­˜æ¨¡å‹æ—¶å…¶ä»–è¿›ç¨‹è·‘å¤ªå¿«
            if is_distributed:
                dist.barrier()

        # Time Limit Check
        if args.time_limit > 0:
            elapsed_time = time.time() - start_time
            if elapsed_time > (args.time_limit - 300):
                if args.local_rank in [-1, 0]:
                    save_checkpoint(latest_ckpt_path, algorithm, optimizer, scheduler, epoch, best_performance)
                    if writer: writer.close()
                if is_distributed:
                    dist.barrier()
                    dist.destroy_process_group()
                sys.exit(0)

    # --- 6. æœ€ç»ˆæµ‹è¯• ---
    debug_log("Training Finished. Starting Final Testing...", args.local_rank)

    # ç­‰å¾… Rank 0 ä¿å­˜å®Œ final_model
    if args.local_rank in [-1, 0]:
        save_checkpoint(final_ckpt_path, algorithm, optimizer, scheduler, cfg.EPOCHS, best_performance)

    if is_distributed:
        dist.barrier()

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    # æ³¨æ„ï¼šæ‰€æœ‰ GPU éƒ½è¦åŠ è½½æœ€ä½³æ¨¡å‹ï¼
    try:
        algorithm.renew_model(log_path)
        if args.local_rank == 0:
            logging.info("Loaded Best Model for Testing.")
    except Exception as e:
        if args.local_rank == 0:
            logging.warning(f"Could not load best model ({e}), using current model.")

    # [å…³é”®] æµ‹è¯•é˜¶æ®µï¼š4å¡å…¨å¼€
    test_auc, _ = algorithm_validate(algorithm, test_loader, writer, cfg.EPOCHS, 'test')

    if args.local_rank in [-1, 0]:
        logging.info(f"ğŸš€ Final Test AUC: {test_auc:.4f}")
        with open(os.path.join(log_path, 'done'), 'w') as f:
            f.write(f'done, best_val={best_performance:.4f}, test={test_auc:.4f}')
        if writer: writer.close()

    if is_distributed:
        dist.barrier()
        dist.destroy_process_group()


if __name__ == "__main__":
    main()