Job 20964045 started at Sun 01 Mar 2026 05:39:31 PM AEDT
========================================================
üöÄ ÂêØÂä® ESDG ÊâπÈáèÂÆûÈ™å (Bash Âæ™ÁéØÊ®°Âºè)
GPU Êï∞Èáè: 4
ÂæÖËøêË°åÊ∫êÂüü: APTOS DEEPDR FGADR IDRID MESSIDOR RLDR
Âü∫Á°ÄËæìÂá∫ÁõÆÂΩï: ./output_esdg_h100
========================================================

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: APTOS
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
[17:44:41][Rank 1] Loading datasets...[17:44:41][Rank 3] Loading datasets...[17:44:41][Rank 2] Loading datasets...


0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='APTOS', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['APTOS']
  TARGET_DOMAINS: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_APTOS
OUT_DIR: ./output_esdg_h100/APTOS
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified4/output_esdg_h100/APTOS/CASS_GDRNet_ESDG_APTOS
[17:44:41][Rank 0] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.üßä [DINOv3] All base parameters frozen.

üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_projInjecting LoRA into: layer.0.attention.k_proj

Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_projInjecting LoRA into: layer.1.mlp.down_proj

Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_projInjecting LoRA into: layer.5.attention.q_proj

Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_projInjecting LoRA into: layer.5.mlp.up_proj

Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_projInjecting LoRA into: layer.5.mlp.up_proj

Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_projInjecting LoRA into: layer.6.attention.v_proj

Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.q_projInjecting LoRA into: layer.7.mlp.down_proj

Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.10.attention.k_projInjecting LoRA into: layer.10.attention.k_proj

Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 4.315649654554284
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 0.7336, Acc: 0.7555, AUC: 0.8688, F1: 0.4158
[VIT] val - Epoch: 1, Loss: 1.1201, Acc: 0.5943, AUC: 0.7937, F1: 0.2349
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8688
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7937
Saving vit model...
epoch: 2, total loss: 4.329206741374472
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 0.7664, Acc: 0.7199, AUC: 0.8269, F1: 0.5173
[VIT] val - Epoch: 2, Loss: 0.9841, Acc: 0.7008, AUC: 0.8232, F1: 0.3037
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8232
Saving vit model...
epoch: 3, total loss: 4.602761014648106
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.7561, Acc: 0.7104, AUC: 0.8873, F1: 0.5323
[VIT] val - Epoch: 3, Loss: 0.9039, Acc: 0.7063, AUC: 0.8319, F1: 0.3929
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8873
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8319
Saving vit model...
epoch: 4, total loss: 4.049197072568147
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.5307, Acc: 0.8279, AUC: 0.9096, F1: 0.6566
[VIT] val - Epoch: 4, Loss: 0.8258, Acc: 0.7268, AUC: 0.8477, F1: 0.3487
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9096
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8477
Saving vit model...
epoch: 5, total loss: 3.8599054139593374
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.5375, Acc: 0.7992, AUC: 0.9010, F1: 0.6280
[VIT] val - Epoch: 5, Loss: 0.7977, Acc: 0.7568, AUC: 0.8509, F1: 0.5038
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8509
Saving vit model...
epoch: 6, total loss: 3.769556102545365
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.5540, Acc: 0.8210, AUC: 0.8942, F1: 0.6706
[VIT] val - Epoch: 6, Loss: 0.7774, Acc: 0.7787, AUC: 0.8640, F1: 0.5566
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8640
Saving vit model...
epoch: 7, total loss: 3.734654296999392
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.5036, Acc: 0.8101, AUC: 0.9135, F1: 0.6238
[VIT] val - Epoch: 7, Loss: 0.7273, Acc: 0.7691, AUC: 0.8652, F1: 0.4685
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9135
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8652
Saving vit model...
epoch: 8, total loss: 3.5795300732488218
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.6067, Acc: 0.7828, AUC: 0.8803, F1: 0.5680
[VIT] val - Epoch: 8, Loss: 0.7188, Acc: 0.7527, AUC: 0.8684, F1: 0.5109
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8684
Saving vit model...
epoch: 9, total loss: 3.71192569318025
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.6435, Acc: 0.7486, AUC: 0.8682, F1: 0.5847
[VIT] val - Epoch: 9, Loss: 0.7011, Acc: 0.7555, AUC: 0.8703, F1: 0.4970
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8703
Saving vit model...
epoch: 10, total loss: 3.677140360293181
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.5586, Acc: 0.7978, AUC: 0.8988, F1: 0.6508
[VIT] val - Epoch: 10, Loss: 0.6850, Acc: 0.7869, AUC: 0.8752, F1: 0.5616
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8752
Saving vit model...
epoch: 11, total loss: 3.5663105871366416
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.4653, Acc: 0.8511, AUC: 0.9198, F1: 0.6818
[VIT] val - Epoch: 11, Loss: 0.6643, Acc: 0.7951, AUC: 0.8765, F1: 0.5567
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9198
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8765
Saving vit model...
epoch: 12, total loss: 3.4647475714268894
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.5546, Acc: 0.7992, AUC: 0.9092, F1: 0.6736
[VIT] val - Epoch: 12, Loss: 0.6860, Acc: 0.7992, AUC: 0.8811, F1: 0.6145
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8811
Saving vit model...
epoch: 13, total loss: 3.1514043004616448
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.4539, Acc: 0.8593, AUC: 0.9183, F1: 0.7039
[VIT] val - Epoch: 13, Loss: 0.6646, Acc: 0.7883, AUC: 0.8825, F1: 0.5992
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8825
Saving vit model...
epoch: 14, total loss: 3.407152885976045
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.4774, Acc: 0.8238, AUC: 0.9238, F1: 0.6598
[VIT] val - Epoch: 14, Loss: 0.6467, Acc: 0.7869, AUC: 0.8798, F1: 0.6079
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9238
Saving cnn model...
epoch: 15, total loss: 3.4831575492153997
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.4906, Acc: 0.8388, AUC: 0.9099, F1: 0.6864
[VIT] val - Epoch: 15, Loss: 0.6301, Acc: 0.7896, AUC: 0.8803, F1: 0.5907
epoch: 16, total loss: 3.450974246729975
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.7070, Acc: 0.7377, AUC: 0.8805, F1: 0.5992
[VIT] val - Epoch: 16, Loss: 0.6067, Acc: 0.7964, AUC: 0.8790, F1: 0.5945
epoch: 17, total loss: 3.464529592057933
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.4546, Acc: 0.8538, AUC: 0.9176, F1: 0.7144
[VIT] val - Epoch: 17, Loss: 0.6379, Acc: 0.7801, AUC: 0.8846, F1: 0.6148
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8846
Saving vit model...
epoch: 18, total loss: 3.322412169497946
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.4769, Acc: 0.8374, AUC: 0.9210, F1: 0.7045
[VIT] val - Epoch: 18, Loss: 0.6111, Acc: 0.8060, AUC: 0.8863, F1: 0.5938
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8863
Saving vit model...
epoch: 19, total loss: 3.3449792913768603
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.5071, Acc: 0.8361, AUC: 0.9050, F1: 0.6864
[VIT] val - Epoch: 19, Loss: 0.6105, Acc: 0.7937, AUC: 0.8839, F1: 0.6258
epoch: 20, total loss: 3.2102362176646357
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.4496, Acc: 0.8415, AUC: 0.9220, F1: 0.7045
[VIT] val - Epoch: 20, Loss: 0.6050, Acc: 0.7978, AUC: 0.8846, F1: 0.6275
epoch: 21, total loss: 3.3303086420764094
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.4353, Acc: 0.8443, AUC: 0.9321, F1: 0.7185
[VIT] val - Epoch: 21, Loss: 0.5913, Acc: 0.8101, AUC: 0.8840, F1: 0.6447
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9321
Saving cnn model...
epoch: 22, total loss: 3.431018484675366
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.4841, Acc: 0.8210, AUC: 0.9202, F1: 0.6803
[VIT] val - Epoch: 22, Loss: 0.5938, Acc: 0.8060, AUC: 0.8887, F1: 0.5927
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8887
Saving vit model...
epoch: 23, total loss: 3.083104133605957
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.4658, Acc: 0.8388, AUC: 0.9303, F1: 0.7080
[VIT] val - Epoch: 23, Loss: 0.5990, Acc: 0.8046, AUC: 0.8895, F1: 0.6444
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8895
Saving vit model...
epoch: 24, total loss: 3.1593263848968176
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.5795, Acc: 0.8005, AUC: 0.8957, F1: 0.6340
[VIT] val - Epoch: 24, Loss: 0.5805, Acc: 0.8142, AUC: 0.8874, F1: 0.6465
epoch: 25, total loss: 3.172488977079806
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.4876, Acc: 0.8333, AUC: 0.9161, F1: 0.7080
[VIT] val - Epoch: 25, Loss: 0.6057, Acc: 0.7828, AUC: 0.8913, F1: 0.6169
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8913
Saving vit model...
epoch: 26, total loss: 3.4223183834034465
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.5662, Acc: 0.7937, AUC: 0.8906, F1: 0.6334
[VIT] val - Epoch: 26, Loss: 0.5986, Acc: 0.7855, AUC: 0.8908, F1: 0.6233
epoch: 27, total loss: 3.2543184394421787
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.5111, Acc: 0.8074, AUC: 0.9189, F1: 0.6606
[VIT] val - Epoch: 27, Loss: 0.5636, Acc: 0.8033, AUC: 0.8894, F1: 0.6047
epoch: 28, total loss: 3.2122723356537195
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.4663, Acc: 0.8251, AUC: 0.9238, F1: 0.6901
[VIT] val - Epoch: 28, Loss: 0.5648, Acc: 0.8142, AUC: 0.8907, F1: 0.6405
epoch: 29, total loss: 3.154385110606318
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.5169, Acc: 0.8115, AUC: 0.9237, F1: 0.6833
[VIT] val - Epoch: 29, Loss: 0.5867, Acc: 0.7801, AUC: 0.8898, F1: 0.5999
epoch: 30, total loss: 3.1968267352684685
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.5035, Acc: 0.8156, AUC: 0.9223, F1: 0.6868
[VIT] val - Epoch: 30, Loss: 0.5756, Acc: 0.7964, AUC: 0.8928, F1: 0.6208
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8928
Saving vit model...
epoch: 31, total loss: 3.1229641515275706
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.5211, Acc: 0.7951, AUC: 0.9145, F1: 0.6718
[VIT] val - Epoch: 31, Loss: 0.5567, Acc: 0.8156, AUC: 0.8923, F1: 0.6476
epoch: 32, total loss: 3.0169143365777056
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.5617, Acc: 0.8115, AUC: 0.9134, F1: 0.6912
[VIT] val - Epoch: 32, Loss: 0.5582, Acc: 0.8115, AUC: 0.8929, F1: 0.6446
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8929
Saving vit model...
epoch: 33, total loss: 3.1804104421449746
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.4521, Acc: 0.8402, AUC: 0.9280, F1: 0.7196
[VIT] val - Epoch: 33, Loss: 0.5599, Acc: 0.8115, AUC: 0.8941, F1: 0.6487
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8941
Saving vit model...
epoch: 34, total loss: 3.2713206799133965
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.4382, Acc: 0.8429, AUC: 0.9260, F1: 0.7146
[VIT] val - Epoch: 34, Loss: 0.5557, Acc: 0.8156, AUC: 0.8953, F1: 0.6542
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8953
Saving vit model...
epoch: 35, total loss: 3.1687397153481194
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.4577, Acc: 0.8402, AUC: 0.9234, F1: 0.7237
[VIT] val - Epoch: 35, Loss: 0.5549, Acc: 0.8128, AUC: 0.8948, F1: 0.6432
epoch: 36, total loss: 2.985602635404338
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.4531, Acc: 0.8306, AUC: 0.9158, F1: 0.6892
[VIT] val - Epoch: 36, Loss: 0.5501, Acc: 0.8128, AUC: 0.8937, F1: 0.6519
epoch: 37, total loss: 3.0408633921457375
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.4544, Acc: 0.8388, AUC: 0.9201, F1: 0.7041
[VIT] val - Epoch: 37, Loss: 0.5452, Acc: 0.8101, AUC: 0.8926, F1: 0.6334
epoch: 38, total loss: 3.0850987175236577
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.5316, Acc: 0.7814, AUC: 0.9222, F1: 0.6571
[VIT] val - Epoch: 38, Loss: 0.5550, Acc: 0.8128, AUC: 0.8972, F1: 0.6485
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8972
Saving vit model...
epoch: 39, total loss: 3.090715019599251
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.4500, Acc: 0.8361, AUC: 0.9226, F1: 0.6999
[VIT] val - Epoch: 39, Loss: 0.5415, Acc: 0.8197, AUC: 0.8968, F1: 0.6496
epoch: 40, total loss: 3.0555307346841563
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.4419, Acc: 0.8429, AUC: 0.9253, F1: 0.7171
[VIT] val - Epoch: 40, Loss: 0.5607, Acc: 0.8060, AUC: 0.8979, F1: 0.6512
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8979
Saving vit model...
epoch: 41, total loss: 2.9531802327736565
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.4491, Acc: 0.8497, AUC: 0.9233, F1: 0.7228
[VIT] val - Epoch: 41, Loss: 0.5418, Acc: 0.8210, AUC: 0.8951, F1: 0.6578
epoch: 42, total loss: 3.098567122998445
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.4846, Acc: 0.8224, AUC: 0.9171, F1: 0.7016
[VIT] val - Epoch: 42, Loss: 0.5426, Acc: 0.8197, AUC: 0.8978, F1: 0.6543
epoch: 43, total loss: 2.9419692236444224
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.4671, Acc: 0.8374, AUC: 0.9186, F1: 0.7262
[VIT] val - Epoch: 43, Loss: 0.5488, Acc: 0.8128, AUC: 0.8953, F1: 0.6598
epoch: 44, total loss: 2.9734479525814885
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.6319, Acc: 0.7486, AUC: 0.9135, F1: 0.6180
[VIT] val - Epoch: 44, Loss: 0.5644, Acc: 0.7910, AUC: 0.8966, F1: 0.6286
epoch: 45, total loss: 2.9188819087069966
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.4629, Acc: 0.8333, AUC: 0.9085, F1: 0.6892
[VIT] val - Epoch: 45, Loss: 0.5279, Acc: 0.8169, AUC: 0.8964, F1: 0.6265
epoch: 46, total loss: 2.842298036036284
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.4895, Acc: 0.8238, AUC: 0.9175, F1: 0.6986
[VIT] val - Epoch: 46, Loss: 0.5446, Acc: 0.8046, AUC: 0.8971, F1: 0.6376
epoch: 47, total loss: 2.7892079171927078
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.5066, Acc: 0.8128, AUC: 0.9089, F1: 0.6670
[VIT] val - Epoch: 47, Loss: 0.5414, Acc: 0.8142, AUC: 0.8972, F1: 0.6599
epoch: 48, total loss: 2.7665710060492805
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.5544, Acc: 0.7896, AUC: 0.9112, F1: 0.6582
[VIT] val - Epoch: 48, Loss: 0.5642, Acc: 0.7814, AUC: 0.8964, F1: 0.6103
epoch: 49, total loss: 2.748024038646532
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.4607, Acc: 0.8306, AUC: 0.9118, F1: 0.6576
[VIT] val - Epoch: 49, Loss: 0.5396, Acc: 0.8197, AUC: 0.8989, F1: 0.6570
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8989
Saving vit model...
epoch: 50, total loss: 3.0373302749965503
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.4304, Acc: 0.8402, AUC: 0.9238, F1: 0.7153
[VIT] val - Epoch: 50, Loss: 0.5271, Acc: 0.8210, AUC: 0.8982, F1: 0.6491
epoch: 51, total loss: 2.608618738858596
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.4429, Acc: 0.8456, AUC: 0.9236, F1: 0.7261
[VIT] val - Epoch: 51, Loss: 0.5291, Acc: 0.8156, AUC: 0.8983, F1: 0.6366
epoch: 52, total loss: 2.9795877829841944
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.4946, Acc: 0.8115, AUC: 0.9136, F1: 0.6910
[VIT] val - Epoch: 52, Loss: 0.5262, Acc: 0.8169, AUC: 0.8978, F1: 0.6546
epoch: 53, total loss: 2.7588153818379277
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.4923, Acc: 0.8306, AUC: 0.9109, F1: 0.7049
[VIT] val - Epoch: 53, Loss: 0.5286, Acc: 0.8210, AUC: 0.8977, F1: 0.6678
epoch: 54, total loss: 2.7405446068100305
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.5256, Acc: 0.8046, AUC: 0.9103, F1: 0.6686
[VIT] val - Epoch: 54, Loss: 0.5323, Acc: 0.8156, AUC: 0.8981, F1: 0.6620
epoch: 55, total loss: 3.066797225371651
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.5911, Acc: 0.7855, AUC: 0.8950, F1: 0.6465
[VIT] val - Epoch: 55, Loss: 0.5228, Acc: 0.8306, AUC: 0.8994, F1: 0.6655
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8994
Saving vit model...
epoch: 56, total loss: 2.867890479771987
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.5667, Acc: 0.7937, AUC: 0.9195, F1: 0.6570
[VIT] val - Epoch: 56, Loss: 0.5242, Acc: 0.8169, AUC: 0.8979, F1: 0.6582
epoch: 57, total loss: 3.085070424753687
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.5303, Acc: 0.8279, AUC: 0.9098, F1: 0.7099
[VIT] val - Epoch: 57, Loss: 0.5353, Acc: 0.8169, AUC: 0.8985, F1: 0.6733
epoch: 58, total loss: 2.8293947266495745
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.5458, Acc: 0.7964, AUC: 0.9177, F1: 0.6693
[VIT] val - Epoch: 58, Loss: 0.5341, Acc: 0.8087, AUC: 0.8993, F1: 0.6240
epoch: 59, total loss: 2.939054152239924
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.5061, Acc: 0.8238, AUC: 0.9156, F1: 0.6960
[VIT] val - Epoch: 59, Loss: 0.5248, Acc: 0.8156, AUC: 0.8989, F1: 0.6402
epoch: 60, total loss: 2.8529867659444395
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.4355, Acc: 0.8511, AUC: 0.9279, F1: 0.7395
[VIT] val - Epoch: 60, Loss: 0.5309, Acc: 0.8169, AUC: 0.8987, F1: 0.6665
epoch: 61, total loss: 2.677299196305482
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.4712, Acc: 0.8333, AUC: 0.9165, F1: 0.7016
[VIT] val - Epoch: 61, Loss: 0.5259, Acc: 0.8142, AUC: 0.8985, F1: 0.6571
epoch: 62, total loss: 2.6636218869167827
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.4555, Acc: 0.8470, AUC: 0.9190, F1: 0.7234
[VIT] val - Epoch: 62, Loss: 0.5374, Acc: 0.8115, AUC: 0.8997, F1: 0.6614
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8997
Saving vit model...
epoch: 63, total loss: 2.8457223176956177
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.4735, Acc: 0.8279, AUC: 0.9155, F1: 0.6997
[VIT] val - Epoch: 63, Loss: 0.5474, Acc: 0.8033, AUC: 0.8987, F1: 0.6580
epoch: 64, total loss: 2.8268111814623293
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.4992, Acc: 0.8238, AUC: 0.9162, F1: 0.6868
[VIT] val - Epoch: 64, Loss: 0.5367, Acc: 0.8005, AUC: 0.8989, F1: 0.6223
epoch: 65, total loss: 2.9155316508334614
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.4933, Acc: 0.8279, AUC: 0.9098, F1: 0.6985
[VIT] val - Epoch: 65, Loss: 0.5233, Acc: 0.8238, AUC: 0.8997, F1: 0.6689
epoch: 66, total loss: 2.9304925747539685
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.5289, Acc: 0.8074, AUC: 0.9116, F1: 0.6874
[VIT] val - Epoch: 66, Loss: 0.5210, Acc: 0.8169, AUC: 0.8973, F1: 0.6650
epoch: 67, total loss: 2.940233129522075
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.5318, Acc: 0.7992, AUC: 0.9157, F1: 0.6700
[VIT] val - Epoch: 67, Loss: 0.5191, Acc: 0.8156, AUC: 0.8979, F1: 0.6499
epoch: 68, total loss: 2.6230610945950383
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.4592, Acc: 0.8566, AUC: 0.9164, F1: 0.7428
[VIT] val - Epoch: 68, Loss: 0.5220, Acc: 0.8197, AUC: 0.8999, F1: 0.6602
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8999
Saving vit model...
epoch: 69, total loss: 2.610431461230568
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.5505, Acc: 0.7801, AUC: 0.9210, F1: 0.6495
[VIT] val - Epoch: 69, Loss: 0.5186, Acc: 0.8251, AUC: 0.8998, F1: 0.6702
epoch: 70, total loss: 2.778017855208853
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.5312, Acc: 0.7883, AUC: 0.9160, F1: 0.6564
[VIT] val - Epoch: 70, Loss: 0.5301, Acc: 0.8156, AUC: 0.9005, F1: 0.6595
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9005
Saving vit model...
epoch: 71, total loss: 2.8667202410490615
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.4413, Acc: 0.8525, AUC: 0.9166, F1: 0.7314
[VIT] val - Epoch: 71, Loss: 0.5180, Acc: 0.8224, AUC: 0.9005, F1: 0.6624
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9005
Saving vit model...
epoch: 72, total loss: 2.6126590941263284
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.5409, Acc: 0.7883, AUC: 0.9129, F1: 0.6663
[VIT] val - Epoch: 72, Loss: 0.5207, Acc: 0.8183, AUC: 0.9000, F1: 0.6621
epoch: 73, total loss: 2.596639099328414
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.4659, Acc: 0.8497, AUC: 0.9151, F1: 0.7243
[VIT] val - Epoch: 73, Loss: 0.5234, Acc: 0.8197, AUC: 0.9002, F1: 0.6715
epoch: 74, total loss: 2.867469098256982
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.4724, Acc: 0.8415, AUC: 0.9089, F1: 0.7121
[VIT] val - Epoch: 74, Loss: 0.5267, Acc: 0.8128, AUC: 0.9012, F1: 0.6555
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9012
Saving vit model...
epoch: 75, total loss: 2.866408892299818
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.4717, Acc: 0.8265, AUC: 0.9159, F1: 0.6949
[VIT] val - Epoch: 75, Loss: 0.5292, Acc: 0.8115, AUC: 0.9014, F1: 0.6617
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9014
Saving vit model...
epoch: 76, total loss: 2.5836189855699954
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.4937, Acc: 0.8265, AUC: 0.9046, F1: 0.6803
[VIT] val - Epoch: 76, Loss: 0.5239, Acc: 0.8128, AUC: 0.9007, F1: 0.6563
epoch: 77, total loss: 2.674764905286872
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.5021, Acc: 0.8197, AUC: 0.9043, F1: 0.6770
[VIT] val - Epoch: 77, Loss: 0.5122, Acc: 0.8238, AUC: 0.9002, F1: 0.6557
epoch: 78, total loss: 2.5942270159721375
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.5591, Acc: 0.7978, AUC: 0.9009, F1: 0.6577
[VIT] val - Epoch: 78, Loss: 0.5312, Acc: 0.8046, AUC: 0.9006, F1: 0.6541
epoch: 79, total loss: 2.769943330598914
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.5507, Acc: 0.7978, AUC: 0.9097, F1: 0.6756
[VIT] val - Epoch: 79, Loss: 0.5209, Acc: 0.8197, AUC: 0.9010, F1: 0.6719
epoch: 80, total loss: 2.542383284672447
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.4504, Acc: 0.8525, AUC: 0.9160, F1: 0.7253
[VIT] val - Epoch: 80, Loss: 0.5180, Acc: 0.8183, AUC: 0.9011, F1: 0.6586
epoch: 81, total loss: 2.545327015545057
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.5494, Acc: 0.8046, AUC: 0.9046, F1: 0.6731
[VIT] val - Epoch: 81, Loss: 0.5274, Acc: 0.8169, AUC: 0.9014, F1: 0.6704
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9014
Saving vit model...
epoch: 82, total loss: 2.469167903713558
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.4583, Acc: 0.8347, AUC: 0.9182, F1: 0.7030
[VIT] val - Epoch: 82, Loss: 0.5201, Acc: 0.8156, AUC: 0.9005, F1: 0.6643
epoch: 83, total loss: 2.754968332207721
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.5345, Acc: 0.8033, AUC: 0.9097, F1: 0.6706
[VIT] val - Epoch: 83, Loss: 0.5124, Acc: 0.8265, AUC: 0.9017, F1: 0.6734
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9017
Saving vit model...
epoch: 84, total loss: 2.631128593631413
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.4634, Acc: 0.8374, AUC: 0.9143, F1: 0.6977
[VIT] val - Epoch: 84, Loss: 0.5182, Acc: 0.8101, AUC: 0.8999, F1: 0.6569
epoch: 85, total loss: 2.5009646726691206
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.4435, Acc: 0.8511, AUC: 0.9189, F1: 0.7218
[VIT] val - Epoch: 85, Loss: 0.5125, Acc: 0.8265, AUC: 0.9009, F1: 0.6721
epoch: 86, total loss: 2.640844632749972
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.4821, Acc: 0.8292, AUC: 0.9113, F1: 0.6977
[VIT] val - Epoch: 86, Loss: 0.5075, Acc: 0.8224, AUC: 0.9007, F1: 0.6583
epoch: 87, total loss: 2.7376328395760576
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.4545, Acc: 0.8402, AUC: 0.9207, F1: 0.7144
[VIT] val - Epoch: 87, Loss: 0.5101, Acc: 0.8224, AUC: 0.9001, F1: 0.6679
epoch: 88, total loss: 2.826515817123911
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.5669, Acc: 0.7814, AUC: 0.9110, F1: 0.6546
[VIT] val - Epoch: 88, Loss: 0.5186, Acc: 0.8183, AUC: 0.9013, F1: 0.6711
epoch: 89, total loss: 2.637511222258858
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.4757, Acc: 0.8347, AUC: 0.9101, F1: 0.7018
[VIT] val - Epoch: 89, Loss: 0.5098, Acc: 0.8279, AUC: 0.9011, F1: 0.6762
epoch: 90, total loss: 2.8606602782788486
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.4980, Acc: 0.8210, AUC: 0.9079, F1: 0.6877
[VIT] val - Epoch: 90, Loss: 0.5207, Acc: 0.8183, AUC: 0.9018, F1: 0.6680
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9018
Saving vit model...
epoch: 91, total loss: 2.5742944738139277
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.4400, Acc: 0.8511, AUC: 0.9114, F1: 0.7076
[VIT] val - Epoch: 91, Loss: 0.5065, Acc: 0.8279, AUC: 0.9019, F1: 0.6731
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9019
Saving vit model...
epoch: 92, total loss: 2.733311072639797
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.4762, Acc: 0.8320, AUC: 0.9132, F1: 0.7085
[VIT] val - Epoch: 92, Loss: 0.5193, Acc: 0.8142, AUC: 0.9020, F1: 0.6599
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9020
Saving vit model...
epoch: 93, total loss: 2.5028682532517808
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.4539, Acc: 0.8415, AUC: 0.9170, F1: 0.7185
[VIT] val - Epoch: 93, Loss: 0.5091, Acc: 0.8279, AUC: 0.9016, F1: 0.6783
epoch: 94, total loss: 2.5597431037736977
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.5071, Acc: 0.8251, AUC: 0.9044, F1: 0.6866
[VIT] val - Epoch: 94, Loss: 0.5152, Acc: 0.8224, AUC: 0.9018, F1: 0.6729
epoch: 95, total loss: 2.441457722498023
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.4613, Acc: 0.8333, AUC: 0.9151, F1: 0.7072
[VIT] val - Epoch: 95, Loss: 0.5122, Acc: 0.8238, AUC: 0.9017, F1: 0.6739
epoch: 96, total loss: 2.359976724438045
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.4651, Acc: 0.8320, AUC: 0.9147, F1: 0.6955
[VIT] val - Epoch: 96, Loss: 0.5147, Acc: 0.8210, AUC: 0.9017, F1: 0.6703
epoch: 97, total loss: 2.820497284764829
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.4776, Acc: 0.8169, AUC: 0.9169, F1: 0.6846
[VIT] val - Epoch: 97, Loss: 0.5145, Acc: 0.8197, AUC: 0.9030, F1: 0.6661
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9030
Saving vit model...
epoch: 98, total loss: 2.58749293503554
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.4715, Acc: 0.8361, AUC: 0.9150, F1: 0.7059
[VIT] val - Epoch: 98, Loss: 0.5166, Acc: 0.8183, AUC: 0.9026, F1: 0.6707
epoch: 99, total loss: 2.688936534135238
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.4789, Acc: 0.8402, AUC: 0.9073, F1: 0.6806
[VIT] val - Epoch: 99, Loss: 0.5099, Acc: 0.8238, AUC: 0.9020, F1: 0.6717
epoch: 100, total loss: 2.5707262573034866
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.4788, Acc: 0.8374, AUC: 0.9110, F1: 0.7101
[VIT] val - Epoch: 100, Loss: 0.5188, Acc: 0.8156, AUC: 0.9022, F1: 0.6661
[18:44:44][Rank 3] Training Finished. Starting Final Testing...
[18:44:44][Rank 2] Training Finished. Starting Final Testing...
[18:44:44][Rank 1] Training Finished. Starting Final Testing...
[18:44:44][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.0331, Acc: 0.6585, AUC: 0.7400, F1: 0.4192
[VIT] test - Epoch: 100, Loss: 1.0283, Acc: 0.6479, AUC: 0.6969, F1: 0.3290
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.2256, Acc: 0.6212, AUC: 0.7104, F1: 0.3622
[VIT] test - Epoch: 100, Loss: 1.0775, Acc: 0.6525, AUC: 0.7140, F1: 0.3688
‚úÖ [ÂÆåÊàê] Ê∫êÂüü APTOS ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: DEEPDR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='DEEPDR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['DEEPDR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_DEEPDR
OUT_DIR: ./output_esdg_h100/DEEPDR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified4/output_esdg_h100/DEEPDR/CASS_GDRNet_ESDG_DEEPDR
[18:52:54][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
[18:52:54][Rank 1] Loading datasets...
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
[18:52:54][Rank 3] Loading datasets...
[18:52:54][Rank 2] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.üßä [DINOv3] All base parameters frozen.

üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj

Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_projInjecting LoRA into: layer.1.attention.k_proj

Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.q_projInjecting LoRA into: layer.1.attention.q_proj

Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_projInjecting LoRA into: layer.5.attention.k_proj

Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_projInjecting LoRA into: layer.4.attention.q_proj

Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_projInjecting LoRA into: layer.6.attention.k_proj

Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_projInjecting LoRA into: layer.7.mlp.down_proj

Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.8.attention.v_projInjecting LoRA into: layer.9.attention.k_proj

Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_projInjecting LoRA into: layer.9.mlp.down_proj

Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.11.attention.v_projInjecting LoRA into: layer.11.attention.v_proj

Injecting LoRA into: layer.10.attention.q_projInjecting LoRA into: layer.11.attention.k_proj

Injecting LoRA into: layer.11.attention.q_projInjecting LoRA into: layer.11.attention.q_proj

Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_projInjecting LoRA into: layer.11.attention.q_proj

Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_projInjecting LoRA into: layer.11.mlp.down_proj

Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.11.mlp.up_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.11.mlp.down_proj
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 5.107759237289429
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.1627, Acc: 0.5531, AUC: 0.7373, F1: 0.2776
[VIT] val - Epoch: 1, Loss: 1.3552, Acc: 0.5719, AUC: 0.6956, F1: 0.2507
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7373
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6956
Saving vit model...
epoch: 2, total loss: 4.695068490505219
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.2159, Acc: 0.5094, AUC: 0.8294, F1: 0.4440
[VIT] val - Epoch: 2, Loss: 1.2829, Acc: 0.6031, AUC: 0.7777, F1: 0.3509
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8294
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7777
Saving vit model...
epoch: 3, total loss: 4.017217814922333
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.8885, Acc: 0.6406, AUC: 0.8714, F1: 0.4336
[VIT] val - Epoch: 3, Loss: 1.2154, Acc: 0.6219, AUC: 0.8227, F1: 0.4198
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8714
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8227
Saving vit model...
epoch: 4, total loss: 4.997405731678009
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.8220, Acc: 0.6594, AUC: 0.8925, F1: 0.5409
[VIT] val - Epoch: 4, Loss: 1.1907, Acc: 0.6438, AUC: 0.8354, F1: 0.4914
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8925
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8354
Saving vit model...
epoch: 5, total loss: 5.385551190376281
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.7454, Acc: 0.7031, AUC: 0.9014, F1: 0.5761
[VIT] val - Epoch: 5, Loss: 1.1238, Acc: 0.6781, AUC: 0.8462, F1: 0.5393
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9014
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8462
Saving vit model...
epoch: 6, total loss: 4.976681590080261
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.7153, Acc: 0.7406, AUC: 0.9009, F1: 0.5722
[VIT] val - Epoch: 6, Loss: 1.0662, Acc: 0.6750, AUC: 0.8529, F1: 0.5012
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8529
Saving vit model...
epoch: 7, total loss: 5.188824725151062
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.7589, Acc: 0.7125, AUC: 0.9077, F1: 0.5869
[VIT] val - Epoch: 7, Loss: 1.0232, Acc: 0.7063, AUC: 0.8577, F1: 0.5269
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9077
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8577
Saving vit model...
epoch: 8, total loss: 4.747728645801544
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.7349, Acc: 0.7250, AUC: 0.8950, F1: 0.5832
[VIT] val - Epoch: 8, Loss: 1.0077, Acc: 0.7156, AUC: 0.8625, F1: 0.5714
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8625
Saving vit model...
epoch: 9, total loss: 4.6069742918014525
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.7170, Acc: 0.7406, AUC: 0.9088, F1: 0.6010
[VIT] val - Epoch: 9, Loss: 1.0099, Acc: 0.6844, AUC: 0.8658, F1: 0.5364
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9088
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8658
Saving vit model...
epoch: 10, total loss: 4.6179880738258365
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.7852, Acc: 0.7219, AUC: 0.8904, F1: 0.6038
[VIT] val - Epoch: 10, Loss: 0.9665, Acc: 0.7031, AUC: 0.8665, F1: 0.5595
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8665
Saving vit model...
epoch: 11, total loss: 4.582210302352905
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.8949, Acc: 0.6375, AUC: 0.8860, F1: 0.5248
[VIT] val - Epoch: 11, Loss: 0.9662, Acc: 0.6906, AUC: 0.8703, F1: 0.5569
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8703
Saving vit model...
epoch: 12, total loss: 4.696424734592438
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.7843, Acc: 0.6906, AUC: 0.9258, F1: 0.5885
[VIT] val - Epoch: 12, Loss: 0.9520, Acc: 0.7188, AUC: 0.8724, F1: 0.6094
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9258
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8724
Saving vit model...
epoch: 13, total loss: 4.344676065444946
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.9521, Acc: 0.6156, AUC: 0.8859, F1: 0.4650
[VIT] val - Epoch: 13, Loss: 0.9150, Acc: 0.7188, AUC: 0.8729, F1: 0.5391
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8729
Saving vit model...
epoch: 14, total loss: 4.534874951839447
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.7204, Acc: 0.7531, AUC: 0.9212, F1: 0.6348
[VIT] val - Epoch: 14, Loss: 0.9620, Acc: 0.6750, AUC: 0.8762, F1: 0.5473
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8762
Saving vit model...
epoch: 15, total loss: 4.477725553512573
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.6893, Acc: 0.7063, AUC: 0.9272, F1: 0.6102
[VIT] val - Epoch: 15, Loss: 0.8991, Acc: 0.7188, AUC: 0.8762, F1: 0.5651
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9272
Saving cnn model...
epoch: 16, total loss: 4.138084805011749
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.6563, Acc: 0.7500, AUC: 0.9360, F1: 0.6861
[VIT] val - Epoch: 16, Loss: 0.9472, Acc: 0.6875, AUC: 0.8779, F1: 0.5641
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9360
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8779
Saving vit model...
epoch: 17, total loss: 4.561617863178253
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.6593, Acc: 0.7625, AUC: 0.9192, F1: 0.6591
[VIT] val - Epoch: 17, Loss: 0.8870, Acc: 0.7156, AUC: 0.8788, F1: 0.5670
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8788
Saving vit model...
epoch: 18, total loss: 4.46711698770523
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.7137, Acc: 0.7375, AUC: 0.9218, F1: 0.6115
[VIT] val - Epoch: 18, Loss: 0.8688, Acc: 0.7281, AUC: 0.8796, F1: 0.5894
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8796
Saving vit model...
epoch: 19, total loss: 4.289039564132691
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.6315, Acc: 0.7750, AUC: 0.9180, F1: 0.6736
[VIT] val - Epoch: 19, Loss: 0.8909, Acc: 0.6813, AUC: 0.8795, F1: 0.5663
epoch: 20, total loss: 4.192244756221771
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.6252, Acc: 0.7562, AUC: 0.9400, F1: 0.6920
[VIT] val - Epoch: 20, Loss: 0.8925, Acc: 0.6937, AUC: 0.8821, F1: 0.5807
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9400
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8821
Saving vit model...
epoch: 21, total loss: 4.270181119441986
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.7268, Acc: 0.7188, AUC: 0.9316, F1: 0.6485
[VIT] val - Epoch: 21, Loss: 0.8603, Acc: 0.7125, AUC: 0.8832, F1: 0.5778
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8832
Saving vit model...
epoch: 22, total loss: 4.231808578968048
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.7814, Acc: 0.6875, AUC: 0.9227, F1: 0.5959
[VIT] val - Epoch: 22, Loss: 0.8244, Acc: 0.7375, AUC: 0.8815, F1: 0.5827
epoch: 23, total loss: 4.076119196414948
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.7943, Acc: 0.6750, AUC: 0.9136, F1: 0.5485
[VIT] val - Epoch: 23, Loss: 0.8532, Acc: 0.7219, AUC: 0.8855, F1: 0.5891
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8855
Saving vit model...
epoch: 24, total loss: 4.067972099781036
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.6284, Acc: 0.7719, AUC: 0.9439, F1: 0.7203
[VIT] val - Epoch: 24, Loss: 0.8808, Acc: 0.6687, AUC: 0.8846, F1: 0.5589
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9439
Saving cnn model...
epoch: 25, total loss: 4.092423903942108
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.8169, Acc: 0.6937, AUC: 0.9180, F1: 0.5645
[VIT] val - Epoch: 25, Loss: 0.8582, Acc: 0.6937, AUC: 0.8865, F1: 0.5688
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8865
Saving vit model...
epoch: 26, total loss: 4.0497264742851256
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.7442, Acc: 0.7156, AUC: 0.9317, F1: 0.6455
[VIT] val - Epoch: 26, Loss: 0.8335, Acc: 0.7281, AUC: 0.8850, F1: 0.6023
epoch: 27, total loss: 3.801440691947937
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.6885, Acc: 0.7531, AUC: 0.9117, F1: 0.6687
[VIT] val - Epoch: 27, Loss: 0.8436, Acc: 0.6969, AUC: 0.8872, F1: 0.5674
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8872
Saving vit model...
epoch: 28, total loss: 3.9978829264640807
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.7217, Acc: 0.7125, AUC: 0.9149, F1: 0.5711
[VIT] val - Epoch: 28, Loss: 0.8403, Acc: 0.6813, AUC: 0.8854, F1: 0.5680
epoch: 29, total loss: 3.924370002746582
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.6240, Acc: 0.7594, AUC: 0.9420, F1: 0.6788
[VIT] val - Epoch: 29, Loss: 0.8248, Acc: 0.7031, AUC: 0.8866, F1: 0.5831
epoch: 30, total loss: 3.6155266046524046
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.6442, Acc: 0.7594, AUC: 0.9325, F1: 0.6610
[VIT] val - Epoch: 30, Loss: 0.8231, Acc: 0.7063, AUC: 0.8878, F1: 0.5847
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8878
Saving vit model...
epoch: 31, total loss: 3.6704962730407713
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.6822, Acc: 0.7344, AUC: 0.9244, F1: 0.6476
[VIT] val - Epoch: 31, Loss: 0.8204, Acc: 0.7063, AUC: 0.8881, F1: 0.5872
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8881
Saving vit model...
epoch: 32, total loss: 3.6117005348205566
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.6692, Acc: 0.7406, AUC: 0.9201, F1: 0.6021
[VIT] val - Epoch: 32, Loss: 0.8080, Acc: 0.7250, AUC: 0.8882, F1: 0.5907
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8882
Saving vit model...
epoch: 33, total loss: 3.7481966733932497
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.7809, Acc: 0.6813, AUC: 0.9214, F1: 0.5818
[VIT] val - Epoch: 33, Loss: 0.8221, Acc: 0.7000, AUC: 0.8895, F1: 0.5775
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8895
Saving vit model...
epoch: 34, total loss: 4.05534336566925
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.6487, Acc: 0.7594, AUC: 0.9208, F1: 0.6438
[VIT] val - Epoch: 34, Loss: 0.7915, Acc: 0.7250, AUC: 0.8894, F1: 0.5907
epoch: 35, total loss: 3.774809992313385
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.6818, Acc: 0.7375, AUC: 0.9270, F1: 0.6444
[VIT] val - Epoch: 35, Loss: 0.8501, Acc: 0.6719, AUC: 0.8907, F1: 0.5749
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8907
Saving vit model...
epoch: 36, total loss: 3.867325222492218
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.6895, Acc: 0.7406, AUC: 0.9299, F1: 0.6326
[VIT] val - Epoch: 36, Loss: 0.7954, Acc: 0.7250, AUC: 0.8908, F1: 0.5882
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8908
Saving vit model...
epoch: 37, total loss: 3.5756632924079894
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8723, Acc: 0.6844, AUC: 0.9166, F1: 0.6110
[VIT] val - Epoch: 37, Loss: 0.8444, Acc: 0.6906, AUC: 0.8912, F1: 0.5872
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8912
Saving vit model...
epoch: 38, total loss: 3.800615656375885
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.7080, Acc: 0.7500, AUC: 0.9323, F1: 0.7012
[VIT] val - Epoch: 38, Loss: 0.7980, Acc: 0.7156, AUC: 0.8901, F1: 0.5839
epoch: 39, total loss: 3.769440305233002
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.7521, Acc: 0.7375, AUC: 0.9115, F1: 0.6003
[VIT] val - Epoch: 39, Loss: 0.7937, Acc: 0.7125, AUC: 0.8905, F1: 0.5884
epoch: 40, total loss: 3.5184070110321044
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.6398, Acc: 0.7438, AUC: 0.9384, F1: 0.6876
[VIT] val - Epoch: 40, Loss: 0.8091, Acc: 0.6844, AUC: 0.8907, F1: 0.5733
epoch: 41, total loss: 3.508808696269989
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.6512, Acc: 0.7469, AUC: 0.9384, F1: 0.6692
[VIT] val - Epoch: 41, Loss: 0.8276, Acc: 0.6969, AUC: 0.8923, F1: 0.6023
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8923
Saving vit model...
epoch: 42, total loss: 3.8731714606285097
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.7053, Acc: 0.7406, AUC: 0.9293, F1: 0.6346
[VIT] val - Epoch: 42, Loss: 0.7936, Acc: 0.7031, AUC: 0.8903, F1: 0.5955
epoch: 43, total loss: 3.605852484703064
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.6868, Acc: 0.7344, AUC: 0.9296, F1: 0.6203
[VIT] val - Epoch: 43, Loss: 0.8109, Acc: 0.6969, AUC: 0.8918, F1: 0.5936
epoch: 44, total loss: 3.784316611289978
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.6566, Acc: 0.7562, AUC: 0.9246, F1: 0.6556
[VIT] val - Epoch: 44, Loss: 0.7867, Acc: 0.7156, AUC: 0.8907, F1: 0.6083
epoch: 45, total loss: 3.565091550350189
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.7064, Acc: 0.7125, AUC: 0.9283, F1: 0.6033
[VIT] val - Epoch: 45, Loss: 0.8040, Acc: 0.6937, AUC: 0.8914, F1: 0.5880
epoch: 46, total loss: 3.9507731318473818
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.6895, Acc: 0.7719, AUC: 0.9296, F1: 0.6507
[VIT] val - Epoch: 46, Loss: 0.7921, Acc: 0.7000, AUC: 0.8913, F1: 0.5882
epoch: 47, total loss: 3.481246769428253
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.7194, Acc: 0.7375, AUC: 0.9350, F1: 0.6960
[VIT] val - Epoch: 47, Loss: 0.8168, Acc: 0.6813, AUC: 0.8928, F1: 0.5801
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8928
Saving vit model...
epoch: 48, total loss: 3.6432049036026
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.7942, Acc: 0.7219, AUC: 0.8913, F1: 0.5274
[VIT] val - Epoch: 48, Loss: 0.7956, Acc: 0.7031, AUC: 0.8940, F1: 0.5967
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8940
Saving vit model...
epoch: 49, total loss: 3.510615861415863
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.6109, Acc: 0.7688, AUC: 0.9404, F1: 0.6948
[VIT] val - Epoch: 49, Loss: 0.7742, Acc: 0.7188, AUC: 0.8914, F1: 0.6111
epoch: 50, total loss: 3.772665297985077
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.6010, Acc: 0.7812, AUC: 0.9374, F1: 0.6941
[VIT] val - Epoch: 50, Loss: 0.8002, Acc: 0.6875, AUC: 0.8927, F1: 0.5909
epoch: 51, total loss: 3.437870216369629
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.6533, Acc: 0.7656, AUC: 0.9243, F1: 0.6524
[VIT] val - Epoch: 51, Loss: 0.7738, Acc: 0.7156, AUC: 0.8929, F1: 0.6013
epoch: 52, total loss: 3.6780645728111265
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.5636, Acc: 0.7750, AUC: 0.9421, F1: 0.6998
[VIT] val - Epoch: 52, Loss: 0.7874, Acc: 0.7000, AUC: 0.8933, F1: 0.5933
epoch: 53, total loss: 3.793409860134125
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.6111, Acc: 0.7875, AUC: 0.9328, F1: 0.7181
[VIT] val - Epoch: 53, Loss: 0.7708, Acc: 0.7188, AUC: 0.8924, F1: 0.6189
epoch: 54, total loss: 3.547409212589264
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.6212, Acc: 0.7656, AUC: 0.9390, F1: 0.6686
[VIT] val - Epoch: 54, Loss: 0.7824, Acc: 0.7094, AUC: 0.8939, F1: 0.6198
epoch: 55, total loss: 3.333321487903595
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.7036, Acc: 0.7594, AUC: 0.9312, F1: 0.6706
[VIT] val - Epoch: 55, Loss: 0.7698, Acc: 0.7219, AUC: 0.8927, F1: 0.6086
epoch: 56, total loss: 3.5412633180618287
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.7242, Acc: 0.6937, AUC: 0.9341, F1: 0.6393
[VIT] val - Epoch: 56, Loss: 0.8189, Acc: 0.6844, AUC: 0.8933, F1: 0.5921
epoch: 57, total loss: 3.3726049065589905
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.6117, Acc: 0.7562, AUC: 0.9355, F1: 0.6541
[VIT] val - Epoch: 57, Loss: 0.7897, Acc: 0.6906, AUC: 0.8935, F1: 0.5996
epoch: 58, total loss: 3.387683117389679
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.6659, Acc: 0.7375, AUC: 0.9350, F1: 0.6930
[VIT] val - Epoch: 58, Loss: 0.7946, Acc: 0.6969, AUC: 0.8937, F1: 0.6023
epoch: 59, total loss: 3.476539146900177
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.6240, Acc: 0.7719, AUC: 0.9316, F1: 0.6940
[VIT] val - Epoch: 59, Loss: 0.7822, Acc: 0.7063, AUC: 0.8943, F1: 0.6168
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8943
Saving vit model...
epoch: 60, total loss: 3.6592809200286864
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.6344, Acc: 0.7719, AUC: 0.9260, F1: 0.6830
[VIT] val - Epoch: 60, Loss: 0.7878, Acc: 0.6937, AUC: 0.8945, F1: 0.5995
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8945
Saving vit model...
epoch: 61, total loss: 3.4960546255111695
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.6624, Acc: 0.7438, AUC: 0.9289, F1: 0.6093
[VIT] val - Epoch: 61, Loss: 0.7833, Acc: 0.6969, AUC: 0.8941, F1: 0.6110
epoch: 62, total loss: 3.2994231283664703
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.6836, Acc: 0.7438, AUC: 0.9312, F1: 0.6940
[VIT] val - Epoch: 62, Loss: 0.7792, Acc: 0.7000, AUC: 0.8945, F1: 0.6011
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8945
Saving vit model...
epoch: 63, total loss: 3.6129193902015686
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.6600, Acc: 0.7531, AUC: 0.9261, F1: 0.6744
[VIT] val - Epoch: 63, Loss: 0.7848, Acc: 0.6844, AUC: 0.8945, F1: 0.5818
epoch: 64, total loss: 3.2256923735141756
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.6475, Acc: 0.7531, AUC: 0.9278, F1: 0.6824
[VIT] val - Epoch: 64, Loss: 0.7783, Acc: 0.6875, AUC: 0.8947, F1: 0.5858
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8947
Saving vit model...
epoch: 65, total loss: 3.195430338382721
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.6663, Acc: 0.7344, AUC: 0.9315, F1: 0.6334
[VIT] val - Epoch: 65, Loss: 0.7610, Acc: 0.7156, AUC: 0.8935, F1: 0.6094
epoch: 66, total loss: 3.565569829940796
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.6560, Acc: 0.7625, AUC: 0.9224, F1: 0.6837
[VIT] val - Epoch: 66, Loss: 0.7728, Acc: 0.7031, AUC: 0.8949, F1: 0.6012
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8949
Saving vit model...
epoch: 67, total loss: 3.3669947743415833
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.6886, Acc: 0.7281, AUC: 0.9314, F1: 0.6564
[VIT] val - Epoch: 67, Loss: 0.7740, Acc: 0.7031, AUC: 0.8938, F1: 0.6139
epoch: 68, total loss: 3.392929208278656
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.6861, Acc: 0.7500, AUC: 0.9292, F1: 0.6607
[VIT] val - Epoch: 68, Loss: 0.7771, Acc: 0.6906, AUC: 0.8944, F1: 0.5985
epoch: 69, total loss: 3.147424304485321
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.6540, Acc: 0.7406, AUC: 0.9346, F1: 0.6949
[VIT] val - Epoch: 69, Loss: 0.7692, Acc: 0.7063, AUC: 0.8945, F1: 0.6173
epoch: 70, total loss: 3.1390011966228486
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.6932, Acc: 0.7312, AUC: 0.9176, F1: 0.6614
[VIT] val - Epoch: 70, Loss: 0.7784, Acc: 0.6906, AUC: 0.8946, F1: 0.6018
epoch: 71, total loss: 3.3714268445968627
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.6380, Acc: 0.7781, AUC: 0.9263, F1: 0.6918
[VIT] val - Epoch: 71, Loss: 0.7725, Acc: 0.6875, AUC: 0.8948, F1: 0.5896
epoch: 72, total loss: 3.3147279739379885
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.6832, Acc: 0.7625, AUC: 0.9249, F1: 0.6929
[VIT] val - Epoch: 72, Loss: 0.7750, Acc: 0.6937, AUC: 0.8944, F1: 0.6052
epoch: 73, total loss: 3.2360074877738954
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.6975, Acc: 0.7344, AUC: 0.9224, F1: 0.6138
[VIT] val - Epoch: 73, Loss: 0.7829, Acc: 0.6906, AUC: 0.8956, F1: 0.5995
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8956
Saving vit model...
epoch: 74, total loss: 3.1940914571285246
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.6785, Acc: 0.7375, AUC: 0.9253, F1: 0.6662
[VIT] val - Epoch: 74, Loss: 0.7671, Acc: 0.7031, AUC: 0.8952, F1: 0.6066
epoch: 75, total loss: 3.5174775958061217
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.6327, Acc: 0.7438, AUC: 0.9366, F1: 0.6837
[VIT] val - Epoch: 75, Loss: 0.7814, Acc: 0.6844, AUC: 0.8952, F1: 0.5944
epoch: 76, total loss: 3.4805163860321047
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.6339, Acc: 0.7500, AUC: 0.9285, F1: 0.6861
[VIT] val - Epoch: 76, Loss: 0.7599, Acc: 0.7094, AUC: 0.8943, F1: 0.6119
epoch: 77, total loss: 3.2029173612594604
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.7331, Acc: 0.7406, AUC: 0.9187, F1: 0.6414
[VIT] val - Epoch: 77, Loss: 0.7647, Acc: 0.6969, AUC: 0.8951, F1: 0.5978
epoch: 78, total loss: 3.3506386160850523
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.6717, Acc: 0.7375, AUC: 0.9292, F1: 0.6881
[VIT] val - Epoch: 78, Loss: 0.7601, Acc: 0.7063, AUC: 0.8939, F1: 0.6141
epoch: 79, total loss: 3.6841058135032654
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.6388, Acc: 0.7500, AUC: 0.9335, F1: 0.6650
[VIT] val - Epoch: 79, Loss: 0.7633, Acc: 0.7063, AUC: 0.8944, F1: 0.6211
epoch: 80, total loss: 3.3182159304618835
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.6613, Acc: 0.7406, AUC: 0.9345, F1: 0.6946
[VIT] val - Epoch: 80, Loss: 0.7636, Acc: 0.7063, AUC: 0.8952, F1: 0.6175
epoch: 81, total loss: 3.2348371028900145
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.6525, Acc: 0.7625, AUC: 0.9260, F1: 0.6783
[VIT] val - Epoch: 81, Loss: 0.7601, Acc: 0.7000, AUC: 0.8962, F1: 0.6007
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8962
Saving vit model...
epoch: 82, total loss: 3.2428004264831545
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.6765, Acc: 0.7438, AUC: 0.9294, F1: 0.6645
[VIT] val - Epoch: 82, Loss: 0.7789, Acc: 0.6937, AUC: 0.8965, F1: 0.6042
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8965
Saving vit model...
epoch: 83, total loss: 3.4422136425971983
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.6769, Acc: 0.7594, AUC: 0.9282, F1: 0.6306
[VIT] val - Epoch: 83, Loss: 0.7575, Acc: 0.7094, AUC: 0.8953, F1: 0.6124
epoch: 84, total loss: 3.240388798713684
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.7344, Acc: 0.7094, AUC: 0.9319, F1: 0.6294
[VIT] val - Epoch: 84, Loss: 0.7634, Acc: 0.7000, AUC: 0.8959, F1: 0.6017
epoch: 85, total loss: 3.2111316323280334
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.6234, Acc: 0.7594, AUC: 0.9381, F1: 0.6597
[VIT] val - Epoch: 85, Loss: 0.7704, Acc: 0.7000, AUC: 0.8971, F1: 0.6095
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8971
Saving vit model...
epoch: 86, total loss: 3.614775335788727
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.7080, Acc: 0.7594, AUC: 0.9132, F1: 0.6577
[VIT] val - Epoch: 86, Loss: 0.7641, Acc: 0.6937, AUC: 0.8962, F1: 0.5982
epoch: 87, total loss: 3.464541232585907
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.6300, Acc: 0.7562, AUC: 0.9421, F1: 0.6613
[VIT] val - Epoch: 87, Loss: 0.7742, Acc: 0.7063, AUC: 0.8972, F1: 0.6234
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8972
Saving vit model...
epoch: 88, total loss: 3.0852108716964723
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.6739, Acc: 0.7406, AUC: 0.9323, F1: 0.6598
[VIT] val - Epoch: 88, Loss: 0.7741, Acc: 0.7000, AUC: 0.8975, F1: 0.6125
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8975
Saving vit model...
epoch: 89, total loss: 3.249895083904266
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.6364, Acc: 0.7500, AUC: 0.9411, F1: 0.6851
[VIT] val - Epoch: 89, Loss: 0.7588, Acc: 0.7094, AUC: 0.8969, F1: 0.6176
epoch: 90, total loss: 3.05111026763916
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.6297, Acc: 0.7562, AUC: 0.9448, F1: 0.7014
[VIT] val - Epoch: 90, Loss: 0.7616, Acc: 0.7063, AUC: 0.8961, F1: 0.6211
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9448
Saving cnn model...
epoch: 91, total loss: 3.311743068695068
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.6445, Acc: 0.7438, AUC: 0.9415, F1: 0.6472
[VIT] val - Epoch: 91, Loss: 0.7542, Acc: 0.7063, AUC: 0.8969, F1: 0.6141
epoch: 92, total loss: 3.3138513445854185
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.6618, Acc: 0.7312, AUC: 0.9386, F1: 0.6832
[VIT] val - Epoch: 92, Loss: 0.7551, Acc: 0.7063, AUC: 0.8966, F1: 0.6177
epoch: 93, total loss: 3.35584841966629
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.6010, Acc: 0.7562, AUC: 0.9414, F1: 0.6907
[VIT] val - Epoch: 93, Loss: 0.7585, Acc: 0.7031, AUC: 0.8967, F1: 0.6162
epoch: 94, total loss: 3.0984134912490844
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.6626, Acc: 0.7469, AUC: 0.9359, F1: 0.6855
[VIT] val - Epoch: 94, Loss: 0.7572, Acc: 0.7000, AUC: 0.8970, F1: 0.6106
epoch: 95, total loss: 3.2094269752502442
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.6325, Acc: 0.7656, AUC: 0.9308, F1: 0.7031
[VIT] val - Epoch: 95, Loss: 0.7539, Acc: 0.7094, AUC: 0.8963, F1: 0.6203
epoch: 96, total loss: 3.222671401500702
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.6570, Acc: 0.7344, AUC: 0.9279, F1: 0.6683
[VIT] val - Epoch: 96, Loss: 0.7605, Acc: 0.7031, AUC: 0.8960, F1: 0.6193
epoch: 97, total loss: 3.4744216561317445
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.6176, Acc: 0.7531, AUC: 0.9356, F1: 0.6559
[VIT] val - Epoch: 97, Loss: 0.7687, Acc: 0.7000, AUC: 0.8969, F1: 0.6163
epoch: 98, total loss: 3.2161773800849915
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.6161, Acc: 0.7531, AUC: 0.9347, F1: 0.6782
[VIT] val - Epoch: 98, Loss: 0.7525, Acc: 0.7063, AUC: 0.8960, F1: 0.6136
epoch: 99, total loss: 3.3944440364837645
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.6672, Acc: 0.7656, AUC: 0.9344, F1: 0.6370
[VIT] val - Epoch: 99, Loss: 0.7526, Acc: 0.7063, AUC: 0.8963, F1: 0.6148
epoch: 100, total loss: 3.314684748649597
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.6257, Acc: 0.7562, AUC: 0.9295, F1: 0.6752
[VIT] val - Epoch: 100, Loss: 0.7568, Acc: 0.7063, AUC: 0.8966, F1: 0.6211
[19:26:58][Rank 2] Training Finished. Starting Final Testing...[19:26:58][Rank 1] Training Finished. Starting Final Testing...[19:26:58][Rank 3] Training Finished. Starting Final Testing...


[19:26:58][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.0897, Acc: 0.5727, AUC: 0.7980, F1: 0.4389
[VIT] test - Epoch: 100, Loss: 1.2215, Acc: 0.4409, AUC: 0.7707, F1: 0.3710
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.1885, Acc: 0.5378, AUC: 0.7907, F1: 0.4014
[VIT] test - Epoch: 100, Loss: 1.2673, Acc: 0.4110, AUC: 0.7702, F1: 0.3562
‚úÖ [ÂÆåÊàê] Ê∫êÂüü DEEPDR ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: FGADR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
[19:34:21][Rank 1] Loading datasets...
[19:34:21][Rank 2] Loading datasets...
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='FGADR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['FGADR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_FGADR
OUT_DIR: ./output_esdg_h100/FGADR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified4/output_esdg_h100/FGADR/CASS_GDRNet_ESDG_FGADR
[19:34:21][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
[19:34:21][Rank 3] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.üßä [DINOv3] All base parameters frozen.

Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_projInjecting LoRA into: layer.5.attention.o_proj

Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_projInjecting LoRA into: layer.6.attention.q_proj

Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_projInjecting LoRA into: layer.10.attention.q_proj

Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_projInjecting LoRA into: layer.10.attention.q_proj

Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 4.6905843416849775
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.3111, Acc: 0.4348, AUC: 0.8418, F1: 0.2222
[VIT] val - Epoch: 1, Loss: 1.3727, Acc: 0.4674, AUC: 0.7239, F1: 0.2367
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8418
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7239
Saving vit model...
epoch: 2, total loss: 4.450550397237142
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 0.9476, Acc: 0.6522, AUC: 0.8720, F1: 0.4060
[VIT] val - Epoch: 2, Loss: 1.2449, Acc: 0.5489, AUC: 0.7919, F1: 0.2548
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8720
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7919
Saving vit model...
epoch: 3, total loss: 4.57245996594429
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.9675, Acc: 0.6630, AUC: 0.8702, F1: 0.4656
[VIT] val - Epoch: 3, Loss: 1.2258, Acc: 0.6196, AUC: 0.8113, F1: 0.4167
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8113
Saving vit model...
epoch: 4, total loss: 5.471033602952957
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.8591, Acc: 0.7038, AUC: 0.8762, F1: 0.5858
[VIT] val - Epoch: 4, Loss: 1.1352, Acc: 0.6522, AUC: 0.8193, F1: 0.4162
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8762
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8193
Saving vit model...
epoch: 5, total loss: 4.942214866479238
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.1544, Acc: 0.5380, AUC: 0.8318, F1: 0.4287
[VIT] val - Epoch: 5, Loss: 1.0838, Acc: 0.6739, AUC: 0.8253, F1: 0.3871
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8253
Saving vit model...
epoch: 6, total loss: 4.7714389860630035
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.8130, Acc: 0.6875, AUC: 0.8677, F1: 0.4824
[VIT] val - Epoch: 6, Loss: 1.0751, Acc: 0.6848, AUC: 0.8320, F1: 0.4295
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8320
Saving vit model...
epoch: 7, total loss: 4.438259581724803
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.0109, Acc: 0.5190, AUC: 0.8902, F1: 0.3371
[VIT] val - Epoch: 7, Loss: 1.0327, Acc: 0.7011, AUC: 0.8376, F1: 0.4716
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8902
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8376
Saving vit model...
epoch: 8, total loss: 5.078606396913528
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.9163, Acc: 0.6141, AUC: 0.8805, F1: 0.5261
[VIT] val - Epoch: 8, Loss: 1.0286, Acc: 0.6821, AUC: 0.8385, F1: 0.4519
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8385
Saving vit model...
epoch: 9, total loss: 4.4971982936064405
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.7283, Acc: 0.7228, AUC: 0.8824, F1: 0.5221
[VIT] val - Epoch: 9, Loss: 1.0090, Acc: 0.6957, AUC: 0.8436, F1: 0.4873
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8436
Saving vit model...
epoch: 10, total loss: 4.455256899197896
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.9035, Acc: 0.6821, AUC: 0.8867, F1: 0.5959
[VIT] val - Epoch: 10, Loss: 0.9736, Acc: 0.7120, AUC: 0.8444, F1: 0.4680
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8444
Saving vit model...
epoch: 11, total loss: 4.58999702334404
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.9334, Acc: 0.5625, AUC: 0.8760, F1: 0.4029
[VIT] val - Epoch: 11, Loss: 0.9632, Acc: 0.7065, AUC: 0.8464, F1: 0.4960
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8464
Saving vit model...
epoch: 12, total loss: 4.506246676047643
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.8351, Acc: 0.6630, AUC: 0.8690, F1: 0.4903
[VIT] val - Epoch: 12, Loss: 0.9766, Acc: 0.7038, AUC: 0.8470, F1: 0.4970
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8470
Saving vit model...
epoch: 13, total loss: 4.184673955043157
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.8349, Acc: 0.6712, AUC: 0.8818, F1: 0.5906
[VIT] val - Epoch: 13, Loss: 0.9503, Acc: 0.6984, AUC: 0.8490, F1: 0.4470
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8490
Saving vit model...
epoch: 14, total loss: 4.665604869524638
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.7721, Acc: 0.6821, AUC: 0.9041, F1: 0.5768
[VIT] val - Epoch: 14, Loss: 0.9521, Acc: 0.7065, AUC: 0.8510, F1: 0.5286
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9041
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8510
Saving vit model...
epoch: 15, total loss: 4.005247662464778
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.7404, Acc: 0.7473, AUC: 0.8961, F1: 0.6330
[VIT] val - Epoch: 15, Loss: 0.9432, Acc: 0.6848, AUC: 0.8527, F1: 0.4872
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8527
Saving vit model...
epoch: 16, total loss: 4.251861264308293
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.7869, Acc: 0.6658, AUC: 0.9052, F1: 0.5129
[VIT] val - Epoch: 16, Loss: 0.9079, Acc: 0.7120, AUC: 0.8568, F1: 0.5034
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9052
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8568
Saving vit model...
epoch: 17, total loss: 4.071786065896352
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.7371, Acc: 0.7473, AUC: 0.8957, F1: 0.6258
[VIT] val - Epoch: 17, Loss: 0.9458, Acc: 0.6793, AUC: 0.8562, F1: 0.5046
epoch: 18, total loss: 4.361759394407272
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.8082, Acc: 0.6658, AUC: 0.9081, F1: 0.5803
[VIT] val - Epoch: 18, Loss: 0.9099, Acc: 0.6984, AUC: 0.8570, F1: 0.4891
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9081
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8570
Saving vit model...
epoch: 19, total loss: 4.048778831958771
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.7904, Acc: 0.6848, AUC: 0.9056, F1: 0.6042
[VIT] val - Epoch: 19, Loss: 0.9233, Acc: 0.6793, AUC: 0.8568, F1: 0.4863
epoch: 20, total loss: 3.9807993670304618
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.9238, Acc: 0.5870, AUC: 0.9012, F1: 0.5369
[VIT] val - Epoch: 20, Loss: 0.9000, Acc: 0.6821, AUC: 0.8571, F1: 0.4804
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8571
Saving vit model...
epoch: 21, total loss: 3.938808341821035
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.7289, Acc: 0.7391, AUC: 0.8995, F1: 0.6111
[VIT] val - Epoch: 21, Loss: 0.9028, Acc: 0.7011, AUC: 0.8571, F1: 0.5099
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8571
Saving vit model...
epoch: 22, total loss: 3.9078490336736045
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.8352, Acc: 0.6658, AUC: 0.8968, F1: 0.5976
[VIT] val - Epoch: 22, Loss: 0.9336, Acc: 0.6766, AUC: 0.8578, F1: 0.5160
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8578
Saving vit model...
epoch: 23, total loss: 3.8649681905905404
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 1.0359, Acc: 0.5924, AUC: 0.8793, F1: 0.5263
[VIT] val - Epoch: 23, Loss: 0.9146, Acc: 0.6739, AUC: 0.8590, F1: 0.5034
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8590
Saving vit model...
epoch: 24, total loss: 4.388971984386444
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.7421, Acc: 0.7201, AUC: 0.8898, F1: 0.5788
[VIT] val - Epoch: 24, Loss: 0.9038, Acc: 0.6984, AUC: 0.8598, F1: 0.5324
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8598
Saving vit model...
epoch: 25, total loss: 3.965354800224304
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.7909, Acc: 0.6685, AUC: 0.8999, F1: 0.5429
[VIT] val - Epoch: 25, Loss: 0.8948, Acc: 0.6902, AUC: 0.8588, F1: 0.4977
epoch: 26, total loss: 3.662722647190094
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.8740, Acc: 0.6522, AUC: 0.9008, F1: 0.5501
[VIT] val - Epoch: 26, Loss: 0.8805, Acc: 0.6984, AUC: 0.8588, F1: 0.5139
epoch: 27, total loss: 3.7776004672050476
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.7496, Acc: 0.7065, AUC: 0.9097, F1: 0.6350
[VIT] val - Epoch: 27, Loss: 0.9465, Acc: 0.6495, AUC: 0.8585, F1: 0.5264
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9097
Saving cnn model...
epoch: 28, total loss: 3.888945132493973
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.7057, Acc: 0.7255, AUC: 0.9075, F1: 0.6118
[VIT] val - Epoch: 28, Loss: 0.8663, Acc: 0.6984, AUC: 0.8600, F1: 0.5088
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8600
Saving vit model...
epoch: 29, total loss: 3.8388048509756723
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.9979, Acc: 0.6087, AUC: 0.8935, F1: 0.5219
[VIT] val - Epoch: 29, Loss: 0.9464, Acc: 0.6630, AUC: 0.8580, F1: 0.5280
epoch: 30, total loss: 3.5934885144233704
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.6771, Acc: 0.7337, AUC: 0.9045, F1: 0.6258
[VIT] val - Epoch: 30, Loss: 0.8767, Acc: 0.6957, AUC: 0.8595, F1: 0.5182
epoch: 31, total loss: 3.809661547342936
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.8715, Acc: 0.6495, AUC: 0.8968, F1: 0.5529
[VIT] val - Epoch: 31, Loss: 0.8604, Acc: 0.6957, AUC: 0.8617, F1: 0.5133
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8617
Saving vit model...
epoch: 32, total loss: 3.6387725422779718
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.7042, Acc: 0.7500, AUC: 0.8956, F1: 0.6684
[VIT] val - Epoch: 32, Loss: 0.8598, Acc: 0.7011, AUC: 0.8618, F1: 0.4766
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8618
Saving vit model...
epoch: 33, total loss: 3.8730254471302032
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.6822, Acc: 0.7418, AUC: 0.9072, F1: 0.6514
[VIT] val - Epoch: 33, Loss: 0.8812, Acc: 0.6957, AUC: 0.8627, F1: 0.5566
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8627
Saving vit model...
epoch: 34, total loss: 4.2331802546978
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.7072, Acc: 0.7201, AUC: 0.8987, F1: 0.6185
[VIT] val - Epoch: 34, Loss: 0.8587, Acc: 0.6929, AUC: 0.8616, F1: 0.5051
epoch: 35, total loss: 3.7157347202301025
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.7930, Acc: 0.7065, AUC: 0.8908, F1: 0.6219
[VIT] val - Epoch: 35, Loss: 0.8567, Acc: 0.7011, AUC: 0.8616, F1: 0.5206
epoch: 36, total loss: 3.6583539644877114
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.7481, Acc: 0.7011, AUC: 0.9010, F1: 0.6154
[VIT] val - Epoch: 36, Loss: 0.8706, Acc: 0.6875, AUC: 0.8616, F1: 0.5318
epoch: 37, total loss: 3.461328307787577
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.9106, Acc: 0.6223, AUC: 0.8931, F1: 0.5191
[VIT] val - Epoch: 37, Loss: 0.9170, Acc: 0.6603, AUC: 0.8637, F1: 0.5312
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8637
Saving vit model...
epoch: 38, total loss: 3.821842556198438
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 1.1552, Acc: 0.4918, AUC: 0.8766, F1: 0.4454
[VIT] val - Epoch: 38, Loss: 0.8682, Acc: 0.6875, AUC: 0.8639, F1: 0.5379
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8639
Saving vit model...
epoch: 39, total loss: 3.6850992242495217
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.7767, Acc: 0.7092, AUC: 0.9038, F1: 0.6350
[VIT] val - Epoch: 39, Loss: 0.8893, Acc: 0.6957, AUC: 0.8643, F1: 0.5569
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8643
Saving vit model...
epoch: 40, total loss: 3.7430896858374276
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.8239, Acc: 0.6848, AUC: 0.9009, F1: 0.5497
[VIT] val - Epoch: 40, Loss: 0.8555, Acc: 0.7011, AUC: 0.8645, F1: 0.5279
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8645
Saving vit model...
epoch: 41, total loss: 3.4427683254083
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.8186, Acc: 0.6685, AUC: 0.8978, F1: 0.5954
[VIT] val - Epoch: 41, Loss: 0.8985, Acc: 0.6821, AUC: 0.8643, F1: 0.5383
epoch: 42, total loss: 3.5931907296180725
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.7767, Acc: 0.7174, AUC: 0.8864, F1: 0.6313
[VIT] val - Epoch: 42, Loss: 0.8916, Acc: 0.6766, AUC: 0.8637, F1: 0.5252
epoch: 43, total loss: 3.6344653566678367
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.7348, Acc: 0.7038, AUC: 0.8944, F1: 0.5816
[VIT] val - Epoch: 43, Loss: 0.8554, Acc: 0.7011, AUC: 0.8646, F1: 0.5317
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8646
Saving vit model...
epoch: 44, total loss: 3.608186503251394
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.7543, Acc: 0.7337, AUC: 0.8954, F1: 0.6601
[VIT] val - Epoch: 44, Loss: 0.8549, Acc: 0.6984, AUC: 0.8651, F1: 0.5469
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8651
Saving vit model...
epoch: 45, total loss: 4.055437684059143
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.7576, Acc: 0.7120, AUC: 0.8915, F1: 0.5962
[VIT] val - Epoch: 45, Loss: 0.8617, Acc: 0.6902, AUC: 0.8657, F1: 0.5314
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8657
Saving vit model...
epoch: 46, total loss: 3.741439789533615
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 1.0395, Acc: 0.5951, AUC: 0.8822, F1: 0.5016
[VIT] val - Epoch: 46, Loss: 0.8579, Acc: 0.6848, AUC: 0.8660, F1: 0.5352
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8660
Saving vit model...
epoch: 47, total loss: 3.7169353564580283
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.8637, Acc: 0.6848, AUC: 0.8881, F1: 0.5922
[VIT] val - Epoch: 47, Loss: 0.8463, Acc: 0.6902, AUC: 0.8654, F1: 0.4923
epoch: 48, total loss: 3.3807211816310883
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.7582, Acc: 0.7228, AUC: 0.9012, F1: 0.6143
[VIT] val - Epoch: 48, Loss: 0.8732, Acc: 0.6821, AUC: 0.8654, F1: 0.5407
epoch: 49, total loss: 3.4117378393809
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.7349, Acc: 0.7120, AUC: 0.8902, F1: 0.5836
[VIT] val - Epoch: 49, Loss: 0.8647, Acc: 0.6957, AUC: 0.8667, F1: 0.5366
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8667
Saving vit model...
epoch: 50, total loss: 3.3583440681298575
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.8048, Acc: 0.6929, AUC: 0.8935, F1: 0.6110
[VIT] val - Epoch: 50, Loss: 0.8589, Acc: 0.7038, AUC: 0.8660, F1: 0.5502
epoch: 51, total loss: 3.280414561430613
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.8263, Acc: 0.6848, AUC: 0.8992, F1: 0.6051
[VIT] val - Epoch: 51, Loss: 0.8438, Acc: 0.6902, AUC: 0.8664, F1: 0.5215
epoch: 52, total loss: 3.3747576475143433
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.9772, Acc: 0.6033, AUC: 0.8891, F1: 0.5412
[VIT] val - Epoch: 52, Loss: 0.8577, Acc: 0.6957, AUC: 0.8666, F1: 0.5386
epoch: 53, total loss: 3.4570698738098145
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.8393, Acc: 0.7011, AUC: 0.8963, F1: 0.5957
[VIT] val - Epoch: 53, Loss: 0.8280, Acc: 0.6929, AUC: 0.8670, F1: 0.5203
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8670
Saving vit model...
epoch: 54, total loss: 3.443640887737274
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.9167, Acc: 0.6576, AUC: 0.8975, F1: 0.6061
[VIT] val - Epoch: 54, Loss: 0.8534, Acc: 0.6902, AUC: 0.8663, F1: 0.5379
epoch: 55, total loss: 3.335588758190473
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.8650, Acc: 0.6685, AUC: 0.8968, F1: 0.6137
[VIT] val - Epoch: 55, Loss: 0.8364, Acc: 0.6984, AUC: 0.8658, F1: 0.5290
epoch: 56, total loss: 3.4652464787165322
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.8273, Acc: 0.6902, AUC: 0.8939, F1: 0.6234
[VIT] val - Epoch: 56, Loss: 0.8605, Acc: 0.6957, AUC: 0.8671, F1: 0.5463
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8671
Saving vit model...
epoch: 57, total loss: 3.3250861863295236
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.8146, Acc: 0.6984, AUC: 0.8953, F1: 0.5638
[VIT] val - Epoch: 57, Loss: 0.8553, Acc: 0.6875, AUC: 0.8663, F1: 0.5461
epoch: 58, total loss: 3.256558746099472
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.7934, Acc: 0.6848, AUC: 0.8949, F1: 0.6065
[VIT] val - Epoch: 58, Loss: 0.8547, Acc: 0.6957, AUC: 0.8675, F1: 0.5670
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8675
Saving vit model...
epoch: 59, total loss: 3.1406913002332053
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.8504, Acc: 0.6766, AUC: 0.9027, F1: 0.6079
[VIT] val - Epoch: 59, Loss: 0.8336, Acc: 0.7038, AUC: 0.8674, F1: 0.5476
epoch: 60, total loss: 3.4807472229003906
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.8072, Acc: 0.6739, AUC: 0.8900, F1: 0.5941
[VIT] val - Epoch: 60, Loss: 0.8404, Acc: 0.7011, AUC: 0.8676, F1: 0.5585
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8676
Saving vit model...
epoch: 61, total loss: 3.6631410717964172
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.7313, Acc: 0.7201, AUC: 0.9021, F1: 0.6261
[VIT] val - Epoch: 61, Loss: 0.8463, Acc: 0.6929, AUC: 0.8680, F1: 0.5646
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8680
Saving vit model...
epoch: 62, total loss: 3.010561337073644
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.7481, Acc: 0.7310, AUC: 0.8983, F1: 0.6200
[VIT] val - Epoch: 62, Loss: 0.8426, Acc: 0.7011, AUC: 0.8677, F1: 0.5575
epoch: 63, total loss: 3.33841809630394
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.8005, Acc: 0.7038, AUC: 0.8934, F1: 0.5940
[VIT] val - Epoch: 63, Loss: 0.8283, Acc: 0.7011, AUC: 0.8679, F1: 0.5381
epoch: 64, total loss: 3.2652244170506797
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.7979, Acc: 0.7201, AUC: 0.8835, F1: 0.6112
[VIT] val - Epoch: 64, Loss: 0.8604, Acc: 0.6766, AUC: 0.8693, F1: 0.5340
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8693
Saving vit model...
epoch: 65, total loss: 3.4495607713858285
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.8364, Acc: 0.7038, AUC: 0.8862, F1: 0.6059
[VIT] val - Epoch: 65, Loss: 0.8449, Acc: 0.6902, AUC: 0.8679, F1: 0.5393
epoch: 66, total loss: 3.162028670310974
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.8393, Acc: 0.6984, AUC: 0.8973, F1: 0.6311
[VIT] val - Epoch: 66, Loss: 0.8389, Acc: 0.7011, AUC: 0.8678, F1: 0.5528
epoch: 67, total loss: 3.1689270039399466
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.7555, Acc: 0.7174, AUC: 0.8919, F1: 0.6109
[VIT] val - Epoch: 67, Loss: 0.8327, Acc: 0.7065, AUC: 0.8675, F1: 0.5600
epoch: 68, total loss: 3.305276374022166
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.7921, Acc: 0.7228, AUC: 0.8984, F1: 0.6276
[VIT] val - Epoch: 68, Loss: 0.8404, Acc: 0.7038, AUC: 0.8678, F1: 0.5611
epoch: 69, total loss: 3.176597515741984
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.8671, Acc: 0.6984, AUC: 0.8828, F1: 0.5906
[VIT] val - Epoch: 69, Loss: 0.8444, Acc: 0.6957, AUC: 0.8687, F1: 0.5609
epoch: 70, total loss: 3.2729457318782806
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.9270, Acc: 0.6875, AUC: 0.8843, F1: 0.6211
[VIT] val - Epoch: 70, Loss: 0.8423, Acc: 0.6902, AUC: 0.8677, F1: 0.5480
epoch: 71, total loss: 3.4426206946372986
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.7529, Acc: 0.7147, AUC: 0.8983, F1: 0.6126
[VIT] val - Epoch: 71, Loss: 0.8441, Acc: 0.6984, AUC: 0.8685, F1: 0.5618
epoch: 72, total loss: 3.521730283896128
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.7982, Acc: 0.7174, AUC: 0.8937, F1: 0.6347
[VIT] val - Epoch: 72, Loss: 0.8319, Acc: 0.7011, AUC: 0.8684, F1: 0.5555
epoch: 73, total loss: 3.0378558337688446
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.7473, Acc: 0.7201, AUC: 0.8980, F1: 0.6176
[VIT] val - Epoch: 73, Loss: 0.8275, Acc: 0.6984, AUC: 0.8689, F1: 0.5499
epoch: 74, total loss: 3.2519918382167816
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.7569, Acc: 0.7310, AUC: 0.8875, F1: 0.6130
[VIT] val - Epoch: 74, Loss: 0.8457, Acc: 0.6929, AUC: 0.8694, F1: 0.5629
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8694
Saving vit model...
epoch: 75, total loss: 3.4865952531496682
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.7491, Acc: 0.7120, AUC: 0.8995, F1: 0.6265
[VIT] val - Epoch: 75, Loss: 0.8476, Acc: 0.6821, AUC: 0.8689, F1: 0.5403
epoch: 76, total loss: 3.245951915780703
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.8934, Acc: 0.6495, AUC: 0.8914, F1: 0.5767
[VIT] val - Epoch: 76, Loss: 0.8502, Acc: 0.7011, AUC: 0.8694, F1: 0.5773
epoch: 77, total loss: 3.230769286553065
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.7624, Acc: 0.7255, AUC: 0.8901, F1: 0.6315
[VIT] val - Epoch: 77, Loss: 0.8339, Acc: 0.7092, AUC: 0.8692, F1: 0.5634
epoch: 78, total loss: 3.1479433675607047
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.7819, Acc: 0.7174, AUC: 0.8904, F1: 0.5993
[VIT] val - Epoch: 78, Loss: 0.8369, Acc: 0.6984, AUC: 0.8684, F1: 0.5435
epoch: 79, total loss: 3.1917688846588135
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.9223, Acc: 0.6413, AUC: 0.8913, F1: 0.5733
[VIT] val - Epoch: 79, Loss: 0.8554, Acc: 0.6875, AUC: 0.8692, F1: 0.5573
epoch: 80, total loss: 3.231867710749308
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.8111, Acc: 0.7011, AUC: 0.8968, F1: 0.6022
[VIT] val - Epoch: 80, Loss: 0.8248, Acc: 0.6957, AUC: 0.8684, F1: 0.5330
epoch: 81, total loss: 3.3030394911766052
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.9094, Acc: 0.6821, AUC: 0.8792, F1: 0.5913
[VIT] val - Epoch: 81, Loss: 0.8534, Acc: 0.6957, AUC: 0.8681, F1: 0.5732
epoch: 82, total loss: 2.880377391974131
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.7469, Acc: 0.7364, AUC: 0.9020, F1: 0.6495
[VIT] val - Epoch: 82, Loss: 0.8242, Acc: 0.7011, AUC: 0.8682, F1: 0.5456
epoch: 83, total loss: 3.360270311435064
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.7600, Acc: 0.7065, AUC: 0.8992, F1: 0.5992
[VIT] val - Epoch: 83, Loss: 0.8435, Acc: 0.6929, AUC: 0.8677, F1: 0.5393
epoch: 84, total loss: 3.1998944779237113
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.8205, Acc: 0.7038, AUC: 0.8949, F1: 0.6175
[VIT] val - Epoch: 84, Loss: 0.8338, Acc: 0.6902, AUC: 0.8683, F1: 0.5404
epoch: 85, total loss: 2.9522951543331146
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.9462, Acc: 0.6359, AUC: 0.8873, F1: 0.5647
[VIT] val - Epoch: 85, Loss: 0.8328, Acc: 0.7092, AUC: 0.8684, F1: 0.5549
epoch: 86, total loss: 3.0061691254377365
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.7579, Acc: 0.7283, AUC: 0.9022, F1: 0.6280
[VIT] val - Epoch: 86, Loss: 0.8253, Acc: 0.6957, AUC: 0.8684, F1: 0.5356
epoch: 87, total loss: 3.1680196722348533
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.7972, Acc: 0.7038, AUC: 0.8947, F1: 0.6250
[VIT] val - Epoch: 87, Loss: 0.8467, Acc: 0.6902, AUC: 0.8681, F1: 0.5499
epoch: 88, total loss: 3.0467773973941803
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.7293, Acc: 0.7418, AUC: 0.8947, F1: 0.6291
[VIT] val - Epoch: 88, Loss: 0.8371, Acc: 0.7065, AUC: 0.8681, F1: 0.5681
epoch: 89, total loss: 3.3072971800963082
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.9236, Acc: 0.6630, AUC: 0.8818, F1: 0.5859
[VIT] val - Epoch: 89, Loss: 0.8440, Acc: 0.6875, AUC: 0.8674, F1: 0.5436
epoch: 90, total loss: 3.25997386376063
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.7948, Acc: 0.7065, AUC: 0.8923, F1: 0.6003
[VIT] val - Epoch: 90, Loss: 0.8464, Acc: 0.6984, AUC: 0.8675, F1: 0.5582
epoch: 91, total loss: 3.212727665901184
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.8512, Acc: 0.6658, AUC: 0.8934, F1: 0.5811
[VIT] val - Epoch: 91, Loss: 0.8335, Acc: 0.7065, AUC: 0.8683, F1: 0.5673
epoch: 92, total loss: 3.053719441095988
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.7463, Acc: 0.7201, AUC: 0.8965, F1: 0.5830
[VIT] val - Epoch: 92, Loss: 0.8391, Acc: 0.6984, AUC: 0.8682, F1: 0.5569
epoch: 93, total loss: 2.908358857035637
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.8543, Acc: 0.6576, AUC: 0.8974, F1: 0.5540
[VIT] val - Epoch: 93, Loss: 0.8320, Acc: 0.7011, AUC: 0.8687, F1: 0.5504
epoch: 94, total loss: 3.182847186923027
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.8024, Acc: 0.7065, AUC: 0.8923, F1: 0.6105
[VIT] val - Epoch: 94, Loss: 0.8393, Acc: 0.7011, AUC: 0.8689, F1: 0.5567
epoch: 95, total loss: 2.997615228096644
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.7785, Acc: 0.7174, AUC: 0.9017, F1: 0.6335
[VIT] val - Epoch: 95, Loss: 0.8309, Acc: 0.7147, AUC: 0.8684, F1: 0.5661
epoch: 96, total loss: 3.1320815483729043
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.8336, Acc: 0.6766, AUC: 0.8928, F1: 0.6014
[VIT] val - Epoch: 96, Loss: 0.8353, Acc: 0.7092, AUC: 0.8692, F1: 0.5625
epoch: 97, total loss: 3.1851945221424103
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.8263, Acc: 0.6957, AUC: 0.8922, F1: 0.6184
[VIT] val - Epoch: 97, Loss: 0.8646, Acc: 0.6821, AUC: 0.8696, F1: 0.5610
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8696
Saving vit model...
epoch: 98, total loss: 2.894215618570646
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.7861, Acc: 0.7174, AUC: 0.8935, F1: 0.5978
[VIT] val - Epoch: 98, Loss: 0.8281, Acc: 0.7174, AUC: 0.8698, F1: 0.5800
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8698
Saving vit model...
epoch: 99, total loss: 3.0614439845085144
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.8309, Acc: 0.7065, AUC: 0.8929, F1: 0.6159
[VIT] val - Epoch: 99, Loss: 0.8257, Acc: 0.6848, AUC: 0.8692, F1: 0.5368
epoch: 100, total loss: 2.9681966602802277
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.7736, Acc: 0.7147, AUC: 0.8973, F1: 0.6127
[VIT] val - Epoch: 100, Loss: 0.8399, Acc: 0.6902, AUC: 0.8697, F1: 0.5617
[20:11:49][Rank 2] Training Finished. Starting Final Testing...
[20:11:49][Rank 3] Training Finished. Starting Final Testing...[20:11:49][Rank 1] Training Finished. Starting Final Testing...

[20:11:49][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 2.3583, Acc: 0.2330, AUC: 0.7223, F1: 0.1524
[VIT] test - Epoch: 100, Loss: 1.7611, Acc: 0.2330, AUC: 0.7200, F1: 0.1499
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 2.8708, Acc: 0.0937, AUC: 0.7172, F1: 0.0994
[VIT] test - Epoch: 100, Loss: 2.1351, Acc: 0.1611, AUC: 0.7184, F1: 0.1173
‚úÖ [ÂÆåÊàê] Ê∫êÂüü FGADR ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: IDRID
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='IDRID', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['IDRID']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_IDRID
OUT_DIR: ./output_esdg_h100/IDRID
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified4/output_esdg_h100/IDRID/CASS_GDRNet_ESDG_IDRID
[20:19:07][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
[20:19:07][Rank 1] Loading datasets...
[20:19:07][Rank 3] Loading datasets...
[20:19:07][Rank 2] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.mlp.up_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.0.mlp.down_projInjecting LoRA into: layer.2.attention.o_proj

Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.mlp.up_projInjecting LoRA into: layer.4.mlp.up_proj

Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.mlp.up_projInjecting LoRA into: layer.5.mlp.up_proj

Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.8.attention.k_projInjecting LoRA into: layer.8.attention.q_proj

Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.10.attention.q_projInjecting LoRA into: layer.11.mlp.down_proj

Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.11.attention.k_proj
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.11.attention.v_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 5.774553843906948
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.9843, Acc: 0.3592, AUC: 0.6070, F1: 0.1065
[VIT] val - Epoch: 1, Loss: 1.4510, Acc: 0.3786, AUC: 0.5875, F1: 0.1771
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6070
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5875
Saving vit model...
epoch: 2, total loss: 4.889753546033587
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.5306, Acc: 0.1845, AUC: 0.7655, F1: 0.1151
[VIT] val - Epoch: 2, Loss: 1.3880, Acc: 0.4660, AUC: 0.6952, F1: 0.2259
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7655
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6952
Saving vit model...
epoch: 3, total loss: 4.713738305228097
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 1.1110, Acc: 0.5243, AUC: 0.7615, F1: 0.2454
[VIT] val - Epoch: 3, Loss: 1.3953, Acc: 0.4563, AUC: 0.7666, F1: 0.2212
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7666
Saving vit model...
epoch: 4, total loss: 4.50406721660069
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 1.2128, Acc: 0.4854, AUC: 0.7694, F1: 0.2936
[VIT] val - Epoch: 4, Loss: 1.4083, Acc: 0.5049, AUC: 0.7889, F1: 0.3061
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7694
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7889
Saving vit model...
epoch: 5, total loss: 4.451484135219029
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.9802, Acc: 0.6214, AUC: 0.7220, F1: 0.3960
[VIT] val - Epoch: 5, Loss: 1.3220, Acc: 0.5243, AUC: 0.8056, F1: 0.3163
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8056
Saving vit model...
epoch: 6, total loss: 4.268216678074428
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.0284, Acc: 0.6117, AUC: 0.8216, F1: 0.3755
[VIT] val - Epoch: 6, Loss: 1.2726, Acc: 0.5922, AUC: 0.8121, F1: 0.3839
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8216
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8121
Saving vit model...
epoch: 7, total loss: 4.652631521224976
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.7966, Acc: 0.3398, AUC: 0.7564, F1: 0.1649
[VIT] val - Epoch: 7, Loss: 1.2459, Acc: 0.5340, AUC: 0.8169, F1: 0.3225
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8169
Saving vit model...
epoch: 8, total loss: 4.144132375717163
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 1.1905, Acc: 0.5825, AUC: 0.7689, F1: 0.2832
[VIT] val - Epoch: 8, Loss: 1.2468, Acc: 0.5534, AUC: 0.8200, F1: 0.3604
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8200
Saving vit model...
epoch: 9, total loss: 4.1387878486088345
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 1.0933, Acc: 0.5534, AUC: 0.8292, F1: 0.4134
[VIT] val - Epoch: 9, Loss: 1.2380, Acc: 0.5825, AUC: 0.8247, F1: 0.3982
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8292
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8247
Saving vit model...
epoch: 10, total loss: 4.78176668712071
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 1.1254, Acc: 0.4951, AUC: 0.8063, F1: 0.3269
[VIT] val - Epoch: 10, Loss: 1.2152, Acc: 0.5728, AUC: 0.8281, F1: 0.3979
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8281
Saving vit model...
epoch: 11, total loss: 5.349569116319929
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 1.0015, Acc: 0.6117, AUC: 0.8275, F1: 0.3796
[VIT] val - Epoch: 11, Loss: 1.2044, Acc: 0.5825, AUC: 0.8332, F1: 0.4310
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8332
Saving vit model...
epoch: 12, total loss: 5.251401015690395
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.9272, Acc: 0.6311, AUC: 0.8721, F1: 0.5497
[VIT] val - Epoch: 12, Loss: 1.1863, Acc: 0.5922, AUC: 0.8349, F1: 0.4167
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8721
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8349
Saving vit model...
epoch: 13, total loss: 5.126748902457101
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.0242, Acc: 0.5534, AUC: 0.8498, F1: 0.3845
[VIT] val - Epoch: 13, Loss: 1.1742, Acc: 0.5922, AUC: 0.8362, F1: 0.4084
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8362
Saving vit model...
epoch: 14, total loss: 5.222014631543841
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.9709, Acc: 0.6214, AUC: 0.8000, F1: 0.3904
[VIT] val - Epoch: 14, Loss: 1.1649, Acc: 0.6019, AUC: 0.8405, F1: 0.4227
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8405
Saving vit model...
epoch: 15, total loss: 5.247853755950928
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.8917, Acc: 0.6019, AUC: 0.8584, F1: 0.4189
[VIT] val - Epoch: 15, Loss: 1.1417, Acc: 0.6019, AUC: 0.8442, F1: 0.4198
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8442
Saving vit model...
epoch: 16, total loss: 4.925917421068464
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.8537, Acc: 0.6214, AUC: 0.8408, F1: 0.4074
[VIT] val - Epoch: 16, Loss: 1.1403, Acc: 0.6117, AUC: 0.8485, F1: 0.4620
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8485
Saving vit model...
epoch: 17, total loss: 4.942919118063791
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.9390, Acc: 0.5825, AUC: 0.8418, F1: 0.4407
[VIT] val - Epoch: 17, Loss: 1.1270, Acc: 0.6117, AUC: 0.8481, F1: 0.4529
epoch: 18, total loss: 4.804967948368618
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.8164, Acc: 0.6699, AUC: 0.7771, F1: 0.4770
[VIT] val - Epoch: 18, Loss: 1.1000, Acc: 0.5922, AUC: 0.8502, F1: 0.4395
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8502
Saving vit model...
epoch: 19, total loss: 4.930350644247873
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.1764, Acc: 0.5340, AUC: 0.8408, F1: 0.4669
[VIT] val - Epoch: 19, Loss: 1.0969, Acc: 0.6019, AUC: 0.8503, F1: 0.4762
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8503
Saving vit model...
epoch: 20, total loss: 5.241030897412982
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.8513, Acc: 0.6311, AUC: 0.8776, F1: 0.4267
[VIT] val - Epoch: 20, Loss: 1.1128, Acc: 0.5825, AUC: 0.8526, F1: 0.4135
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8776
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8526
Saving vit model...
epoch: 21, total loss: 4.784691061292376
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.9460, Acc: 0.5534, AUC: 0.8510, F1: 0.3502
[VIT] val - Epoch: 21, Loss: 1.0889, Acc: 0.6214, AUC: 0.8523, F1: 0.4577
epoch: 22, total loss: 4.597041402544294
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 1.0373, Acc: 0.5825, AUC: 0.7699, F1: 0.3380
[VIT] val - Epoch: 22, Loss: 1.0478, Acc: 0.6408, AUC: 0.8494, F1: 0.4764
epoch: 23, total loss: 4.569361822945731
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 1.1426, Acc: 0.5825, AUC: 0.8145, F1: 0.2843
[VIT] val - Epoch: 23, Loss: 1.0460, Acc: 0.6117, AUC: 0.8516, F1: 0.4491
epoch: 24, total loss: 4.987520558493478
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 1.0371, Acc: 0.5534, AUC: 0.8239, F1: 0.3600
[VIT] val - Epoch: 24, Loss: 1.0568, Acc: 0.6019, AUC: 0.8557, F1: 0.4444
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8557
Saving vit model...
epoch: 25, total loss: 4.740389278956822
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 1.0048, Acc: 0.5728, AUC: 0.8607, F1: 0.3462
[VIT] val - Epoch: 25, Loss: 1.0640, Acc: 0.5922, AUC: 0.8580, F1: 0.4585
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8580
Saving vit model...
epoch: 26, total loss: 4.921630859375
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 1.2502, Acc: 0.4660, AUC: 0.8539, F1: 0.3975
[VIT] val - Epoch: 26, Loss: 1.0412, Acc: 0.6214, AUC: 0.8565, F1: 0.4930
epoch: 27, total loss: 4.810818263462612
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.8746, Acc: 0.6019, AUC: 0.8280, F1: 0.5387
[VIT] val - Epoch: 27, Loss: 1.0393, Acc: 0.6408, AUC: 0.8550, F1: 0.4944
epoch: 28, total loss: 4.514228480202811
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.7943, Acc: 0.6893, AUC: 0.8594, F1: 0.5167
[VIT] val - Epoch: 28, Loss: 1.0303, Acc: 0.6117, AUC: 0.8542, F1: 0.4613
epoch: 29, total loss: 4.29985100882394
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.9325, Acc: 0.6019, AUC: 0.8254, F1: 0.4646
[VIT] val - Epoch: 29, Loss: 1.0285, Acc: 0.5922, AUC: 0.8560, F1: 0.4513
epoch: 30, total loss: 4.602700063160488
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 1.0581, Acc: 0.4660, AUC: 0.8862, F1: 0.4202
[VIT] val - Epoch: 30, Loss: 1.0304, Acc: 0.5728, AUC: 0.8577, F1: 0.4491
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8862
Saving cnn model...
epoch: 31, total loss: 4.484904970441546
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.7911, Acc: 0.6699, AUC: 0.8798, F1: 0.5187
[VIT] val - Epoch: 31, Loss: 1.0224, Acc: 0.6505, AUC: 0.8580, F1: 0.5142
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8580
Saving vit model...
epoch: 32, total loss: 4.371042081287929
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.8008, Acc: 0.6990, AUC: 0.8639, F1: 0.4753
[VIT] val - Epoch: 32, Loss: 1.0168, Acc: 0.6117, AUC: 0.8576, F1: 0.4630
epoch: 33, total loss: 4.960172380719866
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.8827, Acc: 0.6699, AUC: 0.8387, F1: 0.4248
[VIT] val - Epoch: 33, Loss: 1.0181, Acc: 0.6214, AUC: 0.8574, F1: 0.4842
epoch: 34, total loss: 4.590907846178327
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.8800, Acc: 0.6408, AUC: 0.8536, F1: 0.5380
[VIT] val - Epoch: 34, Loss: 1.0190, Acc: 0.6019, AUC: 0.8582, F1: 0.4646
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8582
Saving vit model...
epoch: 35, total loss: 4.300359180995396
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.8728, Acc: 0.6311, AUC: 0.8615, F1: 0.4394
[VIT] val - Epoch: 35, Loss: 1.0239, Acc: 0.6117, AUC: 0.8575, F1: 0.4775
epoch: 36, total loss: 4.611698150634766
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.8556, Acc: 0.6796, AUC: 0.8850, F1: 0.6151
[VIT] val - Epoch: 36, Loss: 1.0277, Acc: 0.6214, AUC: 0.8579, F1: 0.4948
epoch: 37, total loss: 4.102699075426374
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8828, Acc: 0.6699, AUC: 0.8637, F1: 0.4515
[VIT] val - Epoch: 37, Loss: 1.0403, Acc: 0.6214, AUC: 0.8586, F1: 0.4814
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8586
Saving vit model...
epoch: 38, total loss: 4.720477376665388
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.7918, Acc: 0.6505, AUC: 0.8810, F1: 0.4910
[VIT] val - Epoch: 38, Loss: 1.0151, Acc: 0.6019, AUC: 0.8579, F1: 0.4842
epoch: 39, total loss: 4.117891141346523
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.8412, Acc: 0.6311, AUC: 0.8766, F1: 0.4347
[VIT] val - Epoch: 39, Loss: 1.0205, Acc: 0.6117, AUC: 0.8587, F1: 0.4906
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8587
Saving vit model...
epoch: 40, total loss: 3.9505209922790527
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 1.0015, Acc: 0.5728, AUC: 0.8628, F1: 0.4581
[VIT] val - Epoch: 40, Loss: 1.0194, Acc: 0.5922, AUC: 0.8605, F1: 0.4575
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8605
Saving vit model...
epoch: 41, total loss: 4.091899735586984
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 1.1361, Acc: 0.5340, AUC: 0.8352, F1: 0.4505
[VIT] val - Epoch: 41, Loss: 1.0095, Acc: 0.6311, AUC: 0.8570, F1: 0.4908
epoch: 42, total loss: 4.517212697437832
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.8883, Acc: 0.6602, AUC: 0.8506, F1: 0.5505
[VIT] val - Epoch: 42, Loss: 1.0012, Acc: 0.6311, AUC: 0.8555, F1: 0.5019
epoch: 43, total loss: 4.101676395961216
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.8721, Acc: 0.6117, AUC: 0.8833, F1: 0.4435
[VIT] val - Epoch: 43, Loss: 0.9862, Acc: 0.6214, AUC: 0.8518, F1: 0.4867
epoch: 44, total loss: 4.162708759307861
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.8908, Acc: 0.6602, AUC: 0.8602, F1: 0.4317
[VIT] val - Epoch: 44, Loss: 0.9776, Acc: 0.6214, AUC: 0.8522, F1: 0.4778
epoch: 45, total loss: 4.240164041519165
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.8457, Acc: 0.6990, AUC: 0.8496, F1: 0.4984
[VIT] val - Epoch: 45, Loss: 0.9885, Acc: 0.6311, AUC: 0.8541, F1: 0.4979
epoch: 46, total loss: 4.126665319715228
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.9298, Acc: 0.6408, AUC: 0.8355, F1: 0.5008
[VIT] val - Epoch: 46, Loss: 0.9935, Acc: 0.6019, AUC: 0.8549, F1: 0.4527
epoch: 47, total loss: 3.7835417134421214
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.9984, Acc: 0.5825, AUC: 0.8685, F1: 0.5082
[VIT] val - Epoch: 47, Loss: 0.9914, Acc: 0.6117, AUC: 0.8563, F1: 0.4632
epoch: 48, total loss: 4.501021180834089
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.7794, Acc: 0.6893, AUC: 0.8793, F1: 0.5078
[VIT] val - Epoch: 48, Loss: 0.9763, Acc: 0.6311, AUC: 0.8559, F1: 0.4970
epoch: 49, total loss: 4.189590113503592
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.9698, Acc: 0.6214, AUC: 0.8679, F1: 0.4314
[VIT] val - Epoch: 49, Loss: 0.9679, Acc: 0.6311, AUC: 0.8552, F1: 0.4989
epoch: 50, total loss: 4.294368437358311
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 1.0821, Acc: 0.5534, AUC: 0.8608, F1: 0.4483
[VIT] val - Epoch: 50, Loss: 0.9740, Acc: 0.5922, AUC: 0.8550, F1: 0.4483
epoch: 51, total loss: 3.999108246394566
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 1.0310, Acc: 0.5631, AUC: 0.8758, F1: 0.4817
[VIT] val - Epoch: 51, Loss: 0.9800, Acc: 0.6117, AUC: 0.8549, F1: 0.4792
epoch: 52, total loss: 4.195590223584857
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.7876, Acc: 0.6699, AUC: 0.8724, F1: 0.4833
[VIT] val - Epoch: 52, Loss: 0.9804, Acc: 0.5922, AUC: 0.8550, F1: 0.4485
epoch: 53, total loss: 4.746986593518939
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.8423, Acc: 0.6311, AUC: 0.8794, F1: 0.4881
[VIT] val - Epoch: 53, Loss: 0.9722, Acc: 0.6311, AUC: 0.8559, F1: 0.4938
epoch: 54, total loss: 3.965246779578073
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.9391, Acc: 0.6019, AUC: 0.8862, F1: 0.4905
[VIT] val - Epoch: 54, Loss: 0.9791, Acc: 0.6311, AUC: 0.8570, F1: 0.5009
epoch: 55, total loss: 3.5984339714050293
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.8857, Acc: 0.5728, AUC: 0.8709, F1: 0.4320
[VIT] val - Epoch: 55, Loss: 0.9681, Acc: 0.6019, AUC: 0.8581, F1: 0.4676
epoch: 56, total loss: 4.088547366006034
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.8282, Acc: 0.6214, AUC: 0.8741, F1: 0.4985
[VIT] val - Epoch: 56, Loss: 0.9761, Acc: 0.5922, AUC: 0.8583, F1: 0.4527
epoch: 57, total loss: 4.272684063230242
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.9123, Acc: 0.6214, AUC: 0.8512, F1: 0.4749
[VIT] val - Epoch: 57, Loss: 0.9915, Acc: 0.6019, AUC: 0.8571, F1: 0.4575
epoch: 58, total loss: 4.004118919372559
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 1.0709, Acc: 0.5631, AUC: 0.8633, F1: 0.4828
[VIT] val - Epoch: 58, Loss: 0.9831, Acc: 0.6214, AUC: 0.8573, F1: 0.4874
epoch: 59, total loss: 4.092503615788051
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 1.0050, Acc: 0.5922, AUC: 0.8735, F1: 0.5031
[VIT] val - Epoch: 59, Loss: 0.9747, Acc: 0.6117, AUC: 0.8577, F1: 0.4801
epoch: 60, total loss: 3.927633353642055
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.8458, Acc: 0.6796, AUC: 0.8768, F1: 0.6127
[VIT] val - Epoch: 60, Loss: 0.9695, Acc: 0.6214, AUC: 0.8567, F1: 0.4918
epoch: 61, total loss: 4.14176344871521
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.9074, Acc: 0.6019, AUC: 0.8733, F1: 0.4918
[VIT] val - Epoch: 61, Loss: 0.9665, Acc: 0.6019, AUC: 0.8554, F1: 0.4687
epoch: 62, total loss: 4.111059461321149
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.9267, Acc: 0.6214, AUC: 0.8985, F1: 0.5237
[VIT] val - Epoch: 62, Loss: 0.9720, Acc: 0.6214, AUC: 0.8563, F1: 0.4887
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8985
Saving cnn model...
epoch: 63, total loss: 4.524263075419834
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.8995, Acc: 0.6408, AUC: 0.9004, F1: 0.5224
[VIT] val - Epoch: 63, Loss: 0.9573, Acc: 0.6019, AUC: 0.8566, F1: 0.4729
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9004
Saving cnn model...
epoch: 64, total loss: 3.639469657625471
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.9753, Acc: 0.5825, AUC: 0.8983, F1: 0.5166
[VIT] val - Epoch: 64, Loss: 0.9490, Acc: 0.5825, AUC: 0.8567, F1: 0.4478
epoch: 65, total loss: 4.240035908562796
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.9244, Acc: 0.6311, AUC: 0.8941, F1: 0.5911
[VIT] val - Epoch: 65, Loss: 0.9563, Acc: 0.5631, AUC: 0.8567, F1: 0.4161
epoch: 66, total loss: 3.885096107210432
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.8872, Acc: 0.6214, AUC: 0.8873, F1: 0.4962
[VIT] val - Epoch: 66, Loss: 0.9674, Acc: 0.5631, AUC: 0.8574, F1: 0.4161
epoch: 67, total loss: 3.857184784752982
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.9743, Acc: 0.6117, AUC: 0.8737, F1: 0.5463
[VIT] val - Epoch: 67, Loss: 0.9758, Acc: 0.5534, AUC: 0.8585, F1: 0.4073
epoch: 68, total loss: 4.008327484130859
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.9252, Acc: 0.6311, AUC: 0.8691, F1: 0.4612
[VIT] val - Epoch: 68, Loss: 0.9718, Acc: 0.5728, AUC: 0.8586, F1: 0.4259
epoch: 69, total loss: 4.53727092061724
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.8728, Acc: 0.6699, AUC: 0.8787, F1: 0.5381
[VIT] val - Epoch: 69, Loss: 0.9508, Acc: 0.5825, AUC: 0.8578, F1: 0.4309
epoch: 70, total loss: 3.832563945225307
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.9606, Acc: 0.6214, AUC: 0.8807, F1: 0.5204
[VIT] val - Epoch: 70, Loss: 0.9422, Acc: 0.5922, AUC: 0.8570, F1: 0.4389
epoch: 71, total loss: 4.203709091459002
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.9939, Acc: 0.6214, AUC: 0.8673, F1: 0.4512
[VIT] val - Epoch: 71, Loss: 0.9431, Acc: 0.6117, AUC: 0.8565, F1: 0.4741
epoch: 72, total loss: 4.38370064326695
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.8820, Acc: 0.6893, AUC: 0.8744, F1: 0.5882
[VIT] val - Epoch: 72, Loss: 0.9415, Acc: 0.6117, AUC: 0.8585, F1: 0.4792
epoch: 73, total loss: 3.7947424820491245
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.9500, Acc: 0.5922, AUC: 0.8650, F1: 0.4724
[VIT] val - Epoch: 73, Loss: 0.9449, Acc: 0.6117, AUC: 0.8589, F1: 0.4826
epoch: 74, total loss: 4.213875600269863
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.9730, Acc: 0.6214, AUC: 0.8662, F1: 0.4779
[VIT] val - Epoch: 74, Loss: 0.9496, Acc: 0.5922, AUC: 0.8592, F1: 0.4620
epoch: 75, total loss: 3.794184480394636
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.9307, Acc: 0.6311, AUC: 0.8739, F1: 0.4536
[VIT] val - Epoch: 75, Loss: 0.9476, Acc: 0.5631, AUC: 0.8588, F1: 0.4161
epoch: 76, total loss: 4.194032805306571
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 1.0593, Acc: 0.5825, AUC: 0.8761, F1: 0.4809
[VIT] val - Epoch: 76, Loss: 0.9605, Acc: 0.5631, AUC: 0.8596, F1: 0.4151
epoch: 77, total loss: 4.440877403531756
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.9674, Acc: 0.6408, AUC: 0.8903, F1: 0.5301
[VIT] val - Epoch: 77, Loss: 0.9699, Acc: 0.6019, AUC: 0.8595, F1: 0.4661
epoch: 78, total loss: 3.6476404326302663
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.9521, Acc: 0.6214, AUC: 0.8761, F1: 0.5406
[VIT] val - Epoch: 78, Loss: 0.9567, Acc: 0.5922, AUC: 0.8584, F1: 0.4569
epoch: 79, total loss: 3.8595167568751743
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.9902, Acc: 0.6117, AUC: 0.8706, F1: 0.5416
[VIT] val - Epoch: 79, Loss: 0.9485, Acc: 0.6019, AUC: 0.8581, F1: 0.4544
epoch: 80, total loss: 4.033233438219343
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.9196, Acc: 0.6602, AUC: 0.8752, F1: 0.5729
[VIT] val - Epoch: 80, Loss: 0.9371, Acc: 0.6019, AUC: 0.8560, F1: 0.4629
epoch: 81, total loss: 3.5523328440529958
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.8944, Acc: 0.6214, AUC: 0.8641, F1: 0.5585
[VIT] val - Epoch: 81, Loss: 0.9356, Acc: 0.5922, AUC: 0.8560, F1: 0.4585
epoch: 82, total loss: 3.7422317436763217
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.8449, Acc: 0.6408, AUC: 0.8625, F1: 0.4789
[VIT] val - Epoch: 82, Loss: 0.9418, Acc: 0.5922, AUC: 0.8576, F1: 0.4585
epoch: 83, total loss: 3.9870613643101285
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.8248, Acc: 0.6699, AUC: 0.8731, F1: 0.4981
[VIT] val - Epoch: 83, Loss: 0.9489, Acc: 0.5728, AUC: 0.8588, F1: 0.4318
epoch: 84, total loss: 3.7267019067491804
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.7639, Acc: 0.6796, AUC: 0.8822, F1: 0.4972
[VIT] val - Epoch: 84, Loss: 0.9455, Acc: 0.5922, AUC: 0.8590, F1: 0.4384
epoch: 85, total loss: 3.7151259013584683
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.7951, Acc: 0.6893, AUC: 0.8808, F1: 0.4972
[VIT] val - Epoch: 85, Loss: 0.9439, Acc: 0.6019, AUC: 0.8591, F1: 0.4539
epoch: 86, total loss: 3.485753740583147
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.7994, Acc: 0.7087, AUC: 0.8792, F1: 0.6628
[VIT] val - Epoch: 86, Loss: 0.9432, Acc: 0.5728, AUC: 0.8590, F1: 0.4207
epoch: 87, total loss: 3.9915759563446045
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.9109, Acc: 0.6602, AUC: 0.8657, F1: 0.5608
[VIT] val - Epoch: 87, Loss: 0.9443, Acc: 0.5922, AUC: 0.8598, F1: 0.4559
epoch: 88, total loss: 3.7210134097508023
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.8034, Acc: 0.6990, AUC: 0.8896, F1: 0.5831
[VIT] val - Epoch: 88, Loss: 0.9434, Acc: 0.6019, AUC: 0.8590, F1: 0.4741
epoch: 89, total loss: 3.2717739173344205
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.7859, Acc: 0.7087, AUC: 0.8871, F1: 0.5984
[VIT] val - Epoch: 89, Loss: 0.9401, Acc: 0.6311, AUC: 0.8587, F1: 0.4987
epoch: 90, total loss: 3.5157450267246793
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.8944, Acc: 0.6311, AUC: 0.8802, F1: 0.4725
[VIT] val - Epoch: 90, Loss: 0.9364, Acc: 0.6019, AUC: 0.8582, F1: 0.4741
epoch: 91, total loss: 3.736555508204869
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.7848, Acc: 0.6505, AUC: 0.8837, F1: 0.4724
[VIT] val - Epoch: 91, Loss: 0.9416, Acc: 0.6117, AUC: 0.8578, F1: 0.4742
epoch: 92, total loss: 3.9736362184797014
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.8451, Acc: 0.6699, AUC: 0.8845, F1: 0.5822
[VIT] val - Epoch: 92, Loss: 0.9450, Acc: 0.6019, AUC: 0.8588, F1: 0.4606
epoch: 93, total loss: 3.758406332560948
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.8738, Acc: 0.6408, AUC: 0.8711, F1: 0.5867
[VIT] val - Epoch: 93, Loss: 0.9377, Acc: 0.5825, AUC: 0.8583, F1: 0.4281
epoch: 94, total loss: 3.434246335710798
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.9107, Acc: 0.6117, AUC: 0.8677, F1: 0.4347
[VIT] val - Epoch: 94, Loss: 0.9342, Acc: 0.6117, AUC: 0.8586, F1: 0.4639
epoch: 95, total loss: 3.994568790708269
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.9326, Acc: 0.6214, AUC: 0.8669, F1: 0.4594
[VIT] val - Epoch: 95, Loss: 0.9432, Acc: 0.6214, AUC: 0.8589, F1: 0.4802
epoch: 96, total loss: 3.8596774509974887
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.9235, Acc: 0.6117, AUC: 0.8722, F1: 0.4651
[VIT] val - Epoch: 96, Loss: 0.9394, Acc: 0.6214, AUC: 0.8584, F1: 0.4852
epoch: 97, total loss: 4.2058077199118475
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.8575, Acc: 0.6505, AUC: 0.8699, F1: 0.5688
[VIT] val - Epoch: 97, Loss: 0.9337, Acc: 0.6214, AUC: 0.8581, F1: 0.4877
epoch: 98, total loss: 3.2552597522735596
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.8525, Acc: 0.6699, AUC: 0.8793, F1: 0.5894
[VIT] val - Epoch: 98, Loss: 0.9248, Acc: 0.6505, AUC: 0.8587, F1: 0.5029
epoch: 99, total loss: 3.9522952692849294
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.9545, Acc: 0.6117, AUC: 0.8740, F1: 0.4075
[VIT] val - Epoch: 99, Loss: 0.9233, Acc: 0.6117, AUC: 0.8590, F1: 0.4763
epoch: 100, total loss: 3.9711659295218333
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.8629, Acc: 0.6505, AUC: 0.8837, F1: 0.5012
[VIT] val - Epoch: 100, Loss: 0.9261, Acc: 0.6214, AUC: 0.8589, F1: 0.4852
[20:46:03][Rank 2] Training Finished. Starting Final Testing...
[20:46:03][Rank 3] Training Finished. Starting Final Testing...[20:46:03][Rank 1] Training Finished. Starting Final Testing...

[20:46:03][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.0465, Acc: 0.5946, AUC: 0.7751, F1: 0.4123
[VIT] test - Epoch: 100, Loss: 1.0487, Acc: 0.6047, AUC: 0.7514, F1: 0.3688
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.0885, Acc: 0.5584, AUC: 0.7757, F1: 0.4131
[VIT] test - Epoch: 100, Loss: 1.0998, Acc: 0.5963, AUC: 0.7520, F1: 0.3667
‚úÖ [ÂÆåÊàê] Ê∫êÂüü IDRID ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: MESSIDOR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='MESSIDOR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['MESSIDOR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_MESSIDOR
OUT_DIR: ./output_esdg_h100/MESSIDOR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified4/output_esdg_h100/MESSIDOR/CASS_GDRNet_ESDG_MESSIDOR
[20:53:18][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
[20:53:18][Rank 1] Loading datasets...
[20:53:18][Rank 2] Loading datasets...
[20:53:18][Rank 3] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.q_projInjecting LoRA into: layer.2.attention.v_proj

Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_projInjecting LoRA into: layer.10.attention.q_proj

Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.7.attention.k_projInjecting LoRA into: layer.5.mlp.up_proj

Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_projInjecting LoRA into: layer.7.mlp.down_proj

Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 4.47661817073822
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.0105, Acc: 0.6523, AUC: 0.8160, F1: 0.3201
[VIT] val - Epoch: 1, Loss: 1.1943, Acc: 0.5546, AUC: 0.6379, F1: 0.1427
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8160
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6379
Saving vit model...
epoch: 2, total loss: 3.941255807876587
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 0.9924, Acc: 0.6351, AUC: 0.8351, F1: 0.4652
[VIT] val - Epoch: 2, Loss: 1.2375, Acc: 0.5546, AUC: 0.7173, F1: 0.1427
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8351
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7173
Saving vit model...
epoch: 3, total loss: 4.006556424227628
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.8377, Acc: 0.6983, AUC: 0.8390, F1: 0.5143
[VIT] val - Epoch: 3, Loss: 1.1405, Acc: 0.5632, AUC: 0.7427, F1: 0.1592
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8390
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7427
Saving vit model...
epoch: 4, total loss: 4.852913162925026
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.8952, Acc: 0.6609, AUC: 0.7735, F1: 0.2911
[VIT] val - Epoch: 4, Loss: 1.1561, Acc: 0.5948, AUC: 0.7603, F1: 0.2273
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7603
Saving vit model...
epoch: 5, total loss: 5.081528490239924
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.8799, Acc: 0.6322, AUC: 0.8582, F1: 0.5032
[VIT] val - Epoch: 5, Loss: 1.1136, Acc: 0.5920, AUC: 0.7580, F1: 0.2220
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8582
Saving cnn model...
epoch: 6, total loss: 4.981823314319957
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.9877, Acc: 0.6092, AUC: 0.8828, F1: 0.4371
[VIT] val - Epoch: 6, Loss: 1.1024, Acc: 0.6063, AUC: 0.7713, F1: 0.2585
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8828
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7713
Saving vit model...
epoch: 7, total loss: 4.715896747329018
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.8127, Acc: 0.6810, AUC: 0.8873, F1: 0.4969
[VIT] val - Epoch: 7, Loss: 1.1063, Acc: 0.6034, AUC: 0.7755, F1: 0.2662
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8873
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7755
Saving vit model...
epoch: 8, total loss: 4.656500772996382
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.7861, Acc: 0.7126, AUC: 0.8909, F1: 0.5238
[VIT] val - Epoch: 8, Loss: 1.1169, Acc: 0.5747, AUC: 0.7850, F1: 0.2812
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8909
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7850
Saving vit model...
epoch: 9, total loss: 4.089928431944414
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.8180, Acc: 0.7213, AUC: 0.8901, F1: 0.5518
[VIT] val - Epoch: 9, Loss: 1.0703, Acc: 0.6034, AUC: 0.7761, F1: 0.3251
epoch: 10, total loss: 4.407555493441495
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.8129, Acc: 0.7040, AUC: 0.8266, F1: 0.4500
[VIT] val - Epoch: 10, Loss: 1.0565, Acc: 0.6207, AUC: 0.7844, F1: 0.3490
epoch: 11, total loss: 4.521443876353177
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.7892, Acc: 0.6925, AUC: 0.8505, F1: 0.5542
[VIT] val - Epoch: 11, Loss: 1.0661, Acc: 0.5805, AUC: 0.7841, F1: 0.4517
epoch: 12, total loss: 4.097099477594549
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.0173, Acc: 0.5546, AUC: 0.8969, F1: 0.5085
[VIT] val - Epoch: 12, Loss: 1.0583, Acc: 0.6006, AUC: 0.7922, F1: 0.3929
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8969
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7922
Saving vit model...
epoch: 13, total loss: 4.619071234356273
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.8151, Acc: 0.6839, AUC: 0.8945, F1: 0.6194
[VIT] val - Epoch: 13, Loss: 1.0443, Acc: 0.6149, AUC: 0.7949, F1: 0.4129
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7949
Saving vit model...
epoch: 14, total loss: 4.609794920141047
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.8493, Acc: 0.6839, AUC: 0.8325, F1: 0.4607
[VIT] val - Epoch: 14, Loss: 1.0478, Acc: 0.5948, AUC: 0.7943, F1: 0.4082
epoch: 15, total loss: 4.204089099710638
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.8080, Acc: 0.7098, AUC: 0.8507, F1: 0.4698
[VIT] val - Epoch: 15, Loss: 1.0386, Acc: 0.6006, AUC: 0.7990, F1: 0.4095
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7990
Saving vit model...
epoch: 16, total loss: 4.410905502059243
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 1.0154, Acc: 0.6006, AUC: 0.8970, F1: 0.5804
[VIT] val - Epoch: 16, Loss: 1.0257, Acc: 0.5920, AUC: 0.7942, F1: 0.3755
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8970
Saving cnn model...
epoch: 17, total loss: 4.27567038752816
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.7850, Acc: 0.6868, AUC: 0.9054, F1: 0.6241
[VIT] val - Epoch: 17, Loss: 1.0450, Acc: 0.5776, AUC: 0.7969, F1: 0.4204
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9054
Saving cnn model...
epoch: 18, total loss: 4.209087274291298
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.7770, Acc: 0.6925, AUC: 0.8853, F1: 0.5317
[VIT] val - Epoch: 18, Loss: 1.0380, Acc: 0.5833, AUC: 0.8022, F1: 0.4342
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8022
Saving vit model...
epoch: 19, total loss: 4.158213799650019
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.8041, Acc: 0.6724, AUC: 0.8968, F1: 0.5930
[VIT] val - Epoch: 19, Loss: 1.0184, Acc: 0.5891, AUC: 0.7979, F1: 0.4369
epoch: 20, total loss: 4.39460061896931
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.9779, Acc: 0.6006, AUC: 0.8608, F1: 0.5015
[VIT] val - Epoch: 20, Loss: 1.0168, Acc: 0.5977, AUC: 0.8055, F1: 0.4177
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8055
Saving vit model...
epoch: 21, total loss: 4.015060478990728
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.9809, Acc: 0.5747, AUC: 0.9128, F1: 0.5371
[VIT] val - Epoch: 21, Loss: 0.9953, Acc: 0.6149, AUC: 0.7981, F1: 0.4259
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9128
Saving cnn model...
epoch: 22, total loss: 4.408010428602045
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.7726, Acc: 0.6897, AUC: 0.8944, F1: 0.5737
[VIT] val - Epoch: 22, Loss: 1.0042, Acc: 0.6121, AUC: 0.8078, F1: 0.4348
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8078
Saving vit model...
epoch: 23, total loss: 4.046144485473633
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.7866, Acc: 0.6695, AUC: 0.8964, F1: 0.6230
[VIT] val - Epoch: 23, Loss: 1.0027, Acc: 0.5805, AUC: 0.8007, F1: 0.4623
epoch: 24, total loss: 3.9500888911160557
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.7712, Acc: 0.7040, AUC: 0.8815, F1: 0.4774
[VIT] val - Epoch: 24, Loss: 0.9987, Acc: 0.6006, AUC: 0.8103, F1: 0.4361
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8103
Saving vit model...
epoch: 25, total loss: 4.035743442448703
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.8334, Acc: 0.6552, AUC: 0.8922, F1: 0.5472
[VIT] val - Epoch: 25, Loss: 1.0007, Acc: 0.6178, AUC: 0.8095, F1: 0.4769
epoch: 26, total loss: 4.192709185860374
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 1.0347, Acc: 0.5402, AUC: 0.9078, F1: 0.5686
[VIT] val - Epoch: 26, Loss: 1.0215, Acc: 0.5833, AUC: 0.8120, F1: 0.4442
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8120
Saving vit model...
epoch: 27, total loss: 4.200601025061174
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.8165, Acc: 0.6466, AUC: 0.8995, F1: 0.5774
[VIT] val - Epoch: 27, Loss: 0.9894, Acc: 0.5948, AUC: 0.8067, F1: 0.4603
epoch: 28, total loss: 3.957818768241189
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.7240, Acc: 0.7126, AUC: 0.8944, F1: 0.5747
[VIT] val - Epoch: 28, Loss: 1.0017, Acc: 0.5920, AUC: 0.8173, F1: 0.4301
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8173
Saving vit model...
epoch: 29, total loss: 3.9421413161537866
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.8303, Acc: 0.6897, AUC: 0.8773, F1: 0.4789
[VIT] val - Epoch: 29, Loss: 1.0052, Acc: 0.6006, AUC: 0.8161, F1: 0.4912
epoch: 30, total loss: 4.109661058946089
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.7838, Acc: 0.7098, AUC: 0.8830, F1: 0.5656
[VIT] val - Epoch: 30, Loss: 0.9798, Acc: 0.6264, AUC: 0.8143, F1: 0.4619
epoch: 31, total loss: 4.002373933792114
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.8831, Acc: 0.6236, AUC: 0.9019, F1: 0.5172
[VIT] val - Epoch: 31, Loss: 0.9901, Acc: 0.6063, AUC: 0.8158, F1: 0.4948
epoch: 32, total loss: 3.8695267980748955
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.7229, Acc: 0.6983, AUC: 0.9020, F1: 0.6105
[VIT] val - Epoch: 32, Loss: 1.0000, Acc: 0.5862, AUC: 0.8174, F1: 0.4874
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8174
Saving vit model...
epoch: 33, total loss: 3.887484452941201
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.7679, Acc: 0.7155, AUC: 0.8747, F1: 0.5803
[VIT] val - Epoch: 33, Loss: 0.9966, Acc: 0.6063, AUC: 0.8193, F1: 0.4952
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8193
Saving vit model...
epoch: 34, total loss: 4.091904921965166
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.8121, Acc: 0.6609, AUC: 0.8999, F1: 0.6062
[VIT] val - Epoch: 34, Loss: 0.9729, Acc: 0.6006, AUC: 0.8111, F1: 0.4710
epoch: 35, total loss: 3.8503332138061523
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.8213, Acc: 0.6580, AUC: 0.8972, F1: 0.5868
[VIT] val - Epoch: 35, Loss: 0.9924, Acc: 0.6034, AUC: 0.8218, F1: 0.4858
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8218
Saving vit model...
epoch: 36, total loss: 4.16317786953666
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.8421, Acc: 0.6264, AUC: 0.8818, F1: 0.5853
[VIT] val - Epoch: 36, Loss: 0.9879, Acc: 0.6121, AUC: 0.8239, F1: 0.4610
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8239
Saving vit model...
epoch: 37, total loss: 4.124110308560458
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8747, Acc: 0.6753, AUC: 0.9116, F1: 0.5429
[VIT] val - Epoch: 37, Loss: 0.9798, Acc: 0.6063, AUC: 0.8207, F1: 0.4798
epoch: 38, total loss: 3.9910090186379175
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.8684, Acc: 0.6322, AUC: 0.9033, F1: 0.5777
[VIT] val - Epoch: 38, Loss: 1.0145, Acc: 0.5603, AUC: 0.8203, F1: 0.4790
epoch: 39, total loss: 4.211527943611145
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.8680, Acc: 0.6293, AUC: 0.8993, F1: 0.5998
[VIT] val - Epoch: 39, Loss: 0.9917, Acc: 0.5977, AUC: 0.8221, F1: 0.4601
epoch: 40, total loss: 4.173162839629433
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.7644, Acc: 0.6724, AUC: 0.9085, F1: 0.5796
[VIT] val - Epoch: 40, Loss: 0.9602, Acc: 0.6236, AUC: 0.8177, F1: 0.4308
epoch: 41, total loss: 3.742552323774858
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.7703, Acc: 0.7213, AUC: 0.8788, F1: 0.5707
[VIT] val - Epoch: 41, Loss: 0.9722, Acc: 0.6178, AUC: 0.8192, F1: 0.5011
epoch: 42, total loss: 4.1820419376546685
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.9965, Acc: 0.5690, AUC: 0.9033, F1: 0.5768
[VIT] val - Epoch: 42, Loss: 0.9814, Acc: 0.6006, AUC: 0.8220, F1: 0.4812
epoch: 43, total loss: 3.7992647127671675
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.8412, Acc: 0.6466, AUC: 0.9121, F1: 0.5809
[VIT] val - Epoch: 43, Loss: 0.9915, Acc: 0.6006, AUC: 0.8279, F1: 0.4907
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8279
Saving vit model...
epoch: 44, total loss: 3.7119368856603447
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.7593, Acc: 0.6753, AUC: 0.8969, F1: 0.5569
[VIT] val - Epoch: 44, Loss: 0.9703, Acc: 0.6034, AUC: 0.8194, F1: 0.4763
epoch: 45, total loss: 3.6854402043602685
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.7699, Acc: 0.6925, AUC: 0.8980, F1: 0.5436
[VIT] val - Epoch: 45, Loss: 0.9709, Acc: 0.6063, AUC: 0.8220, F1: 0.4637
epoch: 46, total loss: 3.806777238845825
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.8632, Acc: 0.6322, AUC: 0.9116, F1: 0.5931
[VIT] val - Epoch: 46, Loss: 1.0077, Acc: 0.5833, AUC: 0.8297, F1: 0.4837
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8297
Saving vit model...
epoch: 47, total loss: 3.7973387132991445
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.7388, Acc: 0.7011, AUC: 0.9038, F1: 0.6413
[VIT] val - Epoch: 47, Loss: 0.9575, Acc: 0.6121, AUC: 0.8229, F1: 0.4568
epoch: 48, total loss: 3.888464320789684
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.7495, Acc: 0.6954, AUC: 0.9152, F1: 0.6197
[VIT] val - Epoch: 48, Loss: 0.9590, Acc: 0.6092, AUC: 0.8225, F1: 0.4815
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9152
Saving cnn model...
epoch: 49, total loss: 4.261897412213412
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.9419, Acc: 0.6034, AUC: 0.8367, F1: 0.5480
[VIT] val - Epoch: 49, Loss: 0.9715, Acc: 0.5891, AUC: 0.8233, F1: 0.4721
epoch: 50, total loss: 3.7868451313538984
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 1.0524, Acc: 0.5862, AUC: 0.8526, F1: 0.5590
[VIT] val - Epoch: 50, Loss: 0.9680, Acc: 0.6063, AUC: 0.8234, F1: 0.4812
epoch: 51, total loss: 4.093380158597773
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.7228, Acc: 0.7184, AUC: 0.9070, F1: 0.5904
[VIT] val - Epoch: 51, Loss: 0.9590, Acc: 0.6149, AUC: 0.8252, F1: 0.4831
epoch: 52, total loss: 3.8859714052893897
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.8640, Acc: 0.6351, AUC: 0.8823, F1: 0.5556
[VIT] val - Epoch: 52, Loss: 0.9945, Acc: 0.5948, AUC: 0.8304, F1: 0.4905
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8304
Saving vit model...
epoch: 53, total loss: 3.9075843312523584
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.7853, Acc: 0.6724, AUC: 0.9017, F1: 0.6067
[VIT] val - Epoch: 53, Loss: 0.9444, Acc: 0.6293, AUC: 0.8212, F1: 0.4945
epoch: 54, total loss: 4.115373795682734
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.7844, Acc: 0.6897, AUC: 0.8991, F1: 0.6395
[VIT] val - Epoch: 54, Loss: 0.9564, Acc: 0.6236, AUC: 0.8279, F1: 0.4891
epoch: 55, total loss: 3.9060745672746138
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.7302, Acc: 0.7241, AUC: 0.9033, F1: 0.6534
[VIT] val - Epoch: 55, Loss: 0.9648, Acc: 0.5805, AUC: 0.8224, F1: 0.4561
epoch: 56, total loss: 3.6608690565282647
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.8262, Acc: 0.6437, AUC: 0.8984, F1: 0.6191
[VIT] val - Epoch: 56, Loss: 0.9866, Acc: 0.5920, AUC: 0.8311, F1: 0.4825
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8311
Saving vit model...
epoch: 57, total loss: 3.606377504088662
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.8106, Acc: 0.6810, AUC: 0.8737, F1: 0.6164
[VIT] val - Epoch: 57, Loss: 0.9453, Acc: 0.6178, AUC: 0.8247, F1: 0.4772
epoch: 58, total loss: 3.8499812104485254
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.8389, Acc: 0.6724, AUC: 0.8655, F1: 0.5487
[VIT] val - Epoch: 58, Loss: 0.9464, Acc: 0.6264, AUC: 0.8274, F1: 0.4878
epoch: 59, total loss: 3.754137310114774
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.7607, Acc: 0.6868, AUC: 0.8909, F1: 0.5880
[VIT] val - Epoch: 59, Loss: 0.9623, Acc: 0.6034, AUC: 0.8304, F1: 0.4899
epoch: 60, total loss: 3.769920587539673
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.8140, Acc: 0.7011, AUC: 0.8711, F1: 0.6086
[VIT] val - Epoch: 60, Loss: 0.9581, Acc: 0.6121, AUC: 0.8308, F1: 0.4937
epoch: 61, total loss: 3.8011715737256138
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.8253, Acc: 0.6408, AUC: 0.9016, F1: 0.6090
[VIT] val - Epoch: 61, Loss: 0.9436, Acc: 0.6121, AUC: 0.8266, F1: 0.4918
epoch: 62, total loss: 3.9265462810342964
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.7466, Acc: 0.7270, AUC: 0.8821, F1: 0.6131
[VIT] val - Epoch: 62, Loss: 0.9539, Acc: 0.6092, AUC: 0.8325, F1: 0.4746
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8325
Saving vit model...
epoch: 63, total loss: 4.01832150329243
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.7739, Acc: 0.7040, AUC: 0.8771, F1: 0.6014
[VIT] val - Epoch: 63, Loss: 0.9352, Acc: 0.6379, AUC: 0.8272, F1: 0.4907
epoch: 64, total loss: 3.736653284593062
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.7614, Acc: 0.7011, AUC: 0.9010, F1: 0.6019
[VIT] val - Epoch: 64, Loss: 0.9387, Acc: 0.6207, AUC: 0.8272, F1: 0.4836
epoch: 65, total loss: 3.9817271557721226
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.8381, Acc: 0.6494, AUC: 0.8627, F1: 0.5662
[VIT] val - Epoch: 65, Loss: 0.9669, Acc: 0.6034, AUC: 0.8342, F1: 0.4882
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8342
Saving vit model...
epoch: 66, total loss: 3.765552683310075
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.8845, Acc: 0.6667, AUC: 0.8654, F1: 0.6245
[VIT] val - Epoch: 66, Loss: 0.9564, Acc: 0.6092, AUC: 0.8322, F1: 0.4784
epoch: 67, total loss: 3.5781386982310903
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.8226, Acc: 0.6753, AUC: 0.8831, F1: 0.5673
[VIT] val - Epoch: 67, Loss: 0.9412, Acc: 0.6149, AUC: 0.8271, F1: 0.4761
epoch: 68, total loss: 3.676429033279419
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.8502, Acc: 0.6609, AUC: 0.8786, F1: 0.6456
[VIT] val - Epoch: 68, Loss: 0.9639, Acc: 0.5977, AUC: 0.8334, F1: 0.4843
epoch: 69, total loss: 3.835562890226191
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.8006, Acc: 0.7155, AUC: 0.8954, F1: 0.6399
[VIT] val - Epoch: 69, Loss: 0.9384, Acc: 0.6322, AUC: 0.8309, F1: 0.4886
epoch: 70, total loss: 3.556910601529208
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.7875, Acc: 0.6954, AUC: 0.8995, F1: 0.5726
[VIT] val - Epoch: 70, Loss: 0.9509, Acc: 0.6063, AUC: 0.8294, F1: 0.4936
epoch: 71, total loss: 3.5841293876821343
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.7592, Acc: 0.7011, AUC: 0.8916, F1: 0.6556
[VIT] val - Epoch: 71, Loss: 0.9370, Acc: 0.6293, AUC: 0.8267, F1: 0.4888
epoch: 72, total loss: 3.583784406835383
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.8576, Acc: 0.6437, AUC: 0.8868, F1: 0.5995
[VIT] val - Epoch: 72, Loss: 0.9474, Acc: 0.6121, AUC: 0.8300, F1: 0.4796
epoch: 73, total loss: 3.3321317434310913
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.9192, Acc: 0.6466, AUC: 0.8573, F1: 0.6107
[VIT] val - Epoch: 73, Loss: 0.9411, Acc: 0.6063, AUC: 0.8273, F1: 0.4851
epoch: 74, total loss: 3.5705277052792637
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.8196, Acc: 0.7098, AUC: 0.8481, F1: 0.5829
[VIT] val - Epoch: 74, Loss: 0.9365, Acc: 0.6293, AUC: 0.8301, F1: 0.4907
epoch: 75, total loss: 3.370872378349304
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.7751, Acc: 0.7069, AUC: 0.8942, F1: 0.6142
[VIT] val - Epoch: 75, Loss: 0.9456, Acc: 0.6063, AUC: 0.8281, F1: 0.4780
epoch: 76, total loss: 3.4770632548765703
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.8252, Acc: 0.6695, AUC: 0.8883, F1: 0.5615
[VIT] val - Epoch: 76, Loss: 0.9448, Acc: 0.6063, AUC: 0.8307, F1: 0.4915
epoch: 77, total loss: 3.468084768815474
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.9445, Acc: 0.5948, AUC: 0.8855, F1: 0.5631
[VIT] val - Epoch: 77, Loss: 0.9538, Acc: 0.6034, AUC: 0.8336, F1: 0.4774
epoch: 78, total loss: 3.6686071049083364
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.9594, Acc: 0.6178, AUC: 0.8908, F1: 0.6048
[VIT] val - Epoch: 78, Loss: 0.9338, Acc: 0.6236, AUC: 0.8287, F1: 0.4816
epoch: 79, total loss: 3.614339373328469
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.8801, Acc: 0.6351, AUC: 0.8930, F1: 0.6074
[VIT] val - Epoch: 79, Loss: 0.9460, Acc: 0.6121, AUC: 0.8317, F1: 0.4897
epoch: 80, total loss: 3.8813847303390503
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.8377, Acc: 0.6724, AUC: 0.8845, F1: 0.6049
[VIT] val - Epoch: 80, Loss: 0.9603, Acc: 0.5776, AUC: 0.8318, F1: 0.4831
epoch: 81, total loss: 3.7839201255278154
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.8002, Acc: 0.6839, AUC: 0.8878, F1: 0.6191
[VIT] val - Epoch: 81, Loss: 0.9560, Acc: 0.5920, AUC: 0.8317, F1: 0.4781
epoch: 82, total loss: 3.700460336425088
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.7611, Acc: 0.7098, AUC: 0.8879, F1: 0.5920
[VIT] val - Epoch: 82, Loss: 0.9563, Acc: 0.6006, AUC: 0.8337, F1: 0.4899
epoch: 83, total loss: 3.683804165233265
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.7844, Acc: 0.6839, AUC: 0.8920, F1: 0.6320
[VIT] val - Epoch: 83, Loss: 0.9345, Acc: 0.6178, AUC: 0.8280, F1: 0.4875
epoch: 84, total loss: 3.683169581673362
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.7870, Acc: 0.6983, AUC: 0.8804, F1: 0.5855
[VIT] val - Epoch: 84, Loss: 0.9440, Acc: 0.6063, AUC: 0.8286, F1: 0.4905
epoch: 85, total loss: 3.4305259097706187
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.8105, Acc: 0.6925, AUC: 0.8845, F1: 0.6065
[VIT] val - Epoch: 85, Loss: 0.9399, Acc: 0.6178, AUC: 0.8310, F1: 0.4784
epoch: 86, total loss: 4.008950764482671
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.7804, Acc: 0.7098, AUC: 0.8794, F1: 0.5694
[VIT] val - Epoch: 86, Loss: 0.9406, Acc: 0.6207, AUC: 0.8319, F1: 0.4770
epoch: 87, total loss: 3.583605495366183
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.8086, Acc: 0.6983, AUC: 0.8846, F1: 0.6180
[VIT] val - Epoch: 87, Loss: 0.9563, Acc: 0.5977, AUC: 0.8334, F1: 0.4818
epoch: 88, total loss: 3.9225113716992466
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.8064, Acc: 0.6925, AUC: 0.8708, F1: 0.6209
[VIT] val - Epoch: 88, Loss: 0.9422, Acc: 0.6034, AUC: 0.8326, F1: 0.4721
epoch: 89, total loss: 3.5604333877563477
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.8599, Acc: 0.6667, AUC: 0.8646, F1: 0.6002
[VIT] val - Epoch: 89, Loss: 0.9499, Acc: 0.6006, AUC: 0.8337, F1: 0.4766
epoch: 90, total loss: 3.599346301772378
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.7954, Acc: 0.6954, AUC: 0.8877, F1: 0.6279
[VIT] val - Epoch: 90, Loss: 0.9423, Acc: 0.6034, AUC: 0.8325, F1: 0.4934
epoch: 91, total loss: 3.346377806230025
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.8774, Acc: 0.6379, AUC: 0.8777, F1: 0.5726
[VIT] val - Epoch: 91, Loss: 0.9457, Acc: 0.5977, AUC: 0.8325, F1: 0.4698
epoch: 92, total loss: 3.5223680301146074
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.7833, Acc: 0.7011, AUC: 0.8864, F1: 0.6338
[VIT] val - Epoch: 92, Loss: 0.9359, Acc: 0.6121, AUC: 0.8314, F1: 0.4774
epoch: 93, total loss: 3.43425283648751
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.8325, Acc: 0.6695, AUC: 0.8947, F1: 0.5874
[VIT] val - Epoch: 93, Loss: 0.9437, Acc: 0.5977, AUC: 0.8347, F1: 0.4656
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8347
Saving vit model...
epoch: 94, total loss: 3.760666749694131
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.8044, Acc: 0.6667, AUC: 0.8931, F1: 0.5799
[VIT] val - Epoch: 94, Loss: 0.9314, Acc: 0.6293, AUC: 0.8319, F1: 0.4791
epoch: 95, total loss: 3.4672199487686157
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.7812, Acc: 0.7040, AUC: 0.8915, F1: 0.6124
[VIT] val - Epoch: 95, Loss: 0.9427, Acc: 0.6006, AUC: 0.8353, F1: 0.4673
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8353
Saving vit model...
epoch: 96, total loss: 3.5154463702982124
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.8960, Acc: 0.6207, AUC: 0.8921, F1: 0.5757
[VIT] val - Epoch: 96, Loss: 0.9345, Acc: 0.6149, AUC: 0.8337, F1: 0.4775
epoch: 97, total loss: 3.427003166892312
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.7780, Acc: 0.7098, AUC: 0.8872, F1: 0.6075
[VIT] val - Epoch: 97, Loss: 0.9344, Acc: 0.6264, AUC: 0.8367, F1: 0.4670
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8367
Saving vit model...
epoch: 98, total loss: 3.564971609549089
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.8647, Acc: 0.6437, AUC: 0.8707, F1: 0.5924
[VIT] val - Epoch: 98, Loss: 0.9327, Acc: 0.6293, AUC: 0.8342, F1: 0.4852
epoch: 99, total loss: 3.443153803998774
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.8340, Acc: 0.6695, AUC: 0.8820, F1: 0.6183
[VIT] val - Epoch: 99, Loss: 0.9395, Acc: 0.6092, AUC: 0.8373, F1: 0.4731
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8373
Saving vit model...
epoch: 100, total loss: 3.5764717080376367
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.7835, Acc: 0.7098, AUC: 0.8866, F1: 0.5849
[VIT] val - Epoch: 100, Loss: 0.9281, Acc: 0.6293, AUC: 0.8324, F1: 0.4791
[21:26:04][Rank 1] Training Finished. Starting Final Testing...
[21:26:04][Rank 3] Training Finished. Starting Final Testing...[21:26:04][Rank 2] Training Finished. Starting Final Testing...

[21:26:04][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.0721, Acc: 0.7429, AUC: 0.8044, F1: 0.4571
[VIT] test - Epoch: 100, Loss: 1.0037, Acc: 0.6408, AUC: 0.7692, F1: 0.3928
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.2832, Acc: 0.4943, AUC: 0.7917, F1: 0.4231
[VIT] test - Epoch: 100, Loss: 0.9992, Acc: 0.6421, AUC: 0.7801, F1: 0.3966
‚úÖ [ÂÆåÊàê] Ê∫êÂüü MESSIDOR ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: RLDR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='RLDR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['RLDR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_RLDR
OUT_DIR: ./output_esdg_h100/RLDR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified4/output_esdg_h100/RLDR/CASS_GDRNet_ESDG_RLDR
[21:33:11][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
[21:33:11][Rank 3] Loading datasets...
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
[21:33:11][Rank 2] Loading datasets...
[21:33:11][Rank 1] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.q_projInjecting LoRA into: layer.1.attention.o_proj

Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.q_projInjecting LoRA into: layer.3.attention.q_proj

Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.attention.o_projInjecting LoRA into: layer.3.attention.o_proj

Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.q_projInjecting LoRA into: layer.5.attention.q_proj

Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.attention.o_projInjecting LoRA into: layer.5.attention.o_proj

Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_projInjecting LoRA into: layer.6.mlp.down_proj

Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.mlp.up_projInjecting LoRA into: layer.7.mlp.up_proj

Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_projInjecting LoRA into: layer.7.mlp.down_proj

Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 4.695900082588196
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 2.3391, Acc: 0.3428, AUC: 0.6339, F1: 0.1532
[VIT] val - Epoch: 1, Loss: 1.2085, Acc: 0.5723, AUC: 0.6251, F1: 0.1568
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6339
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6251
Saving vit model...
epoch: 2, total loss: 3.980249619483948
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.0071, Acc: 0.6038, AUC: 0.8209, F1: 0.3583
[VIT] val - Epoch: 2, Loss: 1.2470, Acc: 0.5723, AUC: 0.6962, F1: 0.1456
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8209
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6962
Saving vit model...
epoch: 3, total loss: 4.109384536743164
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 1.0879, Acc: 0.5409, AUC: 0.8373, F1: 0.3920
[VIT] val - Epoch: 3, Loss: 1.1762, Acc: 0.5849, AUC: 0.7261, F1: 0.1826
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8373
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7261
Saving vit model...
epoch: 4, total loss: 5.0915107250213625
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 1.1466, Acc: 0.5346, AUC: 0.8173, F1: 0.3593
[VIT] val - Epoch: 4, Loss: 1.2289, Acc: 0.5912, AUC: 0.7520, F1: 0.2478
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7520
Saving vit model...
epoch: 5, total loss: 4.866204035282135
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.9693, Acc: 0.6384, AUC: 0.8534, F1: 0.4538
[VIT] val - Epoch: 5, Loss: 1.2469, Acc: 0.5692, AUC: 0.7537, F1: 0.3052
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8534
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7537
Saving vit model...
epoch: 6, total loss: 5.129832124710083
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.3907, Acc: 0.3805, AUC: 0.8243, F1: 0.3203
[VIT] val - Epoch: 6, Loss: 1.1732, Acc: 0.5818, AUC: 0.7543, F1: 0.3077
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7543
Saving vit model...
epoch: 7, total loss: 4.939813566207886
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.9723, Acc: 0.6164, AUC: 0.8581, F1: 0.4399
[VIT] val - Epoch: 7, Loss: 1.1836, Acc: 0.5723, AUC: 0.7683, F1: 0.2846
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8581
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7683
Saving vit model...
epoch: 8, total loss: 5.103021669387817
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.9502, Acc: 0.6132, AUC: 0.8562, F1: 0.4473
[VIT] val - Epoch: 8, Loss: 1.1585, Acc: 0.5849, AUC: 0.7738, F1: 0.2954
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7738
Saving vit model...
epoch: 9, total loss: 5.095001399517059
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.9163, Acc: 0.6258, AUC: 0.8542, F1: 0.4364
[VIT] val - Epoch: 9, Loss: 1.1386, Acc: 0.5943, AUC: 0.7781, F1: 0.2784
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7781
Saving vit model...
epoch: 10, total loss: 4.708881282806397
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.9461, Acc: 0.6195, AUC: 0.8698, F1: 0.4513
[VIT] val - Epoch: 10, Loss: 1.1525, Acc: 0.5849, AUC: 0.7809, F1: 0.3332
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8698
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7809
Saving vit model...
epoch: 11, total loss: 4.596998119354248
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 1.0346, Acc: 0.5472, AUC: 0.8470, F1: 0.4675
[VIT] val - Epoch: 11, Loss: 1.1222, Acc: 0.5975, AUC: 0.7833, F1: 0.3098
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7833
Saving vit model...
epoch: 12, total loss: 4.59181512594223
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.2212, Acc: 0.4560, AUC: 0.8016, F1: 0.3267
[VIT] val - Epoch: 12, Loss: 1.1339, Acc: 0.5881, AUC: 0.7878, F1: 0.3212
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7878
Saving vit model...
epoch: 13, total loss: 4.50176283121109
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.1469, Acc: 0.5472, AUC: 0.8428, F1: 0.4007
[VIT] val - Epoch: 13, Loss: 1.1252, Acc: 0.5786, AUC: 0.7879, F1: 0.3250
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7879
Saving vit model...
epoch: 14, total loss: 4.608121454715729
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.9019, Acc: 0.6289, AUC: 0.8707, F1: 0.5303
[VIT] val - Epoch: 14, Loss: 1.1453, Acc: 0.5723, AUC: 0.7911, F1: 0.3668
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8707
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7911
Saving vit model...
epoch: 15, total loss: 4.6107626080513
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.8920, Acc: 0.6069, AUC: 0.8699, F1: 0.4595
[VIT] val - Epoch: 15, Loss: 1.1347, Acc: 0.5660, AUC: 0.7925, F1: 0.3894
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7925
Saving vit model...
epoch: 16, total loss: 4.824377727508545
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.9205, Acc: 0.6195, AUC: 0.8729, F1: 0.4890
[VIT] val - Epoch: 16, Loss: 1.0747, Acc: 0.5849, AUC: 0.7951, F1: 0.2892
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8729
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7951
Saving vit model...
epoch: 17, total loss: 4.9329346179962155
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 1.0115, Acc: 0.5346, AUC: 0.8265, F1: 0.4141
[VIT] val - Epoch: 17, Loss: 1.0827, Acc: 0.5975, AUC: 0.8000, F1: 0.3625
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8000
Saving vit model...
epoch: 18, total loss: 4.644394433498382
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.8930, Acc: 0.6195, AUC: 0.8753, F1: 0.4756
[VIT] val - Epoch: 18, Loss: 1.0725, Acc: 0.5912, AUC: 0.7996, F1: 0.3564
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8753
Saving cnn model...
epoch: 19, total loss: 4.627052414417267
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.9950, Acc: 0.5723, AUC: 0.8379, F1: 0.4537
[VIT] val - Epoch: 19, Loss: 1.1450, Acc: 0.5566, AUC: 0.8017, F1: 0.4277
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8017
Saving vit model...
epoch: 20, total loss: 4.507838976383209
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.8449, Acc: 0.6635, AUC: 0.8734, F1: 0.5388
[VIT] val - Epoch: 20, Loss: 1.0996, Acc: 0.5818, AUC: 0.8048, F1: 0.4023
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8048
Saving vit model...
epoch: 21, total loss: 4.545375597476959
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 1.0554, Acc: 0.5283, AUC: 0.8661, F1: 0.4140
[VIT] val - Epoch: 21, Loss: 1.1188, Acc: 0.5597, AUC: 0.8000, F1: 0.4046
epoch: 22, total loss: 4.702241253852844
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.8866, Acc: 0.6101, AUC: 0.8631, F1: 0.4990
[VIT] val - Epoch: 22, Loss: 1.0848, Acc: 0.6006, AUC: 0.8065, F1: 0.4192
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8065
Saving vit model...
epoch: 23, total loss: 4.448431313037872
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.8831, Acc: 0.6415, AUC: 0.8618, F1: 0.5001
[VIT] val - Epoch: 23, Loss: 1.0877, Acc: 0.6069, AUC: 0.8051, F1: 0.4342
epoch: 24, total loss: 4.4520128726959225
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.9987, Acc: 0.5818, AUC: 0.8699, F1: 0.4992
[VIT] val - Epoch: 24, Loss: 1.1430, Acc: 0.5409, AUC: 0.8046, F1: 0.4021
epoch: 25, total loss: 4.432714307308197
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.8362, Acc: 0.6572, AUC: 0.8686, F1: 0.4719
[VIT] val - Epoch: 25, Loss: 1.0520, Acc: 0.6038, AUC: 0.8098, F1: 0.4138
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8098
Saving vit model...
epoch: 26, total loss: 4.346280705928803
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.9728, Acc: 0.6132, AUC: 0.8735, F1: 0.4641
[VIT] val - Epoch: 26, Loss: 1.0828, Acc: 0.5849, AUC: 0.8116, F1: 0.4206
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8116
Saving vit model...
epoch: 27, total loss: 4.348149561882019
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.8760, Acc: 0.6384, AUC: 0.8494, F1: 0.5176
[VIT] val - Epoch: 27, Loss: 1.0761, Acc: 0.5881, AUC: 0.8126, F1: 0.4272
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8126
Saving vit model...
epoch: 28, total loss: 4.496569120883942
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.9359, Acc: 0.6069, AUC: 0.8658, F1: 0.4651
[VIT] val - Epoch: 28, Loss: 1.1222, Acc: 0.5597, AUC: 0.8094, F1: 0.4350
epoch: 29, total loss: 4.553162038326263
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.8432, Acc: 0.6667, AUC: 0.8621, F1: 0.4987
[VIT] val - Epoch: 29, Loss: 1.0411, Acc: 0.5975, AUC: 0.8107, F1: 0.4075
epoch: 30, total loss: 4.298526847362519
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 1.0311, Acc: 0.5346, AUC: 0.8693, F1: 0.4687
[VIT] val - Epoch: 30, Loss: 1.0864, Acc: 0.5786, AUC: 0.8119, F1: 0.4403
epoch: 31, total loss: 4.260316348075866
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.9619, Acc: 0.6226, AUC: 0.8642, F1: 0.4848
[VIT] val - Epoch: 31, Loss: 1.0462, Acc: 0.5943, AUC: 0.8119, F1: 0.4098
epoch: 32, total loss: 4.289510381221771
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 1.0047, Acc: 0.5692, AUC: 0.8598, F1: 0.5173
[VIT] val - Epoch: 32, Loss: 1.0554, Acc: 0.5786, AUC: 0.8128, F1: 0.4207
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8128
Saving vit model...
epoch: 33, total loss: 4.164590573310852
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.9673, Acc: 0.6006, AUC: 0.8630, F1: 0.4890
[VIT] val - Epoch: 33, Loss: 1.0520, Acc: 0.5912, AUC: 0.8133, F1: 0.4249
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8133
Saving vit model...
epoch: 34, total loss: 4.2834154725074765
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.9177, Acc: 0.6447, AUC: 0.8722, F1: 0.5144
[VIT] val - Epoch: 34, Loss: 1.0563, Acc: 0.5818, AUC: 0.8146, F1: 0.4259
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8146
Saving vit model...
epoch: 35, total loss: 4.29813449382782
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 1.1989, Acc: 0.5126, AUC: 0.8530, F1: 0.4618
[VIT] val - Epoch: 35, Loss: 1.0921, Acc: 0.5629, AUC: 0.8148, F1: 0.4231
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8148
Saving vit model...
epoch: 36, total loss: 4.398476696014404
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.9434, Acc: 0.6006, AUC: 0.8658, F1: 0.5249
[VIT] val - Epoch: 36, Loss: 1.0728, Acc: 0.5755, AUC: 0.8154, F1: 0.4141
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8154
Saving vit model...
epoch: 37, total loss: 4.186062645912171
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.9167, Acc: 0.6289, AUC: 0.8622, F1: 0.5262
[VIT] val - Epoch: 37, Loss: 1.0609, Acc: 0.5755, AUC: 0.8145, F1: 0.4332
epoch: 38, total loss: 4.359450650215149
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 1.1325, Acc: 0.5535, AUC: 0.8577, F1: 0.4665
[VIT] val - Epoch: 38, Loss: 1.1050, Acc: 0.5660, AUC: 0.8177, F1: 0.4562
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8177
Saving vit model...
epoch: 39, total loss: 4.201712119579315
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.9990, Acc: 0.6069, AUC: 0.8605, F1: 0.4513
[VIT] val - Epoch: 39, Loss: 1.0491, Acc: 0.5881, AUC: 0.8185, F1: 0.4319
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8185
Saving vit model...
epoch: 40, total loss: 4.020385813713074
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.9676, Acc: 0.5755, AUC: 0.8541, F1: 0.4703
[VIT] val - Epoch: 40, Loss: 1.0688, Acc: 0.5912, AUC: 0.8205, F1: 0.4554
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8205
Saving vit model...
epoch: 41, total loss: 4.217779505252838
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 1.1126, Acc: 0.5377, AUC: 0.8430, F1: 0.4733
[VIT] val - Epoch: 41, Loss: 1.0235, Acc: 0.5912, AUC: 0.8208, F1: 0.4188
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8208
Saving vit model...
epoch: 42, total loss: 4.178215336799622
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 1.0531, Acc: 0.5786, AUC: 0.8502, F1: 0.4646
[VIT] val - Epoch: 42, Loss: 1.0305, Acc: 0.5943, AUC: 0.8205, F1: 0.4312
epoch: 43, total loss: 4.0660624504089355
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 1.0381, Acc: 0.5849, AUC: 0.8613, F1: 0.4940
[VIT] val - Epoch: 43, Loss: 1.0243, Acc: 0.5912, AUC: 0.8204, F1: 0.4195
epoch: 44, total loss: 4.070372068881989
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 1.0330, Acc: 0.6069, AUC: 0.8494, F1: 0.5180
[VIT] val - Epoch: 44, Loss: 1.0795, Acc: 0.5755, AUC: 0.8210, F1: 0.4566
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8210
Saving vit model...
epoch: 45, total loss: 4.198495399951935
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 1.0089, Acc: 0.5660, AUC: 0.8580, F1: 0.5110
[VIT] val - Epoch: 45, Loss: 1.0238, Acc: 0.6038, AUC: 0.8196, F1: 0.4215
epoch: 46, total loss: 4.072575426101684
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.8513, Acc: 0.6950, AUC: 0.8713, F1: 0.5653
[VIT] val - Epoch: 46, Loss: 1.0214, Acc: 0.5881, AUC: 0.8230, F1: 0.4139
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8230
Saving vit model...
epoch: 47, total loss: 4.447413945198059
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 1.0038, Acc: 0.6006, AUC: 0.8584, F1: 0.5077
[VIT] val - Epoch: 47, Loss: 1.0673, Acc: 0.5692, AUC: 0.8216, F1: 0.4273
epoch: 48, total loss: 4.130358541011811
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.9153, Acc: 0.6226, AUC: 0.8673, F1: 0.4973
[VIT] val - Epoch: 48, Loss: 1.0105, Acc: 0.5881, AUC: 0.8234, F1: 0.4143
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8234
Saving vit model...
epoch: 49, total loss: 4.04807356595993
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.8670, Acc: 0.6447, AUC: 0.8510, F1: 0.4885
[VIT] val - Epoch: 49, Loss: 1.0265, Acc: 0.5975, AUC: 0.8215, F1: 0.4279
epoch: 50, total loss: 3.952939450740814
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 1.0292, Acc: 0.5535, AUC: 0.8587, F1: 0.4538
[VIT] val - Epoch: 50, Loss: 1.0235, Acc: 0.6006, AUC: 0.8214, F1: 0.4270
epoch: 51, total loss: 4.251410460472107
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.9204, Acc: 0.6226, AUC: 0.8637, F1: 0.5103
[VIT] val - Epoch: 51, Loss: 1.0328, Acc: 0.5912, AUC: 0.8224, F1: 0.4205
epoch: 52, total loss: 4.341473937034607
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.8931, Acc: 0.6384, AUC: 0.8620, F1: 0.5250
[VIT] val - Epoch: 52, Loss: 1.0275, Acc: 0.5912, AUC: 0.8226, F1: 0.4236
epoch: 53, total loss: 4.040794706344604
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.9750, Acc: 0.6195, AUC: 0.8439, F1: 0.4643
[VIT] val - Epoch: 53, Loss: 1.0381, Acc: 0.5849, AUC: 0.8215, F1: 0.4391
epoch: 54, total loss: 4.025347983837127
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 1.1295, Acc: 0.5503, AUC: 0.8423, F1: 0.4673
[VIT] val - Epoch: 54, Loss: 1.0399, Acc: 0.5818, AUC: 0.8229, F1: 0.4367
epoch: 55, total loss: 3.804766309261322
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.9399, Acc: 0.6352, AUC: 0.8617, F1: 0.4853
[VIT] val - Epoch: 55, Loss: 1.0178, Acc: 0.5912, AUC: 0.8221, F1: 0.4186
epoch: 56, total loss: 4.205290615558624
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.9310, Acc: 0.6352, AUC: 0.8593, F1: 0.5265
[VIT] val - Epoch: 56, Loss: 1.0337, Acc: 0.5881, AUC: 0.8233, F1: 0.4242
epoch: 57, total loss: 3.6040958523750306
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.9633, Acc: 0.6164, AUC: 0.8566, F1: 0.5113
[VIT] val - Epoch: 57, Loss: 1.0316, Acc: 0.5786, AUC: 0.8222, F1: 0.4132
epoch: 58, total loss: 3.9771162390708925
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.9557, Acc: 0.6132, AUC: 0.8621, F1: 0.5330
[VIT] val - Epoch: 58, Loss: 1.0361, Acc: 0.5881, AUC: 0.8230, F1: 0.4391
epoch: 59, total loss: 3.9326197862625123
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.9983, Acc: 0.6258, AUC: 0.8466, F1: 0.5120
[VIT] val - Epoch: 59, Loss: 1.0265, Acc: 0.5818, AUC: 0.8214, F1: 0.4179
epoch: 60, total loss: 3.776558041572571
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.9158, Acc: 0.6258, AUC: 0.8734, F1: 0.5245
[VIT] val - Epoch: 60, Loss: 1.0385, Acc: 0.5849, AUC: 0.8223, F1: 0.4545
epoch: 61, total loss: 3.699298858642578
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 1.0958, Acc: 0.5692, AUC: 0.8512, F1: 0.4641
[VIT] val - Epoch: 61, Loss: 1.0124, Acc: 0.5975, AUC: 0.8226, F1: 0.4272
epoch: 62, total loss: 3.8078473567962647
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.9249, Acc: 0.6635, AUC: 0.8630, F1: 0.5666
[VIT] val - Epoch: 62, Loss: 1.0259, Acc: 0.5881, AUC: 0.8224, F1: 0.4166
epoch: 63, total loss: 3.9305476427078245
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.9182, Acc: 0.6321, AUC: 0.8485, F1: 0.4809
[VIT] val - Epoch: 63, Loss: 1.0219, Acc: 0.5912, AUC: 0.8227, F1: 0.4339
epoch: 64, total loss: 3.694531726837158
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 1.0692, Acc: 0.6006, AUC: 0.8556, F1: 0.5143
[VIT] val - Epoch: 64, Loss: 1.0298, Acc: 0.5912, AUC: 0.8214, F1: 0.4212
epoch: 65, total loss: 3.8597421169281008
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.9184, Acc: 0.6352, AUC: 0.8646, F1: 0.5499
[VIT] val - Epoch: 65, Loss: 1.0387, Acc: 0.6006, AUC: 0.8228, F1: 0.4822
epoch: 66, total loss: 3.9116853952407835
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.9698, Acc: 0.6258, AUC: 0.8648, F1: 0.5487
[VIT] val - Epoch: 66, Loss: 1.0332, Acc: 0.5849, AUC: 0.8222, F1: 0.4369
epoch: 67, total loss: 3.867103087902069
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.8581, Acc: 0.6635, AUC: 0.8658, F1: 0.5338
[VIT] val - Epoch: 67, Loss: 1.0396, Acc: 0.5881, AUC: 0.8230, F1: 0.4463
epoch: 68, total loss: 3.960997974872589
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.9851, Acc: 0.6069, AUC: 0.8449, F1: 0.4916
[VIT] val - Epoch: 68, Loss: 1.0155, Acc: 0.5912, AUC: 0.8226, F1: 0.4336
epoch: 69, total loss: 3.695954132080078
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 1.1301, Acc: 0.5723, AUC: 0.8480, F1: 0.4322
[VIT] val - Epoch: 69, Loss: 1.0241, Acc: 0.5912, AUC: 0.8232, F1: 0.4334
epoch: 70, total loss: 3.7294764161109923
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.9685, Acc: 0.6226, AUC: 0.8514, F1: 0.5037
[VIT] val - Epoch: 70, Loss: 1.0184, Acc: 0.5912, AUC: 0.8230, F1: 0.4191
epoch: 71, total loss: 3.8470826745033264
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.9582, Acc: 0.6321, AUC: 0.8481, F1: 0.5605
[VIT] val - Epoch: 71, Loss: 1.0240, Acc: 0.5912, AUC: 0.8223, F1: 0.4437
epoch: 72, total loss: 3.8915999054908754
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.9559, Acc: 0.6226, AUC: 0.8539, F1: 0.5088
[VIT] val - Epoch: 72, Loss: 1.0353, Acc: 0.5943, AUC: 0.8236, F1: 0.4627
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8236
Saving vit model...
epoch: 73, total loss: 3.810236155986786
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.8986, Acc: 0.6258, AUC: 0.8582, F1: 0.4452
[VIT] val - Epoch: 73, Loss: 1.0147, Acc: 0.5943, AUC: 0.8239, F1: 0.4364
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8239
Saving vit model...
epoch: 74, total loss: 3.917916142940521
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 1.0341, Acc: 0.6226, AUC: 0.8477, F1: 0.5217
[VIT] val - Epoch: 74, Loss: 1.0231, Acc: 0.5881, AUC: 0.8245, F1: 0.4161
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8245
Saving vit model...
epoch: 75, total loss: 3.872900104522705
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.9341, Acc: 0.6478, AUC: 0.8518, F1: 0.5079
[VIT] val - Epoch: 75, Loss: 1.0193, Acc: 0.5943, AUC: 0.8237, F1: 0.4327
epoch: 76, total loss: 4.002044117450714
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.9787, Acc: 0.6006, AUC: 0.8482, F1: 0.4858
[VIT] val - Epoch: 76, Loss: 1.0171, Acc: 0.6038, AUC: 0.8234, F1: 0.4401
epoch: 77, total loss: 3.939425730705261
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.9352, Acc: 0.6258, AUC: 0.8545, F1: 0.5019
[VIT] val - Epoch: 77, Loss: 1.0094, Acc: 0.5881, AUC: 0.8237, F1: 0.4207
epoch: 78, total loss: 3.8230235099792482
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.9384, Acc: 0.6321, AUC: 0.8491, F1: 0.5309
[VIT] val - Epoch: 78, Loss: 1.0114, Acc: 0.6038, AUC: 0.8246, F1: 0.4630
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8246
Saving vit model...
epoch: 79, total loss: 3.861702024936676
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.9492, Acc: 0.6321, AUC: 0.8476, F1: 0.4775
[VIT] val - Epoch: 79, Loss: 1.0152, Acc: 0.5943, AUC: 0.8256, F1: 0.4406
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8256
Saving vit model...
epoch: 80, total loss: 3.6450232744216917
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.9584, Acc: 0.6195, AUC: 0.8450, F1: 0.4985
[VIT] val - Epoch: 80, Loss: 1.0045, Acc: 0.5912, AUC: 0.8248, F1: 0.4207
epoch: 81, total loss: 3.7012033462524414
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.9475, Acc: 0.6289, AUC: 0.8484, F1: 0.4862
[VIT] val - Epoch: 81, Loss: 1.0103, Acc: 0.6038, AUC: 0.8257, F1: 0.4633
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8257
Saving vit model...
epoch: 82, total loss: 3.509116864204407
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.9870, Acc: 0.6069, AUC: 0.8494, F1: 0.5015
[VIT] val - Epoch: 82, Loss: 1.0160, Acc: 0.5943, AUC: 0.8257, F1: 0.4418
epoch: 83, total loss: 3.925874924659729
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.9497, Acc: 0.6195, AUC: 0.8428, F1: 0.5096
[VIT] val - Epoch: 83, Loss: 1.0190, Acc: 0.5912, AUC: 0.8257, F1: 0.4484
epoch: 84, total loss: 3.9235998511314394
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.9695, Acc: 0.6509, AUC: 0.8502, F1: 0.4620
[VIT] val - Epoch: 84, Loss: 1.0059, Acc: 0.6006, AUC: 0.8257, F1: 0.4599
epoch: 85, total loss: 3.755097544193268
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.9422, Acc: 0.6195, AUC: 0.8468, F1: 0.4938
[VIT] val - Epoch: 85, Loss: 1.0153, Acc: 0.5786, AUC: 0.8246, F1: 0.4213
epoch: 86, total loss: 3.8278168082237243
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 1.0151, Acc: 0.6258, AUC: 0.8491, F1: 0.4879
[VIT] val - Epoch: 86, Loss: 1.0105, Acc: 0.5943, AUC: 0.8243, F1: 0.4384
epoch: 87, total loss: 3.5956610560417177
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.9359, Acc: 0.6226, AUC: 0.8623, F1: 0.5037
[VIT] val - Epoch: 87, Loss: 1.0123, Acc: 0.5881, AUC: 0.8241, F1: 0.4430
epoch: 88, total loss: 3.5994219303131105
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 1.0401, Acc: 0.6164, AUC: 0.8277, F1: 0.4805
[VIT] val - Epoch: 88, Loss: 1.0182, Acc: 0.5881, AUC: 0.8250, F1: 0.4269
epoch: 89, total loss: 4.120769381523132
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.9539, Acc: 0.6226, AUC: 0.8446, F1: 0.4812
[VIT] val - Epoch: 89, Loss: 1.0199, Acc: 0.5849, AUC: 0.8241, F1: 0.4594
epoch: 90, total loss: 3.93325754404068
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.9740, Acc: 0.5975, AUC: 0.8412, F1: 0.5020
[VIT] val - Epoch: 90, Loss: 1.0109, Acc: 0.5943, AUC: 0.8249, F1: 0.4525
epoch: 91, total loss: 3.6759766697883607
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.9982, Acc: 0.6195, AUC: 0.8366, F1: 0.4209
[VIT] val - Epoch: 91, Loss: 1.0194, Acc: 0.5755, AUC: 0.8243, F1: 0.4503
epoch: 92, total loss: 3.9264267683029175
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 1.0018, Acc: 0.6132, AUC: 0.8476, F1: 0.5345
[VIT] val - Epoch: 92, Loss: 1.0198, Acc: 0.5881, AUC: 0.8247, F1: 0.4587
epoch: 93, total loss: 3.7333616495132445
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 1.0849, Acc: 0.5755, AUC: 0.8268, F1: 0.5170
[VIT] val - Epoch: 93, Loss: 1.0084, Acc: 0.5912, AUC: 0.8254, F1: 0.4365
epoch: 94, total loss: 3.76249121427536
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.9514, Acc: 0.6415, AUC: 0.8486, F1: 0.5255
[VIT] val - Epoch: 94, Loss: 1.0339, Acc: 0.5660, AUC: 0.8250, F1: 0.4468
epoch: 95, total loss: 3.788636493682861
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.9763, Acc: 0.6101, AUC: 0.8525, F1: 0.4817
[VIT] val - Epoch: 95, Loss: 1.0011, Acc: 0.5943, AUC: 0.8252, F1: 0.4494
epoch: 96, total loss: 3.907860255241394
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.9344, Acc: 0.6635, AUC: 0.8641, F1: 0.5475
[VIT] val - Epoch: 96, Loss: 1.0054, Acc: 0.5943, AUC: 0.8246, F1: 0.4416
epoch: 97, total loss: 3.6335880160331726
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 1.0415, Acc: 0.5723, AUC: 0.8438, F1: 0.4720
[VIT] val - Epoch: 97, Loss: 1.0163, Acc: 0.5849, AUC: 0.8258, F1: 0.4581
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8258
Saving vit model...
epoch: 98, total loss: 3.473819172382355
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.9447, Acc: 0.6509, AUC: 0.8590, F1: 0.5118
[VIT] val - Epoch: 98, Loss: 0.9952, Acc: 0.6006, AUC: 0.8252, F1: 0.4408
epoch: 99, total loss: 3.9130271077156067
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.9783, Acc: 0.6132, AUC: 0.8594, F1: 0.5357
[VIT] val - Epoch: 99, Loss: 1.0044, Acc: 0.6038, AUC: 0.8258, F1: 0.4680
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8258
Saving vit model...
epoch: 100, total loss: 3.621145153045654
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.9561, Acc: 0.6321, AUC: 0.8402, F1: 0.4949
[VIT] val - Epoch: 100, Loss: 1.0026, Acc: 0.6006, AUC: 0.8253, F1: 0.4607
[22:05:08][Rank 1] Training Finished. Starting Final Testing...[22:05:08][Rank 3] Training Finished. Starting Final Testing...

[22:05:08][Rank 2] Training Finished. Starting Final Testing...
[22:05:08][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.3212, Acc: 0.3007, AUC: 0.7968, F1: 0.3667
[VIT] test - Epoch: 100, Loss: 1.4737, Acc: 0.2285, AUC: 0.7755, F1: 0.2524
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.6018, Acc: 0.3739, AUC: 0.8257, F1: 0.4118
[VIT] test - Epoch: 100, Loss: 1.3082, Acc: 0.2847, AUC: 0.7997, F1: 0.3322
‚úÖ [ÂÆåÊàê] Ê∫êÂüü RLDR ËÆ≠ÁªÉÁªìÊùü„ÄÇ


