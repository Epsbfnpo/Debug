Job 20382551 started at Mon 16 Feb 2026 02:29:18 PM AEDT
========================================================
üöÄ ÂêØÂä® ESDG ÊâπÈáèÂÆûÈ™å (Bash Âæ™ÁéØÊ®°Âºè)
GPU Êï∞Èáè: 4
ÂæÖËøêË°åÊ∫êÂüü: APTOS DEEPDR FGADR IDRID MESSIDOR RLDR
Âü∫Á°ÄËæìÂá∫ÁõÆÂΩï: ./output_esdg_h100
========================================================

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: APTOS
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
[14:32:02][Rank 1] Loading datasets...[14:32:02][Rank 3] Loading datasets...

[14:32:02][Rank 2] Loading datasets...
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='APTOS', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 32
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['APTOS']
  TARGET_DOMAINS: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_APTOS
OUT_DIR: ./output_esdg_h100/APTOS
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA/output_esdg_h100/APTOS/CASS_GDRNet_ESDG_APTOS
[14:32:02][Rank 0] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.üßä [DINOv3] All base parameters frozen.
üßä [DINOv3] All base parameters frozen.

üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_projInjecting LoRA into: layer.2.attention.o_proj

Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_projInjecting LoRA into: layer.2.mlp.down_proj

Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_projInjecting LoRA into: layer.3.attention.k_proj

Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_projInjecting LoRA into: layer.5.attention.v_proj

Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_projInjecting LoRA into: layer.6.mlp.up_proj

Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.v_projInjecting LoRA into: layer.8.attention.v_proj

Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.attention.k_projInjecting LoRA into: layer.10.attention.k_proj

Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
epoch: 1, total loss: 8.885311458421791
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 0.6906, Acc: 0.7172, AUC: 0.8215, F1: 0.4683
[VIT] val - Epoch: 1, Loss: 0.6960, Acc: 0.7445, AUC: 0.8031, F1: 0.3439
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8215
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8031
Saving vit model...
epoch: 2, total loss: 7.620511656222136
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 0.7227, Acc: 0.7213, AUC: 0.8382, F1: 0.4678
[VIT] val - Epoch: 2, Loss: 0.6072, Acc: 0.7650, AUC: 0.8591, F1: 0.4585
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8382
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8591
Saving vit model...
epoch: 3, total loss: 7.46310980423637
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.5302, Acc: 0.7869, AUC: 0.8800, F1: 0.6094
[VIT] val - Epoch: 3, Loss: 0.5304, Acc: 0.8074, AUC: 0.8780, F1: 0.6119
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8800
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8780
Saving vit model...
epoch: 4, total loss: 7.061581798221754
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.5177, Acc: 0.7896, AUC: 0.8742, F1: 0.5611
[VIT] val - Epoch: 4, Loss: 0.5006, Acc: 0.8142, AUC: 0.8923, F1: 0.6449
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8923
Saving vit model...
epoch: 5, total loss: 6.980536481608516
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.6396, Acc: 0.7186, AUC: 0.8527, F1: 0.5040
[VIT] val - Epoch: 5, Loss: 0.5144, Acc: 0.7869, AUC: 0.8782, F1: 0.5471
epoch: 6, total loss: 6.8678090261376425
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.5025, Acc: 0.8087, AUC: 0.8865, F1: 0.6615
[VIT] val - Epoch: 6, Loss: 0.4974, Acc: 0.8128, AUC: 0.8955, F1: 0.6666
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8865
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8955
Saving vit model...
epoch: 7, total loss: 6.7879578548928965
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.5938, Acc: 0.7596, AUC: 0.8634, F1: 0.5678
[VIT] val - Epoch: 7, Loss: 0.4884, Acc: 0.8033, AUC: 0.8936, F1: 0.6385
epoch: 8, total loss: 6.8982119974882705
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.5379, Acc: 0.7842, AUC: 0.9009, F1: 0.6327
[VIT] val - Epoch: 8, Loss: 0.4522, Acc: 0.8292, AUC: 0.9099, F1: 0.6998
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9009
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9099
Saving vit model...
epoch: 9, total loss: 7.086759090423584
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.5081, Acc: 0.8101, AUC: 0.8909, F1: 0.6214
[VIT] val - Epoch: 9, Loss: 0.4373, Acc: 0.8484, AUC: 0.9123, F1: 0.7159
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9123
Saving vit model...
epoch: 10, total loss: 6.97117220837137
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.6617, Acc: 0.7596, AUC: 0.8918, F1: 0.5923
[VIT] val - Epoch: 10, Loss: 0.4743, Acc: 0.8210, AUC: 0.9147, F1: 0.6737
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9147
Saving vit model...
epoch: 11, total loss: 6.904527373935865
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.4978, Acc: 0.8142, AUC: 0.8832, F1: 0.6131
[VIT] val - Epoch: 11, Loss: 0.4342, Acc: 0.8470, AUC: 0.9155, F1: 0.6982
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9155
Saving vit model...
epoch: 12, total loss: 6.995375073474387
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.5779, Acc: 0.7568, AUC: 0.8805, F1: 0.6144
[VIT] val - Epoch: 12, Loss: 0.4348, Acc: 0.8361, AUC: 0.9167, F1: 0.6931
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9167
Saving vit model...
epoch: 13, total loss: 6.636003100353738
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.8501, Acc: 0.7199, AUC: 0.8843, F1: 0.5414
[VIT] val - Epoch: 13, Loss: 0.4409, Acc: 0.8333, AUC: 0.9129, F1: 0.7150
epoch: 14, total loss: 6.76670677765556
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.4831, Acc: 0.8197, AUC: 0.9026, F1: 0.6843
[VIT] val - Epoch: 14, Loss: 0.4649, Acc: 0.8183, AUC: 0.9169, F1: 0.7018
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9026
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9169
Saving vit model...
epoch: 15, total loss: 6.770555537679921
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.6192, Acc: 0.7609, AUC: 0.8795, F1: 0.6078
[VIT] val - Epoch: 15, Loss: 0.4192, Acc: 0.8402, AUC: 0.9157, F1: 0.7178
epoch: 16, total loss: 6.818738605665124
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.4445, Acc: 0.8265, AUC: 0.9218, F1: 0.6476
[VIT] val - Epoch: 16, Loss: 0.4091, Acc: 0.8497, AUC: 0.9179, F1: 0.7260
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9218
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9179
Saving vit model...
epoch: 17, total loss: 6.804257558739704
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.5056, Acc: 0.8033, AUC: 0.8972, F1: 0.6597
[VIT] val - Epoch: 17, Loss: 0.4636, Acc: 0.8128, AUC: 0.9209, F1: 0.6872
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9209
Saving vit model...
epoch: 18, total loss: 6.750633737315303
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.5063, Acc: 0.7937, AUC: 0.8865, F1: 0.6351
[VIT] val - Epoch: 18, Loss: 0.4342, Acc: 0.8402, AUC: 0.9251, F1: 0.6958
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9251
Saving vit model...
epoch: 19, total loss: 6.7815302765887715
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.5174, Acc: 0.7842, AUC: 0.9045, F1: 0.6018
[VIT] val - Epoch: 19, Loss: 0.4128, Acc: 0.8361, AUC: 0.9242, F1: 0.7038
epoch: 20, total loss: 6.763740974923839
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.4761, Acc: 0.8183, AUC: 0.9102, F1: 0.6701
[VIT] val - Epoch: 20, Loss: 0.4264, Acc: 0.8374, AUC: 0.9257, F1: 0.7184
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9257
Saving vit model...
epoch: 21, total loss: 6.53337343879368
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.5018, Acc: 0.8087, AUC: 0.8954, F1: 0.6847
[VIT] val - Epoch: 21, Loss: 0.3968, Acc: 0.8525, AUC: 0.9221, F1: 0.7226
epoch: 22, total loss: 6.645862952522609
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.4878, Acc: 0.8224, AUC: 0.9004, F1: 0.6766
[VIT] val - Epoch: 22, Loss: 0.3994, Acc: 0.8511, AUC: 0.9279, F1: 0.7343
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9279
Saving vit model...
epoch: 23, total loss: 6.709908257360044
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.4816, Acc: 0.8019, AUC: 0.8992, F1: 0.6392
[VIT] val - Epoch: 23, Loss: 0.4201, Acc: 0.8566, AUC: 0.9229, F1: 0.7441
epoch: 24, total loss: 6.657099392103112
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.4955, Acc: 0.7910, AUC: 0.9055, F1: 0.6491
[VIT] val - Epoch: 24, Loss: 0.4051, Acc: 0.8511, AUC: 0.9235, F1: 0.7353
epoch: 25, total loss: 6.340938029081925
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.5525, Acc: 0.7705, AUC: 0.9065, F1: 0.6134
[VIT] val - Epoch: 25, Loss: 0.4121, Acc: 0.8470, AUC: 0.9176, F1: 0.7265
epoch: 26, total loss: 6.683341938516368
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.4399, Acc: 0.8251, AUC: 0.9108, F1: 0.6861
[VIT] val - Epoch: 26, Loss: 0.4209, Acc: 0.8443, AUC: 0.9206, F1: 0.7238
epoch: 27, total loss: 6.678138276805049
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.4853, Acc: 0.8224, AUC: 0.8996, F1: 0.6875
[VIT] val - Epoch: 27, Loss: 0.3826, Acc: 0.8661, AUC: 0.9287, F1: 0.7494
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9287
Saving vit model...
epoch: 28, total loss: 6.385397683019224
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.4559, Acc: 0.8169, AUC: 0.9102, F1: 0.6788
[VIT] val - Epoch: 28, Loss: 0.3970, Acc: 0.8497, AUC: 0.9259, F1: 0.7251
epoch: 29, total loss: 6.5236199627751885
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.5033, Acc: 0.7964, AUC: 0.9131, F1: 0.6446
[VIT] val - Epoch: 29, Loss: 0.3906, Acc: 0.8538, AUC: 0.9309, F1: 0.7383
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9309
Saving vit model...
epoch: 30, total loss: 6.504873959914498
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.5128, Acc: 0.8046, AUC: 0.9028, F1: 0.6687
[VIT] val - Epoch: 30, Loss: 0.4314, Acc: 0.8525, AUC: 0.9204, F1: 0.7326
epoch: 31, total loss: 6.531460305918818
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.4271, Acc: 0.8251, AUC: 0.9223, F1: 0.6946
[VIT] val - Epoch: 31, Loss: 0.3990, Acc: 0.8661, AUC: 0.9286, F1: 0.7540
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9223
Saving cnn model...
epoch: 32, total loss: 6.347827102826989
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.6214, Acc: 0.7623, AUC: 0.8914, F1: 0.6020
[VIT] val - Epoch: 32, Loss: 0.4079, Acc: 0.8579, AUC: 0.9260, F1: 0.7455
epoch: 33, total loss: 6.340537071228027
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.4714, Acc: 0.8224, AUC: 0.9086, F1: 0.6950
[VIT] val - Epoch: 33, Loss: 0.3986, Acc: 0.8634, AUC: 0.9237, F1: 0.7485
epoch: 34, total loss: 6.401748595030411
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.4890, Acc: 0.8005, AUC: 0.9051, F1: 0.6421
[VIT] val - Epoch: 34, Loss: 0.4182, Acc: 0.8443, AUC: 0.9232, F1: 0.7217
epoch: 35, total loss: 6.2873636950617255
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.5396, Acc: 0.7992, AUC: 0.9096, F1: 0.6378
[VIT] val - Epoch: 35, Loss: 0.3952, Acc: 0.8661, AUC: 0.9278, F1: 0.7581
epoch: 36, total loss: 6.513190497522769
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.4940, Acc: 0.8197, AUC: 0.9039, F1: 0.6775
[VIT] val - Epoch: 36, Loss: 0.4267, Acc: 0.8415, AUC: 0.9283, F1: 0.7195
epoch: 37, total loss: 6.285731398540994
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.5330, Acc: 0.7978, AUC: 0.8990, F1: 0.6485
[VIT] val - Epoch: 37, Loss: 0.4008, Acc: 0.8566, AUC: 0.9252, F1: 0.7335
epoch: 38, total loss: 6.296622608018958
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.4406, Acc: 0.8292, AUC: 0.9153, F1: 0.6912
[VIT] val - Epoch: 38, Loss: 0.4280, Acc: 0.8402, AUC: 0.9261, F1: 0.7238
epoch: 39, total loss: 6.339957879937214
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.4289, Acc: 0.8347, AUC: 0.9226, F1: 0.6973
[VIT] val - Epoch: 39, Loss: 0.3965, Acc: 0.8661, AUC: 0.9266, F1: 0.7469
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9226
Saving cnn model...
epoch: 40, total loss: 6.279648946679157
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.4847, Acc: 0.8224, AUC: 0.9054, F1: 0.6645
[VIT] val - Epoch: 40, Loss: 0.4267, Acc: 0.8511, AUC: 0.9269, F1: 0.7337
epoch: 41, total loss: 6.089904826620351
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.4546, Acc: 0.8265, AUC: 0.9195, F1: 0.7033
[VIT] val - Epoch: 41, Loss: 0.4051, Acc: 0.8593, AUC: 0.9285, F1: 0.7440
epoch: 42, total loss: 6.251466958419137
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.5894, Acc: 0.7869, AUC: 0.8939, F1: 0.6581
[VIT] val - Epoch: 42, Loss: 0.4371, Acc: 0.8566, AUC: 0.9248, F1: 0.7433
epoch: 43, total loss: 6.078520795573359
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.6491, Acc: 0.7869, AUC: 0.8888, F1: 0.6285
[VIT] val - Epoch: 43, Loss: 0.4306, Acc: 0.8497, AUC: 0.9237, F1: 0.7350
epoch: 44, total loss: 6.144079996191937
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.6704, Acc: 0.7883, AUC: 0.9012, F1: 0.6820
[VIT] val - Epoch: 44, Loss: 0.4027, Acc: 0.8620, AUC: 0.9252, F1: 0.7516
epoch: 45, total loss: 6.205701662146526
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.4729, Acc: 0.8224, AUC: 0.9141, F1: 0.6743
[VIT] val - Epoch: 45, Loss: 0.4089, Acc: 0.8689, AUC: 0.9200, F1: 0.7473
epoch: 46, total loss: 5.934729099273682
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.5501, Acc: 0.8169, AUC: 0.9062, F1: 0.6951
[VIT] val - Epoch: 46, Loss: 0.4283, Acc: 0.8702, AUC: 0.9214, F1: 0.7592
epoch: 47, total loss: 6.133723839469578
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.4565, Acc: 0.8374, AUC: 0.9164, F1: 0.7109
[VIT] val - Epoch: 47, Loss: 0.4312, Acc: 0.8511, AUC: 0.9250, F1: 0.7278
epoch: 48, total loss: 6.215256442194399
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.5396, Acc: 0.7978, AUC: 0.9087, F1: 0.6657
[VIT] val - Epoch: 48, Loss: 0.4284, Acc: 0.8661, AUC: 0.9220, F1: 0.7572
epoch: 49, total loss: 5.975160847539487
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.5398, Acc: 0.8265, AUC: 0.9038, F1: 0.7016
[VIT] val - Epoch: 49, Loss: 0.4315, Acc: 0.8634, AUC: 0.9186, F1: 0.7480
epoch: 50, total loss: 6.081586443859598
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.4477, Acc: 0.8525, AUC: 0.9172, F1: 0.7164
[VIT] val - Epoch: 50, Loss: 0.4179, Acc: 0.8661, AUC: 0.9238, F1: 0.7406
epoch: 51, total loss: 5.8330024014348565
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.4472, Acc: 0.8593, AUC: 0.9252, F1: 0.7427
[VIT] val - Epoch: 51, Loss: 0.4475, Acc: 0.8525, AUC: 0.9218, F1: 0.7338
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9252
Saving cnn model...
epoch: 52, total loss: 6.1121950978818145
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.4780, Acc: 0.8484, AUC: 0.9209, F1: 0.7224
[VIT] val - Epoch: 52, Loss: 0.4507, Acc: 0.8743, AUC: 0.9216, F1: 0.7647
epoch: 53, total loss: 6.0111500905907675
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.5050, Acc: 0.8306, AUC: 0.9046, F1: 0.6793
[VIT] val - Epoch: 53, Loss: 0.4583, Acc: 0.8415, AUC: 0.9246, F1: 0.7171
epoch: 54, total loss: 5.956522983053456
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.5491, Acc: 0.8033, AUC: 0.9094, F1: 0.6831
[VIT] val - Epoch: 54, Loss: 0.4360, Acc: 0.8538, AUC: 0.9233, F1: 0.7358
epoch: 55, total loss: 6.07799366246099
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.4560, Acc: 0.8456, AUC: 0.9177, F1: 0.7056
[VIT] val - Epoch: 55, Loss: 0.4606, Acc: 0.8593, AUC: 0.9166, F1: 0.7083
epoch: 56, total loss: 5.925315794737442
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.4772, Acc: 0.8443, AUC: 0.9112, F1: 0.7129
[VIT] val - Epoch: 56, Loss: 0.4509, Acc: 0.8634, AUC: 0.9214, F1: 0.7409
epoch: 57, total loss: 5.972556632498036
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.7821, Acc: 0.7555, AUC: 0.9094, F1: 0.6151
[VIT] val - Epoch: 57, Loss: 0.4374, Acc: 0.8552, AUC: 0.9239, F1: 0.7345
epoch: 58, total loss: 5.960910838583241
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.5129, Acc: 0.8388, AUC: 0.9110, F1: 0.7104
[VIT] val - Epoch: 58, Loss: 0.4571, Acc: 0.8620, AUC: 0.9233, F1: 0.7556
epoch: 59, total loss: 5.91748395173446
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.5246, Acc: 0.8279, AUC: 0.9105, F1: 0.7171
[VIT] val - Epoch: 59, Loss: 0.4486, Acc: 0.8730, AUC: 0.9231, F1: 0.7657
epoch: 60, total loss: 5.6920617767002275
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.5431, Acc: 0.8484, AUC: 0.9063, F1: 0.7043
[VIT] val - Epoch: 60, Loss: 0.4517, Acc: 0.8634, AUC: 0.9221, F1: 0.7529
epoch: 61, total loss: 5.734418475109598
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.5491, Acc: 0.8197, AUC: 0.9096, F1: 0.6919
[VIT] val - Epoch: 61, Loss: 0.4660, Acc: 0.8484, AUC: 0.9215, F1: 0.7292
epoch: 62, total loss: 5.868849567745043
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.6271, Acc: 0.7773, AUC: 0.9051, F1: 0.6340
[VIT] val - Epoch: 62, Loss: 0.4727, Acc: 0.8566, AUC: 0.9195, F1: 0.7381
epoch: 63, total loss: 5.725481800411059
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.5097, Acc: 0.8429, AUC: 0.9120, F1: 0.7269
[VIT] val - Epoch: 63, Loss: 0.4568, Acc: 0.8620, AUC: 0.9232, F1: 0.7496
epoch: 64, total loss: 5.8807134006334385
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.6433, Acc: 0.8128, AUC: 0.9011, F1: 0.6883
[VIT] val - Epoch: 64, Loss: 0.5159, Acc: 0.8538, AUC: 0.9197, F1: 0.7275
epoch: 65, total loss: 5.7348432540893555
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.5599, Acc: 0.8279, AUC: 0.9103, F1: 0.6804
[VIT] val - Epoch: 65, Loss: 0.4903, Acc: 0.8566, AUC: 0.9243, F1: 0.7345
epoch: 66, total loss: 5.674103488092837
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.5733, Acc: 0.8470, AUC: 0.9113, F1: 0.6980
[VIT] val - Epoch: 66, Loss: 0.4776, Acc: 0.8730, AUC: 0.9188, F1: 0.7658
epoch: 67, total loss: 5.814266080441683
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.6474, Acc: 0.8238, AUC: 0.8933, F1: 0.6011
[VIT] val - Epoch: 67, Loss: 0.5050, Acc: 0.8511, AUC: 0.9158, F1: 0.7218
epoch: 68, total loss: 5.711569475091022
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.5212, Acc: 0.8374, AUC: 0.9152, F1: 0.7108
[VIT] val - Epoch: 68, Loss: 0.4814, Acc: 0.8675, AUC: 0.9143, F1: 0.7551
epoch: 69, total loss: 5.978542390077011
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.6524, Acc: 0.7992, AUC: 0.9149, F1: 0.6659
[VIT] val - Epoch: 69, Loss: 0.4878, Acc: 0.8620, AUC: 0.9195, F1: 0.7400
epoch: 70, total loss: 5.587580680847168
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.4984, Acc: 0.8320, AUC: 0.9185, F1: 0.6950
[VIT] val - Epoch: 70, Loss: 0.4858, Acc: 0.8689, AUC: 0.9222, F1: 0.7558
epoch: 71, total loss: 5.597433214602263
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.5793, Acc: 0.8292, AUC: 0.9126, F1: 0.7050
[VIT] val - Epoch: 71, Loss: 0.4976, Acc: 0.8579, AUC: 0.9203, F1: 0.7328
epoch: 72, total loss: 5.496215944704802
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.5851, Acc: 0.8538, AUC: 0.9162, F1: 0.7133
[VIT] val - Epoch: 72, Loss: 0.4777, Acc: 0.8757, AUC: 0.9207, F1: 0.7668
epoch: 73, total loss: 5.601647625798765
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.5784, Acc: 0.8347, AUC: 0.9097, F1: 0.7063
[VIT] val - Epoch: 73, Loss: 0.4647, Acc: 0.8743, AUC: 0.9225, F1: 0.7585
epoch: 74, total loss: 5.579153827998949
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.5815, Acc: 0.8251, AUC: 0.9107, F1: 0.6899
[VIT] val - Epoch: 74, Loss: 0.4675, Acc: 0.8798, AUC: 0.9209, F1: 0.7719
epoch: 75, total loss: 5.636582975802214
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.6016, Acc: 0.8415, AUC: 0.9093, F1: 0.6907
[VIT] val - Epoch: 75, Loss: 0.4810, Acc: 0.8784, AUC: 0.9216, F1: 0.7733
epoch: 76, total loss: 5.632640963015349
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.5338, Acc: 0.8347, AUC: 0.9100, F1: 0.6942
[VIT] val - Epoch: 76, Loss: 0.4655, Acc: 0.8675, AUC: 0.9226, F1: 0.7506
epoch: 77, total loss: 5.6866001253542695
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.6504, Acc: 0.8497, AUC: 0.9118, F1: 0.6988
[VIT] val - Epoch: 77, Loss: 0.4741, Acc: 0.8689, AUC: 0.9241, F1: 0.7529
epoch: 78, total loss: 5.640098551045293
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.5287, Acc: 0.8374, AUC: 0.9152, F1: 0.7060
[VIT] val - Epoch: 78, Loss: 0.4958, Acc: 0.8702, AUC: 0.9216, F1: 0.7622
epoch: 79, total loss: 5.547882992288341
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.5643, Acc: 0.8306, AUC: 0.9142, F1: 0.6996
[VIT] val - Epoch: 79, Loss: 0.5015, Acc: 0.8648, AUC: 0.9256, F1: 0.7520
epoch: 80, total loss: 5.568569121153458
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.5865, Acc: 0.8470, AUC: 0.9095, F1: 0.7070
[VIT] val - Epoch: 80, Loss: 0.4838, Acc: 0.8743, AUC: 0.9221, F1: 0.7636
epoch: 81, total loss: 5.599069761193317
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.7875, Acc: 0.7951, AUC: 0.8976, F1: 0.6615
[VIT] val - Epoch: 81, Loss: 0.5080, Acc: 0.8730, AUC: 0.9222, F1: 0.7679
epoch: 82, total loss: 5.441205771073051
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.5436, Acc: 0.8388, AUC: 0.9182, F1: 0.7017
[VIT] val - Epoch: 82, Loss: 0.5144, Acc: 0.8716, AUC: 0.9227, F1: 0.7566
epoch: 83, total loss: 5.762836933135986
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.5532, Acc: 0.8593, AUC: 0.9183, F1: 0.7293
[VIT] val - Epoch: 83, Loss: 0.5201, Acc: 0.8634, AUC: 0.9172, F1: 0.7395
epoch: 84, total loss: 5.43478945027227
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.6010, Acc: 0.8388, AUC: 0.9075, F1: 0.6984
[VIT] val - Epoch: 84, Loss: 0.4846, Acc: 0.8716, AUC: 0.9255, F1: 0.7583
epoch: 85, total loss: 5.588367088981297
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.5400, Acc: 0.8497, AUC: 0.9165, F1: 0.7135
[VIT] val - Epoch: 85, Loss: 0.4857, Acc: 0.8620, AUC: 0.9227, F1: 0.7350
epoch: 86, total loss: 5.417308765908946
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.5810, Acc: 0.8415, AUC: 0.9112, F1: 0.7165
[VIT] val - Epoch: 86, Loss: 0.5097, Acc: 0.8702, AUC: 0.9204, F1: 0.7474
epoch: 87, total loss: 5.343239887900975
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.5768, Acc: 0.8224, AUC: 0.9141, F1: 0.6808
[VIT] val - Epoch: 87, Loss: 0.5239, Acc: 0.8702, AUC: 0.9179, F1: 0.7522
epoch: 88, total loss: 5.568459738855776
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.6047, Acc: 0.8320, AUC: 0.9143, F1: 0.6934
[VIT] val - Epoch: 88, Loss: 0.5003, Acc: 0.8675, AUC: 0.9243, F1: 0.7482
epoch: 89, total loss: 5.499823093414307
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.6077, Acc: 0.8306, AUC: 0.9151, F1: 0.6904
[VIT] val - Epoch: 89, Loss: 0.5299, Acc: 0.8716, AUC: 0.9239, F1: 0.7576
epoch: 90, total loss: 5.4828191425489345
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.6102, Acc: 0.8347, AUC: 0.9103, F1: 0.6955
[VIT] val - Epoch: 90, Loss: 0.5104, Acc: 0.8770, AUC: 0.9217, F1: 0.7537
epoch: 91, total loss: 5.14438537929369
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.6214, Acc: 0.8484, AUC: 0.9081, F1: 0.7076
[VIT] val - Epoch: 91, Loss: 0.5310, Acc: 0.8702, AUC: 0.9188, F1: 0.7434
epoch: 92, total loss: 5.410571595896846
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.6041, Acc: 0.8497, AUC: 0.9155, F1: 0.7052
[VIT] val - Epoch: 92, Loss: 0.5514, Acc: 0.8743, AUC: 0.9225, F1: 0.7618
epoch: 93, total loss: 5.23281657177469
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.5519, Acc: 0.8552, AUC: 0.9205, F1: 0.7306
[VIT] val - Epoch: 93, Loss: 0.5082, Acc: 0.8716, AUC: 0.9210, F1: 0.7536
epoch: 94, total loss: 5.34114644838416
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.7128, Acc: 0.8456, AUC: 0.9034, F1: 0.7045
[VIT] val - Epoch: 94, Loss: 0.5582, Acc: 0.8634, AUC: 0.9186, F1: 0.7393
epoch: 95, total loss: 5.282697573952053
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.5879, Acc: 0.8456, AUC: 0.9116, F1: 0.7094
[VIT] val - Epoch: 95, Loss: 0.5202, Acc: 0.8798, AUC: 0.9220, F1: 0.7646
epoch: 96, total loss: 5.098020657249119
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.5727, Acc: 0.8470, AUC: 0.9143, F1: 0.7159
[VIT] val - Epoch: 96, Loss: 0.5169, Acc: 0.8620, AUC: 0.9223, F1: 0.7380
epoch: 97, total loss: 5.415855801623801
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.6956, Acc: 0.8115, AUC: 0.9113, F1: 0.6673
[VIT] val - Epoch: 97, Loss: 0.5581, Acc: 0.8661, AUC: 0.9183, F1: 0.7373
epoch: 98, total loss: 5.415071736211362
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.6464, Acc: 0.8443, AUC: 0.9098, F1: 0.7120
[VIT] val - Epoch: 98, Loss: 0.5379, Acc: 0.8579, AUC: 0.9248, F1: 0.7379
epoch: 99, total loss: 5.306997008945631
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.6179, Acc: 0.8347, AUC: 0.9115, F1: 0.7021
[VIT] val - Epoch: 99, Loss: 0.5271, Acc: 0.8620, AUC: 0.9238, F1: 0.7366
epoch: 100, total loss: 5.14008849600087
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.6120, Acc: 0.8415, AUC: 0.9095, F1: 0.7070
[VIT] val - Epoch: 100, Loss: 0.5716, Acc: 0.8648, AUC: 0.9137, F1: 0.7407
[15:13:56][Rank 1] Training Finished. Starting Final Testing...[15:13:56][Rank 3] Training Finished. Starting Final Testing...

[15:13:56][Rank 2] Training Finished. Starting Final Testing...
[15:13:56][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test_cnn - Epoch: 100, Loss: 3.6454, Acc: 0.5101, AUC: 0.6341, F1: 0.2839
[VIT] test_cnn - Epoch: 100, Loss: 1.9975, Acc: 0.6336, AUC: 0.6842, F1: 0.3729
üöÄ Final Test Results [CNN] - AUC: 0.6341, Acc: 0.5101, F1: 0.2839
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
‚ùå [ÈîôËØØ] Ê∫êÂüü APTOS ËÆ≠ÁªÉÂ§±Ë¥•ÔºÅ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: DEEPDR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='DEEPDR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 32
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['DEEPDR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_DEEPDR
OUT_DIR: ./output_esdg_h100/DEEPDR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA/output_esdg_h100/DEEPDR/CASS_GDRNet_ESDG_DEEPDR
[15:31:50][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
[15:31:50][Rank 1] Loading datasets...
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
[15:31:50][Rank 2] Loading datasets...
[15:31:50][Rank 3] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
üßä [DINOv3] All base parameters frozen.Injecting LoRA into: layer.0.attention.o_proj

Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.5.attention.v_projInjecting LoRA into: layer.5.attention.k_proj

Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.4.mlp.down_projInjecting LoRA into: layer.5.attention.o_proj

Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.q_projInjecting LoRA into: layer.10.attention.o_proj

Injecting LoRA into: layer.10.attention.v_projInjecting LoRA into: layer.10.mlp.up_proj

Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
epoch: 1, total loss: 10.339487266540527
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.1042, Acc: 0.5469, AUC: 0.7800, F1: 0.3714
[VIT] val - Epoch: 1, Loss: 1.3110, Acc: 0.5969, AUC: 0.7529, F1: 0.2733
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7800
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7529
Saving vit model...
epoch: 2, total loss: 9.183796882629395
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.4580, Acc: 0.4906, AUC: 0.7557, F1: 0.3070
[VIT] val - Epoch: 2, Loss: 1.0114, Acc: 0.6906, AUC: 0.8422, F1: 0.4480
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8422
Saving vit model...
epoch: 3, total loss: 8.498918533325195
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.9593, Acc: 0.6219, AUC: 0.8259, F1: 0.4253
[VIT] val - Epoch: 3, Loss: 0.8396, Acc: 0.7063, AUC: 0.8598, F1: 0.4897
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8259
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8598
Saving vit model...
epoch: 4, total loss: 8.350602340698241
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.9276, Acc: 0.6750, AUC: 0.8235, F1: 0.5220
[VIT] val - Epoch: 4, Loss: 0.8645, Acc: 0.6469, AUC: 0.8727, F1: 0.5429
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8727
Saving vit model...
epoch: 5, total loss: 8.406963443756103
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.2737, Acc: 0.5969, AUC: 0.7856, F1: 0.4647
[VIT] val - Epoch: 5, Loss: 0.8314, Acc: 0.6656, AUC: 0.8844, F1: 0.5657
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8844
Saving vit model...
epoch: 6, total loss: 8.372134971618653
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.7482, Acc: 0.7000, AUC: 0.8631, F1: 0.5183
[VIT] val - Epoch: 6, Loss: 0.7064, Acc: 0.7438, AUC: 0.8909, F1: 0.5840
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8631
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8909
Saving vit model...
epoch: 7, total loss: 8.230323266983032
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.8381, Acc: 0.6875, AUC: 0.8477, F1: 0.5009
[VIT] val - Epoch: 7, Loss: 0.7060, Acc: 0.7219, AUC: 0.8952, F1: 0.6196
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8952
Saving vit model...
epoch: 8, total loss: 7.744855833053589
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 1.1147, Acc: 0.5406, AUC: 0.8538, F1: 0.4970
[VIT] val - Epoch: 8, Loss: 0.7503, Acc: 0.6906, AUC: 0.9044, F1: 0.5864
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9044
Saving vit model...
epoch: 9, total loss: 7.976080417633057
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.9816, Acc: 0.6375, AUC: 0.8694, F1: 0.5263
[VIT] val - Epoch: 9, Loss: 0.6774, Acc: 0.7156, AUC: 0.9044, F1: 0.6083
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8694
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9044
Saving vit model...
epoch: 10, total loss: 7.60962290763855
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.7286, Acc: 0.7375, AUC: 0.8925, F1: 0.6375
[VIT] val - Epoch: 10, Loss: 0.6715, Acc: 0.7156, AUC: 0.9084, F1: 0.6036
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8925
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9084
Saving vit model...
epoch: 11, total loss: 7.89891881942749
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.9188, Acc: 0.6062, AUC: 0.8823, F1: 0.5576
[VIT] val - Epoch: 11, Loss: 0.6595, Acc: 0.7312, AUC: 0.9174, F1: 0.6136
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9174
Saving vit model...
epoch: 12, total loss: 7.576742887496948
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.8176, Acc: 0.6500, AUC: 0.8816, F1: 0.5885
[VIT] val - Epoch: 12, Loss: 0.6184, Acc: 0.7438, AUC: 0.9175, F1: 0.6565
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9175
Saving vit model...
epoch: 13, total loss: 7.753976917266845
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.7284, Acc: 0.7031, AUC: 0.8987, F1: 0.6247
[VIT] val - Epoch: 13, Loss: 0.6212, Acc: 0.7500, AUC: 0.9229, F1: 0.6472
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8987
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9229
Saving vit model...
epoch: 14, total loss: 7.553038167953491
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.7409, Acc: 0.7188, AUC: 0.8879, F1: 0.5902
[VIT] val - Epoch: 14, Loss: 0.6312, Acc: 0.7344, AUC: 0.9229, F1: 0.6428
epoch: 15, total loss: 7.493515729904175
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.7124, Acc: 0.7250, AUC: 0.9027, F1: 0.6593
[VIT] val - Epoch: 15, Loss: 0.6178, Acc: 0.7375, AUC: 0.9266, F1: 0.6591
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9027
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9266
Saving vit model...
epoch: 16, total loss: 7.495753526687622
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 1.0419, Acc: 0.6531, AUC: 0.8650, F1: 0.6017
[VIT] val - Epoch: 16, Loss: 0.6694, Acc: 0.6875, AUC: 0.9282, F1: 0.6389
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9282
Saving vit model...
epoch: 17, total loss: 7.733529138565063
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.7346, Acc: 0.7156, AUC: 0.8896, F1: 0.6089
[VIT] val - Epoch: 17, Loss: 0.6435, Acc: 0.7219, AUC: 0.9265, F1: 0.6582
epoch: 18, total loss: 7.662437629699707
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.9152, Acc: 0.6562, AUC: 0.8862, F1: 0.5603
[VIT] val - Epoch: 18, Loss: 0.6840, Acc: 0.7031, AUC: 0.9281, F1: 0.6382
epoch: 19, total loss: 7.53698763847351
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.0222, Acc: 0.5875, AUC: 0.8869, F1: 0.5631
[VIT] val - Epoch: 19, Loss: 0.6145, Acc: 0.7250, AUC: 0.9321, F1: 0.6643
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9321
Saving vit model...
epoch: 20, total loss: 7.630556583404541
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.8041, Acc: 0.7312, AUC: 0.8928, F1: 0.5582
[VIT] val - Epoch: 20, Loss: 0.5906, Acc: 0.7406, AUC: 0.9311, F1: 0.6505
epoch: 21, total loss: 7.327194499969482
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.7963, Acc: 0.6969, AUC: 0.8935, F1: 0.6524
[VIT] val - Epoch: 21, Loss: 0.6174, Acc: 0.7281, AUC: 0.9353, F1: 0.6720
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9353
Saving vit model...
epoch: 22, total loss: 7.377589511871338
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.6958, Acc: 0.7063, AUC: 0.9212, F1: 0.6341
[VIT] val - Epoch: 22, Loss: 0.5932, Acc: 0.7375, AUC: 0.9315, F1: 0.6653
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9212
Saving cnn model...
epoch: 23, total loss: 7.56792254447937
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.7833, Acc: 0.7188, AUC: 0.8970, F1: 0.6325
[VIT] val - Epoch: 23, Loss: 0.5803, Acc: 0.7656, AUC: 0.9306, F1: 0.6895
epoch: 24, total loss: 7.156982946395874
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.7048, Acc: 0.7156, AUC: 0.9169, F1: 0.6302
[VIT] val - Epoch: 24, Loss: 0.6522, Acc: 0.7281, AUC: 0.9347, F1: 0.7069
epoch: 25, total loss: 7.329144144058228
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.6566, Acc: 0.7312, AUC: 0.9232, F1: 0.6584
[VIT] val - Epoch: 25, Loss: 0.5958, Acc: 0.7625, AUC: 0.9373, F1: 0.7098
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9232
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9373
Saving vit model...
epoch: 26, total loss: 7.183516120910644
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.8589, Acc: 0.6500, AUC: 0.8931, F1: 0.6071
[VIT] val - Epoch: 26, Loss: 0.5588, Acc: 0.7812, AUC: 0.9355, F1: 0.7055
epoch: 27, total loss: 7.2514667987823485
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.8067, Acc: 0.7250, AUC: 0.8761, F1: 0.6025
[VIT] val - Epoch: 27, Loss: 0.5502, Acc: 0.7750, AUC: 0.9370, F1: 0.7059
epoch: 28, total loss: 6.905304670333862
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.8159, Acc: 0.6781, AUC: 0.9222, F1: 0.5729
[VIT] val - Epoch: 28, Loss: 0.5724, Acc: 0.7531, AUC: 0.9360, F1: 0.7008
epoch: 29, total loss: 7.092117118835449
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.7506, Acc: 0.7281, AUC: 0.8898, F1: 0.5514
[VIT] val - Epoch: 29, Loss: 0.5651, Acc: 0.7750, AUC: 0.9436, F1: 0.7241
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9436
Saving vit model...
epoch: 30, total loss: 7.2329353332519535
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.7974, Acc: 0.7125, AUC: 0.8966, F1: 0.5882
[VIT] val - Epoch: 30, Loss: 0.5965, Acc: 0.7625, AUC: 0.9432, F1: 0.6967
epoch: 31, total loss: 6.659200286865234
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.7879, Acc: 0.7125, AUC: 0.9091, F1: 0.6135
[VIT] val - Epoch: 31, Loss: 0.5879, Acc: 0.7469, AUC: 0.9397, F1: 0.6913
epoch: 32, total loss: 6.9543297290802
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.8472, Acc: 0.7063, AUC: 0.8895, F1: 0.5924
[VIT] val - Epoch: 32, Loss: 0.5401, Acc: 0.7812, AUC: 0.9408, F1: 0.6990
epoch: 33, total loss: 6.632290458679199
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.7122, Acc: 0.7125, AUC: 0.9192, F1: 0.6372
[VIT] val - Epoch: 33, Loss: 0.5706, Acc: 0.7594, AUC: 0.9462, F1: 0.7041
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9462
Saving vit model...
epoch: 34, total loss: 6.804370164871216
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.7220, Acc: 0.7562, AUC: 0.9133, F1: 0.6370
[VIT] val - Epoch: 34, Loss: 0.5266, Acc: 0.8000, AUC: 0.9466, F1: 0.7299
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9466
Saving vit model...
epoch: 35, total loss: 6.990154123306274
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 1.1134, Acc: 0.6406, AUC: 0.8970, F1: 0.6127
[VIT] val - Epoch: 35, Loss: 0.5418, Acc: 0.7812, AUC: 0.9462, F1: 0.7148
epoch: 36, total loss: 6.925646686553955
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.6620, Acc: 0.7156, AUC: 0.9138, F1: 0.6462
[VIT] val - Epoch: 36, Loss: 0.5715, Acc: 0.7875, AUC: 0.9430, F1: 0.7208
epoch: 37, total loss: 6.646960592269897
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.9040, Acc: 0.7031, AUC: 0.8821, F1: 0.5958
[VIT] val - Epoch: 37, Loss: 0.5539, Acc: 0.7688, AUC: 0.9445, F1: 0.7412
epoch: 38, total loss: 6.5397851943969725
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.7874, Acc: 0.7375, AUC: 0.8978, F1: 0.6575
[VIT] val - Epoch: 38, Loss: 0.5905, Acc: 0.7438, AUC: 0.9427, F1: 0.6971
epoch: 39, total loss: 6.947259044647216
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.7486, Acc: 0.7250, AUC: 0.9133, F1: 0.6749
[VIT] val - Epoch: 39, Loss: 0.5580, Acc: 0.7844, AUC: 0.9424, F1: 0.7296
epoch: 40, total loss: 6.741762113571167
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.7957, Acc: 0.7250, AUC: 0.9043, F1: 0.6405
[VIT] val - Epoch: 40, Loss: 0.5948, Acc: 0.7656, AUC: 0.9423, F1: 0.6922
epoch: 41, total loss: 6.4292572975158695
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.8211, Acc: 0.7125, AUC: 0.9212, F1: 0.6359
[VIT] val - Epoch: 41, Loss: 0.5396, Acc: 0.7844, AUC: 0.9456, F1: 0.7492
epoch: 42, total loss: 6.77911901473999
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.7490, Acc: 0.7594, AUC: 0.9314, F1: 0.7099
[VIT] val - Epoch: 42, Loss: 0.5334, Acc: 0.8031, AUC: 0.9468, F1: 0.7421
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9314
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9468
Saving vit model...
epoch: 43, total loss: 6.7386391162872314
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.8337, Acc: 0.7219, AUC: 0.9245, F1: 0.6996
[VIT] val - Epoch: 43, Loss: 0.5600, Acc: 0.7656, AUC: 0.9401, F1: 0.7157
epoch: 44, total loss: 6.5468000888824465
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.8828, Acc: 0.7375, AUC: 0.9150, F1: 0.6942
[VIT] val - Epoch: 44, Loss: 0.5849, Acc: 0.7625, AUC: 0.9489, F1: 0.7203
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9489
Saving vit model...
epoch: 45, total loss: 6.512103080749512
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.7524, Acc: 0.7094, AUC: 0.9265, F1: 0.6877
[VIT] val - Epoch: 45, Loss: 0.6109, Acc: 0.7562, AUC: 0.9401, F1: 0.7115
epoch: 46, total loss: 6.8156561851501465
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.7363, Acc: 0.7375, AUC: 0.9213, F1: 0.6600
[VIT] val - Epoch: 46, Loss: 0.5488, Acc: 0.7937, AUC: 0.9440, F1: 0.7443
epoch: 47, total loss: 6.2824948787689205
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.8312, Acc: 0.7406, AUC: 0.9121, F1: 0.6294
[VIT] val - Epoch: 47, Loss: 0.5901, Acc: 0.7688, AUC: 0.9431, F1: 0.7116
epoch: 48, total loss: 6.50833420753479
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.7116, Acc: 0.7562, AUC: 0.9351, F1: 0.7066
[VIT] val - Epoch: 48, Loss: 0.6744, Acc: 0.7562, AUC: 0.9329, F1: 0.6623
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9351
Saving cnn model...
epoch: 49, total loss: 6.395680379867554
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.6404, Acc: 0.7906, AUC: 0.9346, F1: 0.7060
[VIT] val - Epoch: 49, Loss: 0.5511, Acc: 0.7781, AUC: 0.9477, F1: 0.7371
epoch: 50, total loss: 6.266102504730225
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.6663, Acc: 0.7656, AUC: 0.9303, F1: 0.6693
[VIT] val - Epoch: 50, Loss: 0.6185, Acc: 0.7594, AUC: 0.9433, F1: 0.6674
epoch: 51, total loss: 6.090397691726684
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.8621, Acc: 0.7281, AUC: 0.9228, F1: 0.7038
[VIT] val - Epoch: 51, Loss: 0.5917, Acc: 0.7750, AUC: 0.9422, F1: 0.7120
epoch: 52, total loss: 6.095897769927978
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.8562, Acc: 0.7594, AUC: 0.9066, F1: 0.6722
[VIT] val - Epoch: 52, Loss: 0.5675, Acc: 0.7875, AUC: 0.9507, F1: 0.7158
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9507
Saving vit model...
epoch: 53, total loss: 6.159620237350464
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.7132, Acc: 0.7656, AUC: 0.9321, F1: 0.7087
[VIT] val - Epoch: 53, Loss: 0.5805, Acc: 0.7906, AUC: 0.9457, F1: 0.7104
epoch: 54, total loss: 6.360677909851074
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 1.3128, Acc: 0.6125, AUC: 0.9176, F1: 0.6134
[VIT] val - Epoch: 54, Loss: 0.5179, Acc: 0.8219, AUC: 0.9516, F1: 0.7631
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9516
Saving vit model...
epoch: 55, total loss: 5.990564823150635
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.8524, Acc: 0.7750, AUC: 0.9283, F1: 0.7066
[VIT] val - Epoch: 55, Loss: 0.5732, Acc: 0.7875, AUC: 0.9478, F1: 0.7496
epoch: 56, total loss: 6.240226888656617
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.8456, Acc: 0.7094, AUC: 0.9267, F1: 0.6769
[VIT] val - Epoch: 56, Loss: 0.6214, Acc: 0.7406, AUC: 0.9485, F1: 0.7115
epoch: 57, total loss: 5.985235500335693
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.7522, Acc: 0.7594, AUC: 0.9314, F1: 0.7016
[VIT] val - Epoch: 57, Loss: 0.5995, Acc: 0.7688, AUC: 0.9460, F1: 0.7396
epoch: 58, total loss: 5.890071535110474
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.7489, Acc: 0.7750, AUC: 0.9224, F1: 0.7206
[VIT] val - Epoch: 58, Loss: 0.5874, Acc: 0.7906, AUC: 0.9436, F1: 0.7478
epoch: 59, total loss: 6.031740808486939
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.6226, Acc: 0.8000, AUC: 0.9375, F1: 0.7288
[VIT] val - Epoch: 59, Loss: 0.5743, Acc: 0.8063, AUC: 0.9448, F1: 0.7452
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9375
Saving cnn model...
epoch: 60, total loss: 6.298358345031739
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 1.0534, Acc: 0.7188, AUC: 0.9070, F1: 0.6139
[VIT] val - Epoch: 60, Loss: 0.5959, Acc: 0.8125, AUC: 0.9497, F1: 0.7211
epoch: 61, total loss: 6.243686294555664
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 1.0293, Acc: 0.6969, AUC: 0.9321, F1: 0.6882
[VIT] val - Epoch: 61, Loss: 0.5808, Acc: 0.7906, AUC: 0.9434, F1: 0.7116
epoch: 62, total loss: 6.304595041275024
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.7038, Acc: 0.7719, AUC: 0.9383, F1: 0.7192
[VIT] val - Epoch: 62, Loss: 0.5767, Acc: 0.8187, AUC: 0.9444, F1: 0.7780
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9383
Saving cnn model...
epoch: 63, total loss: 6.128958368301392
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.7139, Acc: 0.7906, AUC: 0.9389, F1: 0.7145
[VIT] val - Epoch: 63, Loss: 0.5976, Acc: 0.7969, AUC: 0.9413, F1: 0.7272
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9389
Saving cnn model...
epoch: 64, total loss: 5.924724340438843
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.6840, Acc: 0.7812, AUC: 0.9383, F1: 0.7169
[VIT] val - Epoch: 64, Loss: 0.6157, Acc: 0.7812, AUC: 0.9413, F1: 0.7168
epoch: 65, total loss: 5.836920213699341
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.7053, Acc: 0.7688, AUC: 0.9345, F1: 0.6996
[VIT] val - Epoch: 65, Loss: 0.6490, Acc: 0.7875, AUC: 0.9394, F1: 0.7374
epoch: 66, total loss: 5.958268690109253
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.7402, Acc: 0.7656, AUC: 0.9439, F1: 0.6905
[VIT] val - Epoch: 66, Loss: 0.6054, Acc: 0.7875, AUC: 0.9395, F1: 0.7296
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9439
Saving cnn model...
epoch: 67, total loss: 5.947921514511108
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.7039, Acc: 0.7969, AUC: 0.9415, F1: 0.7242
[VIT] val - Epoch: 67, Loss: 0.6145, Acc: 0.8156, AUC: 0.9441, F1: 0.7437
epoch: 68, total loss: 6.126042222976684
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.7624, Acc: 0.7781, AUC: 0.9350, F1: 0.7294
[VIT] val - Epoch: 68, Loss: 0.6021, Acc: 0.8031, AUC: 0.9393, F1: 0.7339
epoch: 69, total loss: 5.850839900970459
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.6942, Acc: 0.8094, AUC: 0.9434, F1: 0.7635
[VIT] val - Epoch: 69, Loss: 0.5980, Acc: 0.8031, AUC: 0.9419, F1: 0.7427
epoch: 70, total loss: 5.476572179794312
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.6631, Acc: 0.8156, AUC: 0.9425, F1: 0.7696
[VIT] val - Epoch: 70, Loss: 0.6203, Acc: 0.8094, AUC: 0.9420, F1: 0.7647
epoch: 71, total loss: 5.4303630828857425
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.7790, Acc: 0.7875, AUC: 0.9352, F1: 0.7011
[VIT] val - Epoch: 71, Loss: 0.6083, Acc: 0.8000, AUC: 0.9433, F1: 0.7598
epoch: 72, total loss: 5.583843946456909
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.8168, Acc: 0.7625, AUC: 0.9355, F1: 0.7009
[VIT] val - Epoch: 72, Loss: 0.6231, Acc: 0.8094, AUC: 0.9455, F1: 0.7704
epoch: 73, total loss: 5.760398864746094
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.6863, Acc: 0.8125, AUC: 0.9450, F1: 0.7632
[VIT] val - Epoch: 73, Loss: 0.5908, Acc: 0.8094, AUC: 0.9449, F1: 0.7651
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9450
Saving cnn model...
epoch: 74, total loss: 5.920824337005615
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.7023, Acc: 0.7906, AUC: 0.9438, F1: 0.7264
[VIT] val - Epoch: 74, Loss: 0.6187, Acc: 0.7969, AUC: 0.9371, F1: 0.7376
epoch: 75, total loss: 5.63746485710144
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.7145, Acc: 0.7781, AUC: 0.9415, F1: 0.7174
[VIT] val - Epoch: 75, Loss: 0.5917, Acc: 0.8094, AUC: 0.9468, F1: 0.7517
epoch: 76, total loss: 5.632552719116211
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.6670, Acc: 0.8031, AUC: 0.9461, F1: 0.7464
[VIT] val - Epoch: 76, Loss: 0.6300, Acc: 0.8031, AUC: 0.9341, F1: 0.7241
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9461
Saving cnn model...
epoch: 77, total loss: 5.852486991882325
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.6764, Acc: 0.8000, AUC: 0.9442, F1: 0.7390
[VIT] val - Epoch: 77, Loss: 0.5655, Acc: 0.8219, AUC: 0.9455, F1: 0.7682
epoch: 78, total loss: 5.577663993835449
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.7042, Acc: 0.8000, AUC: 0.9429, F1: 0.7243
[VIT] val - Epoch: 78, Loss: 0.5835, Acc: 0.8031, AUC: 0.9443, F1: 0.7604
epoch: 79, total loss: 5.734224081039429
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.7564, Acc: 0.7969, AUC: 0.9560, F1: 0.7785
[VIT] val - Epoch: 79, Loss: 0.5763, Acc: 0.8187, AUC: 0.9481, F1: 0.7774
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9560
Saving cnn model...
epoch: 80, total loss: 5.699440765380859
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.6742, Acc: 0.7906, AUC: 0.9498, F1: 0.7060
[VIT] val - Epoch: 80, Loss: 0.5996, Acc: 0.7937, AUC: 0.9450, F1: 0.7451
epoch: 81, total loss: 5.25647611618042
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.9689, Acc: 0.7562, AUC: 0.9106, F1: 0.6417
[VIT] val - Epoch: 81, Loss: 0.6124, Acc: 0.8031, AUC: 0.9407, F1: 0.7501
epoch: 82, total loss: 5.511387586593628
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.7631, Acc: 0.7937, AUC: 0.9457, F1: 0.7403
[VIT] val - Epoch: 82, Loss: 0.6141, Acc: 0.8125, AUC: 0.9406, F1: 0.7512
epoch: 83, total loss: 5.204709196090699
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.7442, Acc: 0.8063, AUC: 0.9431, F1: 0.7360
[VIT] val - Epoch: 83, Loss: 0.6711, Acc: 0.8219, AUC: 0.9430, F1: 0.7395
epoch: 84, total loss: 5.670411014556885
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.7628, Acc: 0.7875, AUC: 0.9311, F1: 0.7279
[VIT] val - Epoch: 84, Loss: 0.6304, Acc: 0.8031, AUC: 0.9404, F1: 0.7424
epoch: 85, total loss: 5.062511205673218
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.7037, Acc: 0.8094, AUC: 0.9445, F1: 0.7293
[VIT] val - Epoch: 85, Loss: 0.5923, Acc: 0.8094, AUC: 0.9438, F1: 0.7560
epoch: 86, total loss: 5.641387033462524
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.7871, Acc: 0.7844, AUC: 0.9371, F1: 0.7001
[VIT] val - Epoch: 86, Loss: 0.6126, Acc: 0.8063, AUC: 0.9447, F1: 0.7753
epoch: 87, total loss: 5.607296943664551
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.6810, Acc: 0.8094, AUC: 0.9465, F1: 0.7619
[VIT] val - Epoch: 87, Loss: 0.6412, Acc: 0.7937, AUC: 0.9413, F1: 0.7469
epoch: 88, total loss: 5.644342851638794
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 1.0825, Acc: 0.7438, AUC: 0.9341, F1: 0.6516
[VIT] val - Epoch: 88, Loss: 0.6317, Acc: 0.7812, AUC: 0.9421, F1: 0.7272
epoch: 89, total loss: 5.573365020751953
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.8202, Acc: 0.7750, AUC: 0.9256, F1: 0.6904
[VIT] val - Epoch: 89, Loss: 0.5754, Acc: 0.8187, AUC: 0.9473, F1: 0.7667
epoch: 90, total loss: 5.306069993972779
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.7522, Acc: 0.7688, AUC: 0.9457, F1: 0.7149
[VIT] val - Epoch: 90, Loss: 0.6045, Acc: 0.8063, AUC: 0.9406, F1: 0.7496
epoch: 91, total loss: 5.413151121139526
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.7471, Acc: 0.7937, AUC: 0.9538, F1: 0.6888
[VIT] val - Epoch: 91, Loss: 0.6215, Acc: 0.8000, AUC: 0.9393, F1: 0.7440
epoch: 92, total loss: 5.370367097854614
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.7105, Acc: 0.8063, AUC: 0.9470, F1: 0.7396
[VIT] val - Epoch: 92, Loss: 0.5969, Acc: 0.8187, AUC: 0.9423, F1: 0.7662
epoch: 93, total loss: 5.493902635574341
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.6958, Acc: 0.8250, AUC: 0.9491, F1: 0.7542
[VIT] val - Epoch: 93, Loss: 0.6576, Acc: 0.7969, AUC: 0.9396, F1: 0.7391
epoch: 94, total loss: 5.085592746734619
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.7192, Acc: 0.7875, AUC: 0.9500, F1: 0.7340
[VIT] val - Epoch: 94, Loss: 0.7037, Acc: 0.8187, AUC: 0.9382, F1: 0.7310
epoch: 95, total loss: 5.70898551940918
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.7372, Acc: 0.7937, AUC: 0.9497, F1: 0.7322
[VIT] val - Epoch: 95, Loss: 0.6031, Acc: 0.8250, AUC: 0.9418, F1: 0.7737
epoch: 96, total loss: 5.101942873001098
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.6835, Acc: 0.7906, AUC: 0.9636, F1: 0.7493
[VIT] val - Epoch: 96, Loss: 0.6446, Acc: 0.7969, AUC: 0.9400, F1: 0.7375
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9636
Saving cnn model...
epoch: 97, total loss: 5.378999280929565
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.8350, Acc: 0.8031, AUC: 0.9506, F1: 0.7379
[VIT] val - Epoch: 97, Loss: 0.6503, Acc: 0.8000, AUC: 0.9421, F1: 0.7537
epoch: 98, total loss: 5.371513223648071
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.7340, Acc: 0.8031, AUC: 0.9563, F1: 0.7497
[VIT] val - Epoch: 98, Loss: 0.6596, Acc: 0.8094, AUC: 0.9419, F1: 0.7372
epoch: 99, total loss: 5.339516592025757
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.6770, Acc: 0.8187, AUC: 0.9519, F1: 0.7636
[VIT] val - Epoch: 99, Loss: 0.6951, Acc: 0.7750, AUC: 0.9373, F1: 0.7192
epoch: 100, total loss: 5.154057264328003
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.7288, Acc: 0.8094, AUC: 0.9457, F1: 0.7489
[VIT] val - Epoch: 100, Loss: 0.6538, Acc: 0.7844, AUC: 0.9414, F1: 0.7360
[15:48:43][Rank 2] Training Finished. Starting Final Testing...[15:48:43][Rank 1] Training Finished. Starting Final Testing...

[15:48:43][Rank 3] Training Finished. Starting Final Testing...
[15:48:43][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test_cnn - Epoch: 100, Loss: 2.1117, Acc: 0.4502, AUC: 0.7197, F1: 0.3373
[VIT] test_cnn - Epoch: 100, Loss: 1.6710, Acc: 0.4946, AUC: 0.7642, F1: 0.4152
üöÄ Final Test Results [CNN] - AUC: 0.7197, Acc: 0.4502, F1: 0.3373
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
‚ùå [ÈîôËØØ] Ê∫êÂüü DEEPDR ËÆ≠ÁªÉÂ§±Ë¥•ÔºÅ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: FGADR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='FGADR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 32
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['FGADR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_FGADR
OUT_DIR: ./output_esdg_h100/FGADR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA/output_esdg_h100/FGADR/CASS_GDRNet_ESDG_FGADR
[16:04:35][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
[16:04:35][Rank 1] Loading datasets...
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
[16:04:35][Rank 3] Loading datasets...
[16:04:35][Rank 2] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.0.attention.v_projInjecting LoRA into: layer.9.mlp.up_proj

Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_projInjecting LoRA into: layer.10.attention.k_proj

Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
epoch: 1, total loss: 9.848453521728516
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 3.3121, Acc: 0.2065, AUC: 0.7507, F1: 0.1414
[VIT] val - Epoch: 1, Loss: 1.2851, Acc: 0.4538, AUC: 0.7925, F1: 0.2609
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7507
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7925
Saving vit model...
epoch: 2, total loss: 8.704288562138876
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 0.8616, Acc: 0.6766, AUC: 0.8371, F1: 0.3859
[VIT] val - Epoch: 2, Loss: 0.9041, Acc: 0.6277, AUC: 0.8314, F1: 0.3704
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8371
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8314
Saving vit model...
epoch: 3, total loss: 8.289197325706482
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.8982, Acc: 0.5625, AUC: 0.8572, F1: 0.4404
[VIT] val - Epoch: 3, Loss: 0.9167, Acc: 0.5870, AUC: 0.8404, F1: 0.3916
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8572
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8404
Saving vit model...
epoch: 4, total loss: 8.191466887791952
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.8858, Acc: 0.5897, AUC: 0.8526, F1: 0.4439
[VIT] val - Epoch: 4, Loss: 0.8478, Acc: 0.6495, AUC: 0.8520, F1: 0.4197
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8520
Saving vit model...
epoch: 5, total loss: 7.99554963906606
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.7930, Acc: 0.6685, AUC: 0.8637, F1: 0.4364
[VIT] val - Epoch: 5, Loss: 0.8501, Acc: 0.6685, AUC: 0.8623, F1: 0.5227
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8637
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8623
Saving vit model...
epoch: 6, total loss: 7.9374721844991045
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.7853, Acc: 0.7038, AUC: 0.8689, F1: 0.4657
[VIT] val - Epoch: 6, Loss: 0.9046, Acc: 0.6114, AUC: 0.8633, F1: 0.4699
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8689
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8633
Saving vit model...
epoch: 7, total loss: 7.6884810129801435
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.8474, Acc: 0.6549, AUC: 0.8546, F1: 0.5038
[VIT] val - Epoch: 7, Loss: 0.8122, Acc: 0.6549, AUC: 0.8665, F1: 0.5161
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8665
Saving vit model...
epoch: 8, total loss: 7.766627868016561
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.8397, Acc: 0.6168, AUC: 0.8765, F1: 0.4558
[VIT] val - Epoch: 8, Loss: 0.8245, Acc: 0.6630, AUC: 0.8675, F1: 0.5120
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8765
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8675
Saving vit model...
epoch: 9, total loss: 7.579958597819011
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.8004, Acc: 0.6467, AUC: 0.8783, F1: 0.4461
[VIT] val - Epoch: 9, Loss: 0.7680, Acc: 0.6793, AUC: 0.8705, F1: 0.4861
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8783
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8705
Saving vit model...
epoch: 10, total loss: 7.533247550328572
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.8106, Acc: 0.6848, AUC: 0.8742, F1: 0.5176
[VIT] val - Epoch: 10, Loss: 0.7531, Acc: 0.6929, AUC: 0.8709, F1: 0.5501
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8709
Saving vit model...
epoch: 11, total loss: 7.477341294288635
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.8185, Acc: 0.6304, AUC: 0.8774, F1: 0.4890
[VIT] val - Epoch: 11, Loss: 0.7628, Acc: 0.6929, AUC: 0.8751, F1: 0.5854
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8751
Saving vit model...
epoch: 12, total loss: 7.516113996505737
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.0392, Acc: 0.5299, AUC: 0.8671, F1: 0.4277
[VIT] val - Epoch: 12, Loss: 0.8213, Acc: 0.6467, AUC: 0.8765, F1: 0.5397
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8765
Saving vit model...
epoch: 13, total loss: 7.390009840329488
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.2023, Acc: 0.5299, AUC: 0.8760, F1: 0.4704
[VIT] val - Epoch: 13, Loss: 0.7488, Acc: 0.7038, AUC: 0.8800, F1: 0.5886
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8800
Saving vit model...
epoch: 14, total loss: 7.400989850362142
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.8002, Acc: 0.6495, AUC: 0.8771, F1: 0.4773
[VIT] val - Epoch: 14, Loss: 0.8269, Acc: 0.6630, AUC: 0.8793, F1: 0.5416
epoch: 15, total loss: 7.379299243291219
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.8031, Acc: 0.6766, AUC: 0.8838, F1: 0.5625
[VIT] val - Epoch: 15, Loss: 0.7809, Acc: 0.6522, AUC: 0.8810, F1: 0.5256
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8838
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8810
Saving vit model...
epoch: 16, total loss: 7.221098025639852
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.7724, Acc: 0.6957, AUC: 0.8882, F1: 0.5326
[VIT] val - Epoch: 16, Loss: 0.7431, Acc: 0.7092, AUC: 0.8854, F1: 0.5966
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8882
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8854
Saving vit model...
epoch: 17, total loss: 7.328174511591594
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.9032, Acc: 0.6196, AUC: 0.8737, F1: 0.4741
[VIT] val - Epoch: 17, Loss: 0.7829, Acc: 0.6848, AUC: 0.8842, F1: 0.5724
epoch: 18, total loss: 7.228783965110779
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.7859, Acc: 0.6658, AUC: 0.8914, F1: 0.5382
[VIT] val - Epoch: 18, Loss: 0.7818, Acc: 0.6766, AUC: 0.8848, F1: 0.5897
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8914
Saving cnn model...
epoch: 19, total loss: 7.014684677124023
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.7669, Acc: 0.6875, AUC: 0.8798, F1: 0.5170
[VIT] val - Epoch: 19, Loss: 0.8069, Acc: 0.6522, AUC: 0.8855, F1: 0.5784
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8855
Saving vit model...
epoch: 20, total loss: 7.280239105224609
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.7575, Acc: 0.6957, AUC: 0.8863, F1: 0.5320
[VIT] val - Epoch: 20, Loss: 0.7184, Acc: 0.7065, AUC: 0.8843, F1: 0.5633
epoch: 21, total loss: 7.103144486745198
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.9009, Acc: 0.6386, AUC: 0.8848, F1: 0.5208
[VIT] val - Epoch: 21, Loss: 0.7050, Acc: 0.7201, AUC: 0.8840, F1: 0.5854
epoch: 22, total loss: 7.054797490437825
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.8979, Acc: 0.6386, AUC: 0.8870, F1: 0.5605
[VIT] val - Epoch: 22, Loss: 0.7015, Acc: 0.7120, AUC: 0.8888, F1: 0.5670
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8888
Saving vit model...
epoch: 23, total loss: 6.952474117279053
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.8508, Acc: 0.6630, AUC: 0.8814, F1: 0.5707
[VIT] val - Epoch: 23, Loss: 0.8051, Acc: 0.6386, AUC: 0.8859, F1: 0.5445
epoch: 24, total loss: 7.222988406817119
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.8263, Acc: 0.7092, AUC: 0.8860, F1: 0.5874
[VIT] val - Epoch: 24, Loss: 0.7295, Acc: 0.7011, AUC: 0.8872, F1: 0.5997
epoch: 25, total loss: 6.978413343429565
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.9101, Acc: 0.6630, AUC: 0.8869, F1: 0.5750
[VIT] val - Epoch: 25, Loss: 0.7330, Acc: 0.6902, AUC: 0.8909, F1: 0.5846
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8909
Saving vit model...
epoch: 26, total loss: 6.940469145774841
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.9350, Acc: 0.6168, AUC: 0.8832, F1: 0.5327
[VIT] val - Epoch: 26, Loss: 0.7090, Acc: 0.7147, AUC: 0.8926, F1: 0.5963
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8926
Saving vit model...
epoch: 27, total loss: 6.879400889078776
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.9188, Acc: 0.6332, AUC: 0.8834, F1: 0.5525
[VIT] val - Epoch: 27, Loss: 0.7967, Acc: 0.6766, AUC: 0.8884, F1: 0.5425
epoch: 28, total loss: 6.820002794265747
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.9043, Acc: 0.6766, AUC: 0.8848, F1: 0.5376
[VIT] val - Epoch: 28, Loss: 0.7547, Acc: 0.6929, AUC: 0.8869, F1: 0.5932
epoch: 29, total loss: 7.096428672472636
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.7568, Acc: 0.7092, AUC: 0.8858, F1: 0.5887
[VIT] val - Epoch: 29, Loss: 0.7496, Acc: 0.7092, AUC: 0.8843, F1: 0.5929
epoch: 30, total loss: 6.940872152646382
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.8228, Acc: 0.6739, AUC: 0.8892, F1: 0.5890
[VIT] val - Epoch: 30, Loss: 0.7387, Acc: 0.7011, AUC: 0.8867, F1: 0.5652
epoch: 31, total loss: 6.790067632993062
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 1.2991, Acc: 0.5326, AUC: 0.8600, F1: 0.4565
[VIT] val - Epoch: 31, Loss: 0.7880, Acc: 0.6793, AUC: 0.8925, F1: 0.5537
epoch: 32, total loss: 6.948800762494405
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.8274, Acc: 0.6630, AUC: 0.8913, F1: 0.5004
[VIT] val - Epoch: 32, Loss: 0.7493, Acc: 0.6821, AUC: 0.8843, F1: 0.5222
epoch: 33, total loss: 6.943235596021016
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.8388, Acc: 0.6739, AUC: 0.8859, F1: 0.5706
[VIT] val - Epoch: 33, Loss: 0.7509, Acc: 0.7011, AUC: 0.8943, F1: 0.5789
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8943
Saving vit model...
epoch: 34, total loss: 6.8938701550165815
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.7884, Acc: 0.7011, AUC: 0.8942, F1: 0.5938
[VIT] val - Epoch: 34, Loss: 0.7410, Acc: 0.6957, AUC: 0.8977, F1: 0.5889
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8942
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8977
Saving vit model...
epoch: 35, total loss: 6.812617341677348
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.9546, Acc: 0.7092, AUC: 0.8711, F1: 0.5107
[VIT] val - Epoch: 35, Loss: 0.7339, Acc: 0.6793, AUC: 0.8882, F1: 0.5224
epoch: 36, total loss: 6.839862068494161
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.9062, Acc: 0.6658, AUC: 0.8932, F1: 0.5986
[VIT] val - Epoch: 36, Loss: 0.7225, Acc: 0.7201, AUC: 0.8923, F1: 0.6003
epoch: 37, total loss: 6.680138349533081
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8781, Acc: 0.6495, AUC: 0.8941, F1: 0.5893
[VIT] val - Epoch: 37, Loss: 0.7581, Acc: 0.7092, AUC: 0.8934, F1: 0.5816
epoch: 38, total loss: 6.951011101404826
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 1.0389, Acc: 0.6114, AUC: 0.8963, F1: 0.4775
[VIT] val - Epoch: 38, Loss: 0.8008, Acc: 0.6658, AUC: 0.8928, F1: 0.5515
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8963
Saving cnn model...
epoch: 39, total loss: 6.499012231826782
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.9540, Acc: 0.6902, AUC: 0.8858, F1: 0.5829
[VIT] val - Epoch: 39, Loss: 0.8208, Acc: 0.6685, AUC: 0.8957, F1: 0.5950
epoch: 40, total loss: 6.492282072703044
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 1.0134, Acc: 0.6739, AUC: 0.8816, F1: 0.5230
[VIT] val - Epoch: 40, Loss: 0.7377, Acc: 0.7283, AUC: 0.8904, F1: 0.6027
epoch: 41, total loss: 6.369883179664612
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.9306, Acc: 0.6821, AUC: 0.8897, F1: 0.5543
[VIT] val - Epoch: 41, Loss: 0.7335, Acc: 0.7201, AUC: 0.8918, F1: 0.5940
epoch: 42, total loss: 6.489115913709004
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.9826, Acc: 0.6467, AUC: 0.8960, F1: 0.5794
[VIT] val - Epoch: 42, Loss: 0.7317, Acc: 0.7255, AUC: 0.8935, F1: 0.6196
epoch: 43, total loss: 6.55187714099884
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.9462, Acc: 0.6902, AUC: 0.8870, F1: 0.5361
[VIT] val - Epoch: 43, Loss: 0.7467, Acc: 0.6984, AUC: 0.8939, F1: 0.5755
epoch: 44, total loss: 6.522159218788147
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 1.2831, Acc: 0.5679, AUC: 0.8802, F1: 0.4996
[VIT] val - Epoch: 44, Loss: 0.7444, Acc: 0.7283, AUC: 0.8965, F1: 0.5875
epoch: 45, total loss: 6.4840952555338545
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 1.0614, Acc: 0.6304, AUC: 0.8869, F1: 0.5717
[VIT] val - Epoch: 45, Loss: 0.8307, Acc: 0.6685, AUC: 0.8958, F1: 0.5856
epoch: 46, total loss: 6.408359567324321
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 1.1236, Acc: 0.6576, AUC: 0.8821, F1: 0.5497
[VIT] val - Epoch: 46, Loss: 0.7794, Acc: 0.7147, AUC: 0.8904, F1: 0.6113
epoch: 47, total loss: 6.318350116411845
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 1.1338, Acc: 0.6033, AUC: 0.8902, F1: 0.5419
[VIT] val - Epoch: 47, Loss: 0.8216, Acc: 0.6957, AUC: 0.8949, F1: 0.5931
epoch: 48, total loss: 6.083613276481628
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.9731, Acc: 0.6766, AUC: 0.8976, F1: 0.5472
[VIT] val - Epoch: 48, Loss: 0.7856, Acc: 0.7147, AUC: 0.8928, F1: 0.6153
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8976
Saving cnn model...
epoch: 49, total loss: 6.146723786989848
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 1.0878, Acc: 0.6712, AUC: 0.8855, F1: 0.5587
[VIT] val - Epoch: 49, Loss: 0.7551, Acc: 0.7065, AUC: 0.8950, F1: 0.6155
epoch: 50, total loss: 6.322744647661845
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 1.1348, Acc: 0.6712, AUC: 0.8787, F1: 0.6021
[VIT] val - Epoch: 50, Loss: 0.7714, Acc: 0.7120, AUC: 0.8955, F1: 0.6048
epoch: 51, total loss: 6.12975029150645
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 1.1573, Acc: 0.6739, AUC: 0.8762, F1: 0.5831
[VIT] val - Epoch: 51, Loss: 0.8033, Acc: 0.7337, AUC: 0.8925, F1: 0.6431
epoch: 52, total loss: 6.094117840131124
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 1.0466, Acc: 0.6793, AUC: 0.8899, F1: 0.5354
[VIT] val - Epoch: 52, Loss: 0.8213, Acc: 0.6902, AUC: 0.8968, F1: 0.6055
epoch: 53, total loss: 6.158297499020894
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 1.0754, Acc: 0.6984, AUC: 0.8838, F1: 0.5531
[VIT] val - Epoch: 53, Loss: 0.8050, Acc: 0.7174, AUC: 0.8976, F1: 0.6185
epoch: 54, total loss: 6.179939826329549
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 1.0949, Acc: 0.6440, AUC: 0.8839, F1: 0.5369
[VIT] val - Epoch: 54, Loss: 0.7676, Acc: 0.7147, AUC: 0.8969, F1: 0.6015
epoch: 55, total loss: 6.315059860547383
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 1.2412, Acc: 0.5978, AUC: 0.8761, F1: 0.4890
[VIT] val - Epoch: 55, Loss: 0.7726, Acc: 0.7174, AUC: 0.8949, F1: 0.5986
epoch: 56, total loss: 6.224268396695455
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 1.0922, Acc: 0.6848, AUC: 0.8854, F1: 0.5839
[VIT] val - Epoch: 56, Loss: 0.7881, Acc: 0.7310, AUC: 0.8903, F1: 0.6131
epoch: 57, total loss: 6.266623655954997
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 1.2960, Acc: 0.5951, AUC: 0.8789, F1: 0.5288
[VIT] val - Epoch: 57, Loss: 0.7995, Acc: 0.7092, AUC: 0.8904, F1: 0.5695
epoch: 58, total loss: 5.99865996837616
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 1.1948, Acc: 0.7255, AUC: 0.8906, F1: 0.5743
[VIT] val - Epoch: 58, Loss: 0.7861, Acc: 0.7011, AUC: 0.8922, F1: 0.5856
epoch: 59, total loss: 5.976444602012634
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 1.0197, Acc: 0.7120, AUC: 0.8913, F1: 0.5932
[VIT] val - Epoch: 59, Loss: 0.7755, Acc: 0.7337, AUC: 0.8971, F1: 0.6340
epoch: 60, total loss: 5.914509455362956
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 1.1032, Acc: 0.6304, AUC: 0.8928, F1: 0.5542
[VIT] val - Epoch: 60, Loss: 0.8015, Acc: 0.7201, AUC: 0.8972, F1: 0.6104
epoch: 61, total loss: 5.823562224706014
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 1.0725, Acc: 0.6685, AUC: 0.9035, F1: 0.5816
[VIT] val - Epoch: 61, Loss: 0.7918, Acc: 0.7391, AUC: 0.8989, F1: 0.6540
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9035
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8989
Saving vit model...
epoch: 62, total loss: 6.007667223612468
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 1.1841, Acc: 0.6821, AUC: 0.8812, F1: 0.5573
[VIT] val - Epoch: 62, Loss: 0.7938, Acc: 0.7310, AUC: 0.8990, F1: 0.6286
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8990
Saving vit model...
epoch: 63, total loss: 5.733460903167725
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 1.2271, Acc: 0.6495, AUC: 0.8974, F1: 0.5496
[VIT] val - Epoch: 63, Loss: 0.8804, Acc: 0.6929, AUC: 0.8987, F1: 0.6016
epoch: 64, total loss: 5.973288893699646
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 1.2877, Acc: 0.6386, AUC: 0.8976, F1: 0.5832
[VIT] val - Epoch: 64, Loss: 0.8578, Acc: 0.6902, AUC: 0.8988, F1: 0.6174
epoch: 65, total loss: 5.984549363454183
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 1.2387, Acc: 0.6440, AUC: 0.8983, F1: 0.5645
[VIT] val - Epoch: 65, Loss: 0.8420, Acc: 0.7065, AUC: 0.9007, F1: 0.6395
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9007
Saving vit model...
epoch: 66, total loss: 5.71186089515686
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 1.2406, Acc: 0.6821, AUC: 0.8916, F1: 0.5996
[VIT] val - Epoch: 66, Loss: 0.8002, Acc: 0.7310, AUC: 0.9049, F1: 0.6488
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9049
Saving vit model...
epoch: 67, total loss: 5.924607038497925
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 1.1207, Acc: 0.6739, AUC: 0.8844, F1: 0.5641
[VIT] val - Epoch: 67, Loss: 0.7776, Acc: 0.7310, AUC: 0.9047, F1: 0.6514
epoch: 68, total loss: 5.651835838953654
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 1.2855, Acc: 0.7120, AUC: 0.8856, F1: 0.5669
[VIT] val - Epoch: 68, Loss: 0.7754, Acc: 0.7228, AUC: 0.8995, F1: 0.6072
epoch: 69, total loss: 5.55723504225413
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 1.3357, Acc: 0.6495, AUC: 0.8927, F1: 0.5525
[VIT] val - Epoch: 69, Loss: 0.7986, Acc: 0.7337, AUC: 0.8937, F1: 0.5712
epoch: 70, total loss: 5.7844841082890825
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 1.1394, Acc: 0.6739, AUC: 0.8935, F1: 0.5640
[VIT] val - Epoch: 70, Loss: 0.7834, Acc: 0.7255, AUC: 0.9008, F1: 0.6251
epoch: 71, total loss: 5.540367364883423
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 1.1563, Acc: 0.7120, AUC: 0.8881, F1: 0.5978
[VIT] val - Epoch: 71, Loss: 0.7951, Acc: 0.7364, AUC: 0.8993, F1: 0.6200
epoch: 72, total loss: 5.472680886586507
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 1.1702, Acc: 0.7120, AUC: 0.8915, F1: 0.6065
[VIT] val - Epoch: 72, Loss: 0.8415, Acc: 0.7201, AUC: 0.9003, F1: 0.6023
epoch: 73, total loss: 5.695039828618367
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 1.1792, Acc: 0.6821, AUC: 0.8884, F1: 0.5808
[VIT] val - Epoch: 73, Loss: 0.8720, Acc: 0.7255, AUC: 0.8983, F1: 0.6314
epoch: 74, total loss: 5.54862646261851
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 1.2276, Acc: 0.7038, AUC: 0.8854, F1: 0.5920
[VIT] val - Epoch: 74, Loss: 0.8487, Acc: 0.7364, AUC: 0.8958, F1: 0.6238
epoch: 75, total loss: 5.540973941485087
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 1.1444, Acc: 0.6821, AUC: 0.8877, F1: 0.5766
[VIT] val - Epoch: 75, Loss: 0.8325, Acc: 0.7391, AUC: 0.8972, F1: 0.6278
epoch: 76, total loss: 5.519315401713054
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 1.1550, Acc: 0.6739, AUC: 0.8863, F1: 0.5710
[VIT] val - Epoch: 76, Loss: 0.8177, Acc: 0.7283, AUC: 0.8950, F1: 0.6143
epoch: 77, total loss: 5.402465899785359
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 1.2254, Acc: 0.6576, AUC: 0.8848, F1: 0.5600
[VIT] val - Epoch: 77, Loss: 0.8588, Acc: 0.7337, AUC: 0.8980, F1: 0.6408
epoch: 78, total loss: 5.238853732744853
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 1.4127, Acc: 0.6984, AUC: 0.8646, F1: 0.5807
[VIT] val - Epoch: 78, Loss: 0.8586, Acc: 0.7391, AUC: 0.8995, F1: 0.6630
epoch: 79, total loss: 5.424184083938599
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 1.3468, Acc: 0.6793, AUC: 0.8724, F1: 0.5610
[VIT] val - Epoch: 79, Loss: 0.8888, Acc: 0.7337, AUC: 0.8990, F1: 0.6544
epoch: 80, total loss: 5.4108795921007795
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 1.2822, Acc: 0.6712, AUC: 0.8794, F1: 0.5772
[VIT] val - Epoch: 80, Loss: 0.8972, Acc: 0.7364, AUC: 0.8957, F1: 0.6590
epoch: 81, total loss: 5.358940164248149
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 1.1710, Acc: 0.6712, AUC: 0.8806, F1: 0.5567
[VIT] val - Epoch: 81, Loss: 0.8731, Acc: 0.7418, AUC: 0.8988, F1: 0.6615
epoch: 82, total loss: 5.368710835774739
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 1.3069, Acc: 0.6549, AUC: 0.8865, F1: 0.5626
[VIT] val - Epoch: 82, Loss: 0.8552, Acc: 0.7446, AUC: 0.8995, F1: 0.6510
epoch: 83, total loss: 5.55921729405721
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 1.2840, Acc: 0.6984, AUC: 0.8843, F1: 0.5901
[VIT] val - Epoch: 83, Loss: 0.8470, Acc: 0.7418, AUC: 0.8964, F1: 0.6334
epoch: 84, total loss: 5.500667492548625
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 1.3619, Acc: 0.6630, AUC: 0.8814, F1: 0.5508
[VIT] val - Epoch: 84, Loss: 0.8585, Acc: 0.7609, AUC: 0.8930, F1: 0.6586
epoch: 85, total loss: 5.47477259238561
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 1.3910, Acc: 0.7147, AUC: 0.8836, F1: 0.5768
[VIT] val - Epoch: 85, Loss: 0.8615, Acc: 0.7500, AUC: 0.8934, F1: 0.6367
epoch: 86, total loss: 5.171297550201416
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 1.4521, Acc: 0.6250, AUC: 0.8791, F1: 0.5283
[VIT] val - Epoch: 86, Loss: 0.8568, Acc: 0.7446, AUC: 0.8947, F1: 0.6312
epoch: 87, total loss: 5.455510814984639
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 1.4303, Acc: 0.6766, AUC: 0.8828, F1: 0.5649
[VIT] val - Epoch: 87, Loss: 0.8810, Acc: 0.7364, AUC: 0.8917, F1: 0.6070
epoch: 88, total loss: 5.10120411713918
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 1.3799, Acc: 0.6712, AUC: 0.8863, F1: 0.5582
[VIT] val - Epoch: 88, Loss: 0.9002, Acc: 0.7337, AUC: 0.8980, F1: 0.6422
epoch: 89, total loss: 5.2720595598220825
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 1.4579, Acc: 0.6277, AUC: 0.8828, F1: 0.5401
[VIT] val - Epoch: 89, Loss: 0.8988, Acc: 0.7201, AUC: 0.8951, F1: 0.6352
epoch: 90, total loss: 5.184296568234761
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 1.4338, Acc: 0.6929, AUC: 0.8810, F1: 0.5573
[VIT] val - Epoch: 90, Loss: 0.9517, Acc: 0.7174, AUC: 0.8933, F1: 0.6340
epoch: 91, total loss: 5.537750283877055
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 1.3774, Acc: 0.6685, AUC: 0.8821, F1: 0.5483
[VIT] val - Epoch: 91, Loss: 0.8991, Acc: 0.7418, AUC: 0.8913, F1: 0.6425
epoch: 92, total loss: 5.394010603427887
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 1.3536, Acc: 0.6875, AUC: 0.8854, F1: 0.5767
[VIT] val - Epoch: 92, Loss: 0.9218, Acc: 0.7446, AUC: 0.8922, F1: 0.6335
epoch: 93, total loss: 5.333178758621216
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 1.3461, Acc: 0.6712, AUC: 0.8823, F1: 0.5580
[VIT] val - Epoch: 93, Loss: 0.9304, Acc: 0.7391, AUC: 0.8915, F1: 0.6223
epoch: 94, total loss: 5.000248332818349
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 1.4481, Acc: 0.6658, AUC: 0.8769, F1: 0.5647
[VIT] val - Epoch: 94, Loss: 0.8963, Acc: 0.7337, AUC: 0.8922, F1: 0.6222
epoch: 95, total loss: 5.210055947303772
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 1.4563, Acc: 0.6902, AUC: 0.8774, F1: 0.5781
[VIT] val - Epoch: 95, Loss: 0.9119, Acc: 0.7391, AUC: 0.8921, F1: 0.6318
epoch: 96, total loss: 5.2843629121780396
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 1.4692, Acc: 0.6984, AUC: 0.8794, F1: 0.5839
[VIT] val - Epoch: 96, Loss: 0.9164, Acc: 0.7418, AUC: 0.8904, F1: 0.6132
epoch: 97, total loss: 4.886565208435059
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 1.4140, Acc: 0.6821, AUC: 0.8825, F1: 0.5561
[VIT] val - Epoch: 97, Loss: 0.9166, Acc: 0.7418, AUC: 0.8910, F1: 0.6382
epoch: 98, total loss: 5.144083380699158
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 1.4090, Acc: 0.6685, AUC: 0.8857, F1: 0.5594
[VIT] val - Epoch: 98, Loss: 0.9297, Acc: 0.7391, AUC: 0.8892, F1: 0.6206
epoch: 99, total loss: 5.09289813041687
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 1.4480, Acc: 0.6739, AUC: 0.8843, F1: 0.5579
[VIT] val - Epoch: 99, Loss: 0.9292, Acc: 0.7228, AUC: 0.8955, F1: 0.6334
epoch: 100, total loss: 5.09102996190389
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 1.4274, Acc: 0.6984, AUC: 0.8818, F1: 0.5796
[VIT] val - Epoch: 100, Loss: 0.9609, Acc: 0.7364, AUC: 0.8874, F1: 0.6300
[16:24:04][Rank 1] Training Finished. Starting Final Testing...
[16:24:04][Rank 3] Training Finished. Starting Final Testing...[16:24:04][Rank 2] Training Finished. Starting Final Testing...

[16:24:04][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test_cnn - Epoch: 100, Loss: 7.8545, Acc: 0.0719, AUC: 0.6259, F1: 0.0751
[VIT] test_cnn - Epoch: 100, Loss: 5.9694, Acc: 0.0773, AUC: 0.6679, F1: 0.0807
üöÄ Final Test Results [CNN] - AUC: 0.6259, Acc: 0.0719, F1: 0.0751
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
‚ùå [ÈîôËØØ] Ê∫êÂüü FGADR ËÆ≠ÁªÉÂ§±Ë¥•ÔºÅ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: IDRID
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='IDRID', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 32
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['IDRID']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_IDRID
OUT_DIR: ./output_esdg_h100/IDRID
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA/output_esdg_h100/IDRID/CASS_GDRNet_ESDG_IDRID
[16:40:01][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
[16:40:01][Rank 3] Loading datasets...[16:40:01][Rank 2] Loading datasets...

================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
[16:40:01][Rank 1] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53MInjecting LoRA into: layer.9.attention.v_proj

Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
epoch: 1, total loss: 10.80656886100769
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.1974, Acc: 0.5243, AUC: 0.7581, F1: 0.3288
[VIT] val - Epoch: 1, Loss: 1.3873, Acc: 0.4175, AUC: 0.7632, F1: 0.2010
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7581
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7632
Saving vit model...
epoch: 2, total loss: 9.091437816619873
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.0358, Acc: 0.6311, AUC: 0.8546, F1: 0.4462
[VIT] val - Epoch: 2, Loss: 1.3265, Acc: 0.4078, AUC: 0.7956, F1: 0.1767
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8546
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7956
Saving vit model...
epoch: 3, total loss: 9.08200478553772
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 1.0874, Acc: 0.5825, AUC: 0.7774, F1: 0.3414
[VIT] val - Epoch: 3, Loss: 1.2517, Acc: 0.5437, AUC: 0.8122, F1: 0.3272
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8122
Saving vit model...
epoch: 4, total loss: 8.834993481636047
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.9014, Acc: 0.5631, AUC: 0.7652, F1: 0.3258
[VIT] val - Epoch: 4, Loss: 1.1230, Acc: 0.6117, AUC: 0.8185, F1: 0.4022
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8185
Saving vit model...
epoch: 5, total loss: 8.58649730682373
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.3436, Acc: 0.5825, AUC: 0.8033, F1: 0.2852
[VIT] val - Epoch: 5, Loss: 0.9914, Acc: 0.6311, AUC: 0.8337, F1: 0.4598
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8337
Saving vit model...
epoch: 6, total loss: 8.343181371688843
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.0093, Acc: 0.5631, AUC: 0.7968, F1: 0.3674
[VIT] val - Epoch: 6, Loss: 0.9530, Acc: 0.6311, AUC: 0.8557, F1: 0.4780
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8557
Saving vit model...
epoch: 7, total loss: 8.643618106842041
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.9474, Acc: 0.5631, AUC: 0.8243, F1: 0.4120
[VIT] val - Epoch: 7, Loss: 0.9431, Acc: 0.6699, AUC: 0.8644, F1: 0.5054
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8644
Saving vit model...
epoch: 8, total loss: 8.256343603134155
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 1.2318, Acc: 0.6602, AUC: 0.7883, F1: 0.4467
[VIT] val - Epoch: 8, Loss: 0.9711, Acc: 0.6602, AUC: 0.8606, F1: 0.4849
epoch: 9, total loss: 7.925653696060181
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 1.0921, Acc: 0.6117, AUC: 0.8012, F1: 0.4536
[VIT] val - Epoch: 9, Loss: 1.0010, Acc: 0.6214, AUC: 0.8739, F1: 0.4933
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8739
Saving vit model...
epoch: 10, total loss: 7.902767896652222
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.8820, Acc: 0.6796, AUC: 0.7179, F1: 0.5108
[VIT] val - Epoch: 10, Loss: 0.8563, Acc: 0.6796, AUC: 0.8633, F1: 0.5247
epoch: 11, total loss: 7.5954612493515015
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.8711, Acc: 0.6408, AUC: 0.7370, F1: 0.4511
[VIT] val - Epoch: 11, Loss: 0.8479, Acc: 0.6408, AUC: 0.8629, F1: 0.4567
epoch: 12, total loss: 7.944294452667236
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.8013, Acc: 0.6796, AUC: 0.8092, F1: 0.4787
[VIT] val - Epoch: 12, Loss: 0.8339, Acc: 0.6602, AUC: 0.8616, F1: 0.4943
epoch: 13, total loss: 7.679166316986084
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.9367, Acc: 0.6408, AUC: 0.8435, F1: 0.4544
[VIT] val - Epoch: 13, Loss: 0.8783, Acc: 0.7184, AUC: 0.8760, F1: 0.5367
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8760
Saving vit model...
epoch: 14, total loss: 7.841942548751831
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 1.2354, Acc: 0.5728, AUC: 0.8461, F1: 0.4328
[VIT] val - Epoch: 14, Loss: 0.8974, Acc: 0.6602, AUC: 0.8766, F1: 0.4989
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8766
Saving vit model...
epoch: 15, total loss: 8.017771363258362
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 1.2083, Acc: 0.4951, AUC: 0.8575, F1: 0.3178
[VIT] val - Epoch: 15, Loss: 0.8295, Acc: 0.6699, AUC: 0.8568, F1: 0.4786
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8575
Saving cnn model...
epoch: 16, total loss: 7.7837512493133545
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.9574, Acc: 0.6019, AUC: 0.8263, F1: 0.4261
[VIT] val - Epoch: 16, Loss: 0.8325, Acc: 0.6796, AUC: 0.8545, F1: 0.4662
epoch: 17, total loss: 7.804695248603821
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 1.1593, Acc: 0.5825, AUC: 0.8222, F1: 0.4189
[VIT] val - Epoch: 17, Loss: 0.8071, Acc: 0.6602, AUC: 0.8700, F1: 0.4929
epoch: 18, total loss: 7.722643256187439
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 1.0345, Acc: 0.4951, AUC: 0.8416, F1: 0.3869
[VIT] val - Epoch: 18, Loss: 0.7573, Acc: 0.6408, AUC: 0.8637, F1: 0.4483
epoch: 19, total loss: 7.686081171035767
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.9313, Acc: 0.6019, AUC: 0.8502, F1: 0.4008
[VIT] val - Epoch: 19, Loss: 0.8158, Acc: 0.6796, AUC: 0.8743, F1: 0.4839
epoch: 20, total loss: 8.464760661125183
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.9469, Acc: 0.6214, AUC: 0.8488, F1: 0.4522
[VIT] val - Epoch: 20, Loss: 0.8437, Acc: 0.6408, AUC: 0.8852, F1: 0.5214
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8852
Saving vit model...
epoch: 21, total loss: 7.993310451507568
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.9862, Acc: 0.5825, AUC: 0.8303, F1: 0.4006
[VIT] val - Epoch: 21, Loss: 0.7357, Acc: 0.6796, AUC: 0.8784, F1: 0.5113
epoch: 22, total loss: 7.495673298835754
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.9242, Acc: 0.6117, AUC: 0.8459, F1: 0.3868
[VIT] val - Epoch: 22, Loss: 0.7259, Acc: 0.6699, AUC: 0.8703, F1: 0.4819
epoch: 23, total loss: 7.936779975891113
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.8975, Acc: 0.6408, AUC: 0.8427, F1: 0.3888
[VIT] val - Epoch: 23, Loss: 0.7778, Acc: 0.6699, AUC: 0.8775, F1: 0.4951
epoch: 24, total loss: 7.506984233856201
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 1.2442, Acc: 0.6019, AUC: 0.8099, F1: 0.3231
[VIT] val - Epoch: 24, Loss: 0.8209, Acc: 0.6699, AUC: 0.8574, F1: 0.4679
epoch: 25, total loss: 7.300125241279602
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.9134, Acc: 0.6505, AUC: 0.8044, F1: 0.4573
[VIT] val - Epoch: 25, Loss: 0.8018, Acc: 0.6602, AUC: 0.8587, F1: 0.4594
epoch: 26, total loss: 7.6827592849731445
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.9626, Acc: 0.6311, AUC: 0.8642, F1: 0.4739
[VIT] val - Epoch: 26, Loss: 0.7736, Acc: 0.6796, AUC: 0.8711, F1: 0.5233
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8642
Saving cnn model...
epoch: 27, total loss: 7.822272539138794
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.8458, Acc: 0.6019, AUC: 0.8768, F1: 0.4444
[VIT] val - Epoch: 27, Loss: 0.7342, Acc: 0.6699, AUC: 0.8680, F1: 0.4995
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8768
Saving cnn model...
epoch: 28, total loss: 7.692367315292358
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 1.1632, Acc: 0.6408, AUC: 0.8359, F1: 0.3902
[VIT] val - Epoch: 28, Loss: 0.7987, Acc: 0.6699, AUC: 0.8537, F1: 0.4530
epoch: 29, total loss: 7.397295832633972
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 1.1000, Acc: 0.5825, AUC: 0.8594, F1: 0.3251
[VIT] val - Epoch: 29, Loss: 0.7642, Acc: 0.6505, AUC: 0.8837, F1: 0.4788
epoch: 30, total loss: 7.285004019737244
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.8454, Acc: 0.6602, AUC: 0.8932, F1: 0.4855
[VIT] val - Epoch: 30, Loss: 0.7453, Acc: 0.6699, AUC: 0.8786, F1: 0.4999
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8932
Saving cnn model...
epoch: 31, total loss: 7.46954607963562
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.7723, Acc: 0.6408, AUC: 0.8893, F1: 0.4849
[VIT] val - Epoch: 31, Loss: 0.7229, Acc: 0.6796, AUC: 0.8695, F1: 0.4854
epoch: 32, total loss: 7.395912170410156
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.8887, Acc: 0.6117, AUC: 0.8830, F1: 0.4651
[VIT] val - Epoch: 32, Loss: 0.6997, Acc: 0.6893, AUC: 0.8855, F1: 0.5281
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8855
Saving vit model...
epoch: 33, total loss: 7.626592993736267
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.9784, Acc: 0.6214, AUC: 0.8559, F1: 0.3606
[VIT] val - Epoch: 33, Loss: 0.7145, Acc: 0.6602, AUC: 0.8916, F1: 0.4865
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8916
Saving vit model...
epoch: 34, total loss: 7.045034170150757
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.8948, Acc: 0.6311, AUC: 0.8740, F1: 0.4614
[VIT] val - Epoch: 34, Loss: 0.7136, Acc: 0.6990, AUC: 0.8839, F1: 0.5415
epoch: 35, total loss: 7.013131737709045
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 1.4415, Acc: 0.6117, AUC: 0.7957, F1: 0.3804
[VIT] val - Epoch: 35, Loss: 0.7550, Acc: 0.6990, AUC: 0.8896, F1: 0.5528
epoch: 36, total loss: 7.653604030609131
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.8678, Acc: 0.6796, AUC: 0.8335, F1: 0.4757
[VIT] val - Epoch: 36, Loss: 0.7423, Acc: 0.7087, AUC: 0.8726, F1: 0.5187
epoch: 37, total loss: 6.679319500923157
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8381, Acc: 0.6311, AUC: 0.8601, F1: 0.4685
[VIT] val - Epoch: 37, Loss: 0.7229, Acc: 0.7184, AUC: 0.8838, F1: 0.5514
epoch: 38, total loss: 6.920801281929016
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 1.1646, Acc: 0.5146, AUC: 0.8342, F1: 0.4023
[VIT] val - Epoch: 38, Loss: 0.6911, Acc: 0.6602, AUC: 0.8806, F1: 0.4847
epoch: 39, total loss: 7.342310667037964
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 1.5923, Acc: 0.4854, AUC: 0.8102, F1: 0.3728
[VIT] val - Epoch: 39, Loss: 0.7985, Acc: 0.6602, AUC: 0.8847, F1: 0.6038
epoch: 40, total loss: 6.999661087989807
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 1.1182, Acc: 0.5243, AUC: 0.8352, F1: 0.4556
[VIT] val - Epoch: 40, Loss: 0.8401, Acc: 0.6990, AUC: 0.8656, F1: 0.5099
epoch: 41, total loss: 7.4458794593811035
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.9294, Acc: 0.6311, AUC: 0.8381, F1: 0.4648
[VIT] val - Epoch: 41, Loss: 0.7314, Acc: 0.6796, AUC: 0.8778, F1: 0.5023
epoch: 42, total loss: 6.81969428062439
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.8846, Acc: 0.6602, AUC: 0.8448, F1: 0.4722
[VIT] val - Epoch: 42, Loss: 0.7238, Acc: 0.6602, AUC: 0.8762, F1: 0.4928
epoch: 43, total loss: 6.883184432983398
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.9478, Acc: 0.6699, AUC: 0.8401, F1: 0.4448
[VIT] val - Epoch: 43, Loss: 0.7881, Acc: 0.6893, AUC: 0.8621, F1: 0.4684
epoch: 44, total loss: 7.476795077323914
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 1.0900, Acc: 0.6311, AUC: 0.8385, F1: 0.4417
[VIT] val - Epoch: 44, Loss: 0.7713, Acc: 0.6893, AUC: 0.8886, F1: 0.5441
epoch: 45, total loss: 7.5373982191085815
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 1.0480, Acc: 0.5631, AUC: 0.8206, F1: 0.3873
[VIT] val - Epoch: 45, Loss: 0.7331, Acc: 0.6796, AUC: 0.8759, F1: 0.5291
epoch: 46, total loss: 7.635399341583252
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 1.1902, Acc: 0.5340, AUC: 0.8040, F1: 0.3981
[VIT] val - Epoch: 46, Loss: 0.7493, Acc: 0.6796, AUC: 0.8779, F1: 0.5300
epoch: 47, total loss: 6.961685419082642
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 1.0761, Acc: 0.5243, AUC: 0.7985, F1: 0.3857
[VIT] val - Epoch: 47, Loss: 0.7924, Acc: 0.6602, AUC: 0.8770, F1: 0.5130
epoch: 48, total loss: 6.7700968980789185
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 1.2398, Acc: 0.6214, AUC: 0.8009, F1: 0.4088
[VIT] val - Epoch: 48, Loss: 0.7727, Acc: 0.6796, AUC: 0.8603, F1: 0.5143
epoch: 49, total loss: 6.713180065155029
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 1.0432, Acc: 0.6019, AUC: 0.8037, F1: 0.4056
[VIT] val - Epoch: 49, Loss: 0.8469, Acc: 0.6311, AUC: 0.8826, F1: 0.4851
epoch: 50, total loss: 6.908094525337219
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.9911, Acc: 0.6214, AUC: 0.8363, F1: 0.4383
[VIT] val - Epoch: 50, Loss: 0.7699, Acc: 0.6311, AUC: 0.8773, F1: 0.4613
epoch: 51, total loss: 6.7129716873168945
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.9564, Acc: 0.6311, AUC: 0.8508, F1: 0.4432
[VIT] val - Epoch: 51, Loss: 0.8226, Acc: 0.6311, AUC: 0.8783, F1: 0.4718
epoch: 52, total loss: 6.466343283653259
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 1.0539, Acc: 0.6505, AUC: 0.8312, F1: 0.4335
[VIT] val - Epoch: 52, Loss: 0.7721, Acc: 0.6796, AUC: 0.8762, F1: 0.5133
epoch: 53, total loss: 7.121279239654541
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.9737, Acc: 0.6117, AUC: 0.8452, F1: 0.4471
[VIT] val - Epoch: 53, Loss: 0.7526, Acc: 0.6796, AUC: 0.8658, F1: 0.4906
epoch: 54, total loss: 6.628716468811035
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 1.0756, Acc: 0.5728, AUC: 0.8074, F1: 0.3803
[VIT] val - Epoch: 54, Loss: 0.7968, Acc: 0.6214, AUC: 0.8799, F1: 0.4722
epoch: 55, total loss: 7.009248495101929
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 1.1693, Acc: 0.5825, AUC: 0.7941, F1: 0.3925
[VIT] val - Epoch: 55, Loss: 0.7636, Acc: 0.6699, AUC: 0.8680, F1: 0.4995
epoch: 56, total loss: 7.133528351783752
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 1.8036, Acc: 0.4757, AUC: 0.8083, F1: 0.3449
[VIT] val - Epoch: 56, Loss: 0.7449, Acc: 0.6990, AUC: 0.8627, F1: 0.5242
epoch: 57, total loss: 7.127202749252319
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.9073, Acc: 0.6602, AUC: 0.8309, F1: 0.5297
[VIT] val - Epoch: 57, Loss: 0.8522, Acc: 0.6699, AUC: 0.8720, F1: 0.5058
epoch: 58, total loss: 7.123540163040161
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 1.1110, Acc: 0.6214, AUC: 0.7868, F1: 0.4084
[VIT] val - Epoch: 58, Loss: 0.7756, Acc: 0.6699, AUC: 0.8569, F1: 0.4884
epoch: 59, total loss: 6.8640806674957275
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 1.0607, Acc: 0.6408, AUC: 0.8068, F1: 0.4266
[VIT] val - Epoch: 59, Loss: 0.7393, Acc: 0.6505, AUC: 0.8668, F1: 0.4827
epoch: 60, total loss: 6.906448125839233
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 1.2322, Acc: 0.5340, AUC: 0.8000, F1: 0.2736
[VIT] val - Epoch: 60, Loss: 0.7897, Acc: 0.6796, AUC: 0.8731, F1: 0.5244
epoch: 61, total loss: 7.11226224899292
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 1.0604, Acc: 0.5728, AUC: 0.7868, F1: 0.3874
[VIT] val - Epoch: 61, Loss: 0.8312, Acc: 0.6408, AUC: 0.8615, F1: 0.4669
epoch: 62, total loss: 6.831551432609558
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.9877, Acc: 0.6311, AUC: 0.7800, F1: 0.4339
[VIT] val - Epoch: 62, Loss: 0.7847, Acc: 0.6893, AUC: 0.8660, F1: 0.5318
epoch: 63, total loss: 6.74243426322937
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 1.0270, Acc: 0.6408, AUC: 0.7682, F1: 0.4475
[VIT] val - Epoch: 63, Loss: 0.7653, Acc: 0.6796, AUC: 0.8584, F1: 0.5117
epoch: 64, total loss: 6.334847927093506
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 1.1200, Acc: 0.5437, AUC: 0.7940, F1: 0.4227
[VIT] val - Epoch: 64, Loss: 0.7775, Acc: 0.6699, AUC: 0.8605, F1: 0.5100
epoch: 65, total loss: 6.467692494392395
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 1.0657, Acc: 0.6019, AUC: 0.7953, F1: 0.4331
[VIT] val - Epoch: 65, Loss: 0.8257, Acc: 0.6214, AUC: 0.8659, F1: 0.4583
epoch: 66, total loss: 6.4328309297561646
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 1.1150, Acc: 0.6408, AUC: 0.7843, F1: 0.4582
[VIT] val - Epoch: 66, Loss: 0.8195, Acc: 0.6311, AUC: 0.8667, F1: 0.4777
epoch: 67, total loss: 6.182973265647888
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 1.1434, Acc: 0.6214, AUC: 0.7980, F1: 0.4477
[VIT] val - Epoch: 67, Loss: 0.7804, Acc: 0.6796, AUC: 0.8587, F1: 0.5231
epoch: 68, total loss: 6.554014682769775
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 1.3000, Acc: 0.5728, AUC: 0.7938, F1: 0.3895
[VIT] val - Epoch: 68, Loss: 0.7749, Acc: 0.6602, AUC: 0.8630, F1: 0.4989
epoch: 69, total loss: 7.097945332527161
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 1.1084, Acc: 0.5728, AUC: 0.8275, F1: 0.4383
[VIT] val - Epoch: 69, Loss: 0.7851, Acc: 0.6602, AUC: 0.8632, F1: 0.4901
epoch: 70, total loss: 5.969140708446503
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.9850, Acc: 0.6408, AUC: 0.8125, F1: 0.4864
[VIT] val - Epoch: 70, Loss: 0.8219, Acc: 0.6408, AUC: 0.8699, F1: 0.4720
epoch: 71, total loss: 6.681971192359924
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 1.0634, Acc: 0.6311, AUC: 0.8039, F1: 0.4326
[VIT] val - Epoch: 71, Loss: 0.7681, Acc: 0.6602, AUC: 0.8628, F1: 0.4993
epoch: 72, total loss: 5.992922782897949
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 1.3160, Acc: 0.5146, AUC: 0.8356, F1: 0.3706
[VIT] val - Epoch: 72, Loss: 0.7716, Acc: 0.6699, AUC: 0.8567, F1: 0.5075
epoch: 73, total loss: 6.603596091270447
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 1.2987, Acc: 0.4951, AUC: 0.8586, F1: 0.4139
[VIT] val - Epoch: 73, Loss: 0.7861, Acc: 0.6602, AUC: 0.8674, F1: 0.5019
epoch: 74, total loss: 6.555750846862793
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.9763, Acc: 0.6311, AUC: 0.8536, F1: 0.5002
[VIT] val - Epoch: 74, Loss: 0.7897, Acc: 0.6505, AUC: 0.8667, F1: 0.5068
epoch: 75, total loss: 6.0094040632247925
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 1.1131, Acc: 0.6311, AUC: 0.7887, F1: 0.4715
[VIT] val - Epoch: 75, Loss: 0.8009, Acc: 0.6602, AUC: 0.8539, F1: 0.5200
epoch: 76, total loss: 6.78898811340332
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 1.0134, Acc: 0.6408, AUC: 0.8319, F1: 0.4586
[VIT] val - Epoch: 76, Loss: 0.8167, Acc: 0.6408, AUC: 0.8557, F1: 0.4745
epoch: 77, total loss: 6.401313662528992
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 1.1148, Acc: 0.5631, AUC: 0.8283, F1: 0.4500
[VIT] val - Epoch: 77, Loss: 0.7919, Acc: 0.6893, AUC: 0.8553, F1: 0.5287
epoch: 78, total loss: 7.04647159576416
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 1.2319, Acc: 0.5534, AUC: 0.8180, F1: 0.3846
[VIT] val - Epoch: 78, Loss: 0.8220, Acc: 0.6602, AUC: 0.8455, F1: 0.4829
epoch: 79, total loss: 5.830858588218689
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 1.3490, Acc: 0.5922, AUC: 0.8208, F1: 0.3719
[VIT] val - Epoch: 79, Loss: 0.8519, Acc: 0.6408, AUC: 0.8528, F1: 0.4878
epoch: 80, total loss: 7.283604145050049
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 1.1498, Acc: 0.6408, AUC: 0.8124, F1: 0.4772
[VIT] val - Epoch: 80, Loss: 0.8258, Acc: 0.6602, AUC: 0.8554, F1: 0.4965
epoch: 81, total loss: 5.670027852058411
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 1.1806, Acc: 0.6505, AUC: 0.8093, F1: 0.4905
[VIT] val - Epoch: 81, Loss: 0.7972, Acc: 0.6505, AUC: 0.8584, F1: 0.4969
epoch: 82, total loss: 5.367620348930359
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 1.2785, Acc: 0.6019, AUC: 0.8125, F1: 0.3929
[VIT] val - Epoch: 82, Loss: 0.7883, Acc: 0.6893, AUC: 0.8660, F1: 0.5399
epoch: 83, total loss: 5.525157332420349
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 1.3152, Acc: 0.5728, AUC: 0.8032, F1: 0.3838
[VIT] val - Epoch: 83, Loss: 0.8067, Acc: 0.6505, AUC: 0.8688, F1: 0.4824
epoch: 84, total loss: 6.787238955497742
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 1.4519, Acc: 0.5922, AUC: 0.7680, F1: 0.3814
[VIT] val - Epoch: 84, Loss: 0.8520, Acc: 0.6699, AUC: 0.8682, F1: 0.5136
epoch: 85, total loss: 6.249494552612305
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 1.5764, Acc: 0.5631, AUC: 0.8207, F1: 0.4051
[VIT] val - Epoch: 85, Loss: 0.7995, Acc: 0.6990, AUC: 0.8642, F1: 0.5473
epoch: 86, total loss: 5.817805826663971
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 1.3537, Acc: 0.5922, AUC: 0.8144, F1: 0.4282
[VIT] val - Epoch: 86, Loss: 0.8518, Acc: 0.6796, AUC: 0.8536, F1: 0.4975
epoch: 87, total loss: 6.18406879901886
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 1.3483, Acc: 0.6311, AUC: 0.7818, F1: 0.4347
[VIT] val - Epoch: 87, Loss: 0.8203, Acc: 0.6699, AUC: 0.8528, F1: 0.5058
epoch: 88, total loss: 5.431342720985413
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 1.3887, Acc: 0.6214, AUC: 0.8027, F1: 0.3862
[VIT] val - Epoch: 88, Loss: 0.8886, Acc: 0.6699, AUC: 0.8504, F1: 0.5113
epoch: 89, total loss: 5.458815574645996
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 1.2017, Acc: 0.6311, AUC: 0.8347, F1: 0.4302
[VIT] val - Epoch: 89, Loss: 0.8470, Acc: 0.6796, AUC: 0.8491, F1: 0.5127
epoch: 90, total loss: 5.464411199092865
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 1.2010, Acc: 0.6117, AUC: 0.8584, F1: 0.5010
[VIT] val - Epoch: 90, Loss: 0.8492, Acc: 0.6505, AUC: 0.8520, F1: 0.4933
epoch: 91, total loss: 5.864332318305969
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 1.3008, Acc: 0.6311, AUC: 0.8312, F1: 0.5024
[VIT] val - Epoch: 91, Loss: 0.8709, Acc: 0.6796, AUC: 0.8570, F1: 0.5351
epoch: 92, total loss: 5.955199062824249
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 1.8331, Acc: 0.5437, AUC: 0.7558, F1: 0.3662
[VIT] val - Epoch: 92, Loss: 0.8270, Acc: 0.6311, AUC: 0.8659, F1: 0.4907
epoch: 93, total loss: 5.874079585075378
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 1.7152, Acc: 0.5631, AUC: 0.8017, F1: 0.3612
[VIT] val - Epoch: 93, Loss: 0.8108, Acc: 0.6699, AUC: 0.8666, F1: 0.5192
epoch: 94, total loss: 6.359449863433838
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 1.8550, Acc: 0.4272, AUC: 0.8104, F1: 0.3558
[VIT] val - Epoch: 94, Loss: 0.8154, Acc: 0.6893, AUC: 0.8607, F1: 0.5271
epoch: 95, total loss: 6.119452595710754
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 1.5697, Acc: 0.5340, AUC: 0.8048, F1: 0.4231
[VIT] val - Epoch: 95, Loss: 0.8741, Acc: 0.6796, AUC: 0.8530, F1: 0.5344
epoch: 96, total loss: 6.439235210418701
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 1.5760, Acc: 0.5922, AUC: 0.8076, F1: 0.3943
[VIT] val - Epoch: 96, Loss: 0.8902, Acc: 0.6699, AUC: 0.8501, F1: 0.5012
epoch: 97, total loss: 6.328787684440613
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 1.6075, Acc: 0.6019, AUC: 0.7943, F1: 0.4040
[VIT] val - Epoch: 97, Loss: 0.8508, Acc: 0.6893, AUC: 0.8550, F1: 0.5340
epoch: 98, total loss: 6.213625073432922
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 1.4826, Acc: 0.6019, AUC: 0.7752, F1: 0.4227
[VIT] val - Epoch: 98, Loss: 0.8276, Acc: 0.6796, AUC: 0.8571, F1: 0.5303
epoch: 99, total loss: 6.12371289730072
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 1.6920, Acc: 0.6311, AUC: 0.7628, F1: 0.4810
[VIT] val - Epoch: 99, Loss: 0.8877, Acc: 0.6505, AUC: 0.8480, F1: 0.4933
epoch: 100, total loss: 5.555725812911987
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 2.0977, Acc: 0.5534, AUC: 0.7709, F1: 0.4022
[VIT] val - Epoch: 100, Loss: 0.8460, Acc: 0.6893, AUC: 0.8506, F1: 0.5221
[16:57:42][Rank 3] Training Finished. Starting Final Testing...[16:57:42][Rank 1] Training Finished. Starting Final Testing...[16:57:42][Rank 2] Training Finished. Starting Final Testing...


[16:57:42][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test_cnn - Epoch: 100, Loss: 2.0273, Acc: 0.5814, AUC: 0.6807, F1: 0.3631
[VIT] test_cnn - Epoch: 100, Loss: 1.0157, Acc: 0.6654, AUC: 0.7783, F1: 0.4301
üöÄ Final Test Results [CNN] - AUC: 0.6807, Acc: 0.5814, F1: 0.3631
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
‚ùå [ÈîôËØØ] Ê∫êÂüü IDRID ËÆ≠ÁªÉÂ§±Ë¥•ÔºÅ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: MESSIDOR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='MESSIDOR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 32
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['MESSIDOR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_MESSIDOR
OUT_DIR: ./output_esdg_h100/MESSIDOR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA/output_esdg_h100/MESSIDOR/CASS_GDRNet_ESDG_MESSIDOR
[17:13:46][Rank 0] Loading datasets...
[17:13:46][Rank 3] Loading datasets...
[17:13:46][Rank 2] Loading datasets...
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
[17:13:46][Rank 1] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []========================================================

========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_projInjecting LoRA into: layer.0.attention.q_proj

Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.q_projInjecting LoRA into: layer.3.mlp.up_proj

Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_projInjecting LoRA into: layer.3.attention.q_proj

Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_projInjecting LoRA into: layer.4.mlp.down_proj

Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_projInjecting LoRA into: layer.7.attention.o_proj

Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.q_projInjecting LoRA into: layer.9.attention.v_proj

Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_projInjecting LoRA into: layer.9.mlp.down_proj

Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
epoch: 1, total loss: 10.340784506364303
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.4712, Acc: 0.4052, AUC: 0.7332, F1: 0.2846
[VIT] val - Epoch: 1, Loss: 1.2112, Acc: 0.5489, AUC: 0.6688, F1: 0.1420
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7332
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6688
Saving vit model...
epoch: 2, total loss: 9.491267724470658
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.1022, Acc: 0.5977, AUC: 0.7827, F1: 0.2914
[VIT] val - Epoch: 2, Loss: 1.2019, Acc: 0.5546, AUC: 0.7273, F1: 0.1427
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7827
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7273
Saving vit model...
epoch: 3, total loss: 9.243817329406738
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.9304, Acc: 0.6494, AUC: 0.7944, F1: 0.3635
[VIT] val - Epoch: 3, Loss: 1.0794, Acc: 0.5632, AUC: 0.7790, F1: 0.1680
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7944
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7790
Saving vit model...
epoch: 4, total loss: 8.718841726129705
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.8404, Acc: 0.6695, AUC: 0.8412, F1: 0.4199
[VIT] val - Epoch: 4, Loss: 0.8938, Acc: 0.6782, AUC: 0.8382, F1: 0.4002
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8412
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8382
Saving vit model...
epoch: 5, total loss: 8.531230059537021
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.8553, Acc: 0.6580, AUC: 0.8226, F1: 0.3943
[VIT] val - Epoch: 5, Loss: 0.8770, Acc: 0.6897, AUC: 0.8631, F1: 0.4822
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8631
Saving vit model...
epoch: 6, total loss: 8.473287278955633
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.0147, Acc: 0.6322, AUC: 0.8019, F1: 0.3938
[VIT] val - Epoch: 6, Loss: 0.8378, Acc: 0.6954, AUC: 0.8777, F1: 0.5082
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8777
Saving vit model...
epoch: 7, total loss: 8.287063988772305
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.9705, Acc: 0.6552, AUC: 0.8585, F1: 0.4251
[VIT] val - Epoch: 7, Loss: 0.8407, Acc: 0.7040, AUC: 0.8835, F1: 0.5606
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8585
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8835
Saving vit model...
epoch: 8, total loss: 8.28426252711903
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.8322, Acc: 0.6724, AUC: 0.8528, F1: 0.4279
[VIT] val - Epoch: 8, Loss: 0.8170, Acc: 0.7069, AUC: 0.8755, F1: 0.5310
epoch: 9, total loss: 8.144272500818426
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.9157, Acc: 0.6868, AUC: 0.8065, F1: 0.4137
[VIT] val - Epoch: 9, Loss: 0.7991, Acc: 0.6925, AUC: 0.8834, F1: 0.5133
epoch: 10, total loss: 8.341157046231357
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.8169, Acc: 0.7098, AUC: 0.8588, F1: 0.5152
[VIT] val - Epoch: 10, Loss: 0.7762, Acc: 0.6954, AUC: 0.8800, F1: 0.4685
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8588
Saving cnn model...
epoch: 11, total loss: 8.182999740947377
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.9108, Acc: 0.5632, AUC: 0.8641, F1: 0.5495
[VIT] val - Epoch: 11, Loss: 0.8018, Acc: 0.7069, AUC: 0.8629, F1: 0.5404
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8641
Saving cnn model...
epoch: 12, total loss: 8.14020859111439
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.8501, Acc: 0.6925, AUC: 0.8360, F1: 0.4103
[VIT] val - Epoch: 12, Loss: 0.7639, Acc: 0.7213, AUC: 0.8848, F1: 0.5631
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8848
Saving vit model...
epoch: 13, total loss: 8.117589603770863
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.8066, Acc: 0.6954, AUC: 0.8499, F1: 0.4716
[VIT] val - Epoch: 13, Loss: 0.7571, Acc: 0.7184, AUC: 0.8891, F1: 0.5617
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8891
Saving vit model...
epoch: 14, total loss: 8.159623406150125
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.8562, Acc: 0.6839, AUC: 0.8690, F1: 0.5246
[VIT] val - Epoch: 14, Loss: 0.7552, Acc: 0.7040, AUC: 0.8907, F1: 0.5504
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8690
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8907
Saving vit model...
epoch: 15, total loss: 8.086337046189742
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.8233, Acc: 0.6925, AUC: 0.8905, F1: 0.5113
[VIT] val - Epoch: 15, Loss: 0.8163, Acc: 0.6753, AUC: 0.8989, F1: 0.5810
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8905
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8989
Saving vit model...
epoch: 16, total loss: 8.008946765552867
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.7869, Acc: 0.7270, AUC: 0.8490, F1: 0.5529
[VIT] val - Epoch: 16, Loss: 0.7670, Acc: 0.7040, AUC: 0.8927, F1: 0.5790
epoch: 17, total loss: 8.069900252602316
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.9285, Acc: 0.5489, AUC: 0.8466, F1: 0.5351
[VIT] val - Epoch: 17, Loss: 0.7443, Acc: 0.7241, AUC: 0.8869, F1: 0.5682
epoch: 18, total loss: 8.00458643653176
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.8031, Acc: 0.6925, AUC: 0.8518, F1: 0.4961
[VIT] val - Epoch: 18, Loss: 0.7321, Acc: 0.7385, AUC: 0.8913, F1: 0.6092
epoch: 19, total loss: 7.755693262273615
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.8564, Acc: 0.6351, AUC: 0.8684, F1: 0.5208
[VIT] val - Epoch: 19, Loss: 0.7444, Acc: 0.7213, AUC: 0.8836, F1: 0.6138
epoch: 20, total loss: 8.101283723657781
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.8022, Acc: 0.6897, AUC: 0.8776, F1: 0.5597
[VIT] val - Epoch: 20, Loss: 0.7508, Acc: 0.7184, AUC: 0.8982, F1: 0.6070
epoch: 21, total loss: 8.136418169195002
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.8050, Acc: 0.6839, AUC: 0.8792, F1: 0.5662
[VIT] val - Epoch: 21, Loss: 0.7654, Acc: 0.6897, AUC: 0.8992, F1: 0.5778
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8992
Saving vit model...
epoch: 22, total loss: 8.139696381308816
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.7766, Acc: 0.7184, AUC: 0.8632, F1: 0.5199
[VIT] val - Epoch: 22, Loss: 0.7493, Acc: 0.7040, AUC: 0.8977, F1: 0.5923
epoch: 23, total loss: 8.111708814447576
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.7580, Acc: 0.7270, AUC: 0.8981, F1: 0.6403
[VIT] val - Epoch: 23, Loss: 0.7365, Acc: 0.7299, AUC: 0.8964, F1: 0.6187
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8981
Saving cnn model...
epoch: 24, total loss: 7.656172622333873
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.7753, Acc: 0.6925, AUC: 0.8906, F1: 0.5662
[VIT] val - Epoch: 24, Loss: 0.7463, Acc: 0.6897, AUC: 0.8963, F1: 0.5733
epoch: 25, total loss: 7.94114351272583
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.8101, Acc: 0.6121, AUC: 0.8853, F1: 0.5511
[VIT] val - Epoch: 25, Loss: 0.7132, Acc: 0.7270, AUC: 0.9039, F1: 0.6334
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9039
Saving vit model...
epoch: 26, total loss: 7.920597509904341
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.7326, Acc: 0.7184, AUC: 0.8901, F1: 0.5744
[VIT] val - Epoch: 26, Loss: 0.6958, Acc: 0.7443, AUC: 0.8892, F1: 0.6099
epoch: 27, total loss: 7.94242711500688
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.7272, Acc: 0.7299, AUC: 0.8827, F1: 0.5299
[VIT] val - Epoch: 27, Loss: 0.7001, Acc: 0.7270, AUC: 0.9004, F1: 0.6006
epoch: 28, total loss: 7.779737429185347
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.7564, Acc: 0.7098, AUC: 0.8673, F1: 0.4417
[VIT] val - Epoch: 28, Loss: 0.7175, Acc: 0.7184, AUC: 0.8897, F1: 0.6147
epoch: 29, total loss: 7.862717281688344
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.8339, Acc: 0.6063, AUC: 0.8953, F1: 0.5756
[VIT] val - Epoch: 29, Loss: 0.7036, Acc: 0.7414, AUC: 0.8998, F1: 0.6354
epoch: 30, total loss: 7.832851843400435
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.7952, Acc: 0.6868, AUC: 0.8657, F1: 0.5134
[VIT] val - Epoch: 30, Loss: 0.7147, Acc: 0.7184, AUC: 0.8996, F1: 0.5801
epoch: 31, total loss: 7.514256217262962
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.8211, Acc: 0.7241, AUC: 0.8660, F1: 0.5920
[VIT] val - Epoch: 31, Loss: 0.7137, Acc: 0.7011, AUC: 0.9038, F1: 0.6096
epoch: 32, total loss: 7.5641601302407
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.7727, Acc: 0.6782, AUC: 0.8823, F1: 0.5757
[VIT] val - Epoch: 32, Loss: 0.6952, Acc: 0.7270, AUC: 0.8951, F1: 0.6403
epoch: 33, total loss: 7.824442343278364
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.8427, Acc: 0.7241, AUC: 0.8752, F1: 0.5566
[VIT] val - Epoch: 33, Loss: 0.7433, Acc: 0.6925, AUC: 0.9085, F1: 0.6494
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9085
Saving vit model...
epoch: 34, total loss: 7.6533989039334385
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.7702, Acc: 0.7270, AUC: 0.8799, F1: 0.5266
[VIT] val - Epoch: 34, Loss: 0.6895, Acc: 0.7385, AUC: 0.9060, F1: 0.6516
epoch: 35, total loss: 7.732878121462735
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.9994, Acc: 0.6580, AUC: 0.8116, F1: 0.4358
[VIT] val - Epoch: 35, Loss: 0.7005, Acc: 0.7414, AUC: 0.9004, F1: 0.6492
epoch: 36, total loss: 7.749812993136319
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.8131, Acc: 0.7213, AUC: 0.8594, F1: 0.5088
[VIT] val - Epoch: 36, Loss: 0.6992, Acc: 0.7241, AUC: 0.9069, F1: 0.6441
epoch: 37, total loss: 7.482597351074219
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8559, Acc: 0.6868, AUC: 0.8756, F1: 0.5707
[VIT] val - Epoch: 37, Loss: 0.7026, Acc: 0.7213, AUC: 0.9032, F1: 0.6562
epoch: 38, total loss: 7.54055304960771
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.7860, Acc: 0.7069, AUC: 0.9035, F1: 0.6212
[VIT] val - Epoch: 38, Loss: 0.7722, Acc: 0.6724, AUC: 0.9147, F1: 0.6782
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9035
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9147
Saving vit model...
epoch: 39, total loss: 7.654539368369362
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.7939, Acc: 0.6954, AUC: 0.8778, F1: 0.5675
[VIT] val - Epoch: 39, Loss: 0.7380, Acc: 0.7040, AUC: 0.9147, F1: 0.6347
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9147
Saving vit model...
epoch: 40, total loss: 7.604525696147572
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.7242, Acc: 0.7529, AUC: 0.8705, F1: 0.5397
[VIT] val - Epoch: 40, Loss: 0.7141, Acc: 0.7299, AUC: 0.9062, F1: 0.6529
epoch: 41, total loss: 7.5698513117703525
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.8115, Acc: 0.7126, AUC: 0.8721, F1: 0.5438
[VIT] val - Epoch: 41, Loss: 0.7233, Acc: 0.7184, AUC: 0.9022, F1: 0.6526
epoch: 42, total loss: 7.762080626054243
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.8375, Acc: 0.7011, AUC: 0.8454, F1: 0.5176
[VIT] val - Epoch: 42, Loss: 0.7275, Acc: 0.7184, AUC: 0.9081, F1: 0.6406
epoch: 43, total loss: 7.593101154674184
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.7356, Acc: 0.7241, AUC: 0.8868, F1: 0.6202
[VIT] val - Epoch: 43, Loss: 0.7046, Acc: 0.7241, AUC: 0.9140, F1: 0.6596
epoch: 44, total loss: 7.451181281696666
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.9092, Acc: 0.5489, AUC: 0.8757, F1: 0.5478
[VIT] val - Epoch: 44, Loss: 0.6952, Acc: 0.7241, AUC: 0.9100, F1: 0.6510
epoch: 45, total loss: 7.469810442491011
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.7520, Acc: 0.7184, AUC: 0.8987, F1: 0.5746
[VIT] val - Epoch: 45, Loss: 0.7071, Acc: 0.7213, AUC: 0.9073, F1: 0.6495
epoch: 46, total loss: 7.392760320143267
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.8529, Acc: 0.6207, AUC: 0.8852, F1: 0.6298
[VIT] val - Epoch: 46, Loss: 0.7212, Acc: 0.7098, AUC: 0.9141, F1: 0.6400
epoch: 47, total loss: 7.5432305769486865
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.6934, Acc: 0.7299, AUC: 0.9010, F1: 0.6448
[VIT] val - Epoch: 47, Loss: 0.6670, Acc: 0.7443, AUC: 0.9037, F1: 0.6348
epoch: 48, total loss: 7.51216645674272
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.9293, Acc: 0.7069, AUC: 0.8534, F1: 0.4975
[VIT] val - Epoch: 48, Loss: 0.7056, Acc: 0.7069, AUC: 0.9080, F1: 0.6670
epoch: 49, total loss: 7.475947293368253
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.6956, Acc: 0.7557, AUC: 0.9027, F1: 0.6365
[VIT] val - Epoch: 49, Loss: 0.6840, Acc: 0.7184, AUC: 0.9050, F1: 0.6226
epoch: 50, total loss: 7.755559357729825
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.8742, Acc: 0.6178, AUC: 0.8731, F1: 0.5565
[VIT] val - Epoch: 50, Loss: 0.7409, Acc: 0.7011, AUC: 0.9102, F1: 0.6607
epoch: 51, total loss: 7.549690593372691
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.8342, Acc: 0.6379, AUC: 0.8653, F1: 0.5565
[VIT] val - Epoch: 51, Loss: 0.6884, Acc: 0.7155, AUC: 0.9011, F1: 0.5897
epoch: 52, total loss: 7.435500925237482
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.7885, Acc: 0.6954, AUC: 0.8935, F1: 0.5917
[VIT] val - Epoch: 52, Loss: 0.7255, Acc: 0.7213, AUC: 0.9054, F1: 0.6660
epoch: 53, total loss: 7.402324373071844
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.8366, Acc: 0.6753, AUC: 0.8988, F1: 0.5317
[VIT] val - Epoch: 53, Loss: 0.7415, Acc: 0.7126, AUC: 0.9107, F1: 0.6138
epoch: 54, total loss: 7.594460747458718
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.9412, Acc: 0.6351, AUC: 0.8593, F1: 0.5153
[VIT] val - Epoch: 54, Loss: 0.7386, Acc: 0.7040, AUC: 0.9022, F1: 0.6336
epoch: 55, total loss: 7.328854994340376
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.9836, Acc: 0.6897, AUC: 0.8694, F1: 0.5184
[VIT] val - Epoch: 55, Loss: 0.7441, Acc: 0.6868, AUC: 0.9138, F1: 0.6619
epoch: 56, total loss: 7.446005474437367
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.9509, Acc: 0.6667, AUC: 0.8564, F1: 0.4510
[VIT] val - Epoch: 56, Loss: 0.7326, Acc: 0.6897, AUC: 0.9003, F1: 0.6099
epoch: 57, total loss: 7.5405261733315205
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.8973, Acc: 0.6437, AUC: 0.8728, F1: 0.5052
[VIT] val - Epoch: 57, Loss: 0.7357, Acc: 0.7155, AUC: 0.9025, F1: 0.6481
epoch: 58, total loss: 7.209604089910334
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.8187, Acc: 0.6466, AUC: 0.8847, F1: 0.5658
[VIT] val - Epoch: 58, Loss: 0.7320, Acc: 0.7213, AUC: 0.8985, F1: 0.6096
epoch: 59, total loss: 7.091343446211382
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.8330, Acc: 0.6466, AUC: 0.8785, F1: 0.5831
[VIT] val - Epoch: 59, Loss: 0.7117, Acc: 0.7184, AUC: 0.9073, F1: 0.6415
epoch: 60, total loss: 7.452852032401345
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.8911, Acc: 0.7299, AUC: 0.8535, F1: 0.5117
[VIT] val - Epoch: 60, Loss: 0.7103, Acc: 0.7213, AUC: 0.9044, F1: 0.6574
epoch: 61, total loss: 7.5043840841813525
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.8062, Acc: 0.6638, AUC: 0.8862, F1: 0.5591
[VIT] val - Epoch: 61, Loss: 0.7408, Acc: 0.7098, AUC: 0.9077, F1: 0.6589
epoch: 62, total loss: 7.233673312447288
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 1.0487, Acc: 0.5862, AUC: 0.8620, F1: 0.5813
[VIT] val - Epoch: 62, Loss: 0.7854, Acc: 0.6753, AUC: 0.9022, F1: 0.6167
epoch: 63, total loss: 7.327834216031161
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.9125, Acc: 0.7356, AUC: 0.8777, F1: 0.5744
[VIT] val - Epoch: 63, Loss: 0.7432, Acc: 0.7126, AUC: 0.9104, F1: 0.6683
epoch: 64, total loss: 7.293929143385454
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.8441, Acc: 0.6839, AUC: 0.8741, F1: 0.5582
[VIT] val - Epoch: 64, Loss: 0.7364, Acc: 0.7098, AUC: 0.9184, F1: 0.6667
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.9184
Saving vit model...
epoch: 65, total loss: 7.5608321536671035
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.8487, Acc: 0.6782, AUC: 0.8750, F1: 0.5528
[VIT] val - Epoch: 65, Loss: 0.7308, Acc: 0.6839, AUC: 0.9161, F1: 0.6671
epoch: 66, total loss: 7.153147263960405
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.9537, Acc: 0.6063, AUC: 0.8712, F1: 0.5621
[VIT] val - Epoch: 66, Loss: 0.7553, Acc: 0.6724, AUC: 0.9161, F1: 0.6737
epoch: 67, total loss: 7.21799915487116
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.9508, Acc: 0.6782, AUC: 0.8651, F1: 0.5636
[VIT] val - Epoch: 67, Loss: 0.7350, Acc: 0.7184, AUC: 0.9012, F1: 0.6474
epoch: 68, total loss: 7.500173438679088
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.8366, Acc: 0.7328, AUC: 0.8705, F1: 0.6113
[VIT] val - Epoch: 68, Loss: 0.7429, Acc: 0.7213, AUC: 0.9091, F1: 0.6897
epoch: 69, total loss: 7.029743411324241
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.8603, Acc: 0.7241, AUC: 0.8783, F1: 0.5646
[VIT] val - Epoch: 69, Loss: 0.7448, Acc: 0.7385, AUC: 0.9096, F1: 0.6522
epoch: 70, total loss: 6.906760302456942
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.9198, Acc: 0.6667, AUC: 0.8708, F1: 0.6222
[VIT] val - Epoch: 70, Loss: 0.7270, Acc: 0.7155, AUC: 0.9071, F1: 0.6660
epoch: 71, total loss: 6.7414305860346015
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.9096, Acc: 0.6724, AUC: 0.9001, F1: 0.5695
[VIT] val - Epoch: 71, Loss: 0.7280, Acc: 0.7126, AUC: 0.9021, F1: 0.5955
epoch: 72, total loss: 7.408188386396929
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.9912, Acc: 0.6580, AUC: 0.8520, F1: 0.5998
[VIT] val - Epoch: 72, Loss: 0.7580, Acc: 0.7040, AUC: 0.8918, F1: 0.6339
epoch: 73, total loss: 7.166561343453147
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.9534, Acc: 0.7414, AUC: 0.8737, F1: 0.5673
[VIT] val - Epoch: 73, Loss: 0.7452, Acc: 0.7040, AUC: 0.9009, F1: 0.6416
epoch: 74, total loss: 7.076522567055442
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 1.1053, Acc: 0.5661, AUC: 0.8670, F1: 0.5528
[VIT] val - Epoch: 74, Loss: 0.7144, Acc: 0.7184, AUC: 0.9070, F1: 0.6373
epoch: 75, total loss: 7.0727337490428575
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.9693, Acc: 0.7299, AUC: 0.8713, F1: 0.5801
[VIT] val - Epoch: 75, Loss: 0.7451, Acc: 0.7184, AUC: 0.9087, F1: 0.6432
epoch: 76, total loss: 6.948321559212425
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.8903, Acc: 0.6897, AUC: 0.8850, F1: 0.5848
[VIT] val - Epoch: 76, Loss: 0.7753, Acc: 0.6667, AUC: 0.9100, F1: 0.6615
epoch: 77, total loss: 6.833142670718106
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.7818, Acc: 0.7155, AUC: 0.9051, F1: 0.6155
[VIT] val - Epoch: 77, Loss: 0.7473, Acc: 0.7098, AUC: 0.9108, F1: 0.6496
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9051
Saving cnn model...
epoch: 78, total loss: 6.939933516762474
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.9191, Acc: 0.6724, AUC: 0.8891, F1: 0.5713
[VIT] val - Epoch: 78, Loss: 0.7337, Acc: 0.7126, AUC: 0.9112, F1: 0.5962
epoch: 79, total loss: 7.029477682980624
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.9107, Acc: 0.7299, AUC: 0.8688, F1: 0.5823
[VIT] val - Epoch: 79, Loss: 0.7427, Acc: 0.7356, AUC: 0.9080, F1: 0.6241
epoch: 80, total loss: 7.0474255735223945
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.9125, Acc: 0.6925, AUC: 0.8827, F1: 0.5927
[VIT] val - Epoch: 80, Loss: 0.7641, Acc: 0.7126, AUC: 0.8990, F1: 0.6393
epoch: 81, total loss: 6.980123953385786
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.9779, Acc: 0.6925, AUC: 0.8622, F1: 0.6152
[VIT] val - Epoch: 81, Loss: 0.7622, Acc: 0.7069, AUC: 0.8963, F1: 0.5742
epoch: 82, total loss: 6.74631742997603
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 1.0300, Acc: 0.6437, AUC: 0.8849, F1: 0.5935
[VIT] val - Epoch: 82, Loss: 0.7704, Acc: 0.7069, AUC: 0.9084, F1: 0.6360
epoch: 83, total loss: 6.873211080377752
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.9885, Acc: 0.7184, AUC: 0.8835, F1: 0.5789
[VIT] val - Epoch: 83, Loss: 0.7574, Acc: 0.6897, AUC: 0.8986, F1: 0.6029
epoch: 84, total loss: 6.861864003268155
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 1.0433, Acc: 0.7126, AUC: 0.8613, F1: 0.5969
[VIT] val - Epoch: 84, Loss: 0.8090, Acc: 0.6695, AUC: 0.9029, F1: 0.6063
epoch: 85, total loss: 6.753571293570778
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.9692, Acc: 0.6810, AUC: 0.8798, F1: 0.6103
[VIT] val - Epoch: 85, Loss: 0.7427, Acc: 0.7098, AUC: 0.9037, F1: 0.6507
epoch: 86, total loss: 6.708073312586004
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.9747, Acc: 0.7356, AUC: 0.8679, F1: 0.5726
[VIT] val - Epoch: 86, Loss: 0.7578, Acc: 0.6925, AUC: 0.9054, F1: 0.6398
epoch: 87, total loss: 6.884561625393954
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.9452, Acc: 0.7356, AUC: 0.8856, F1: 0.5895
[VIT] val - Epoch: 87, Loss: 0.7641, Acc: 0.7126, AUC: 0.9082, F1: 0.6473
epoch: 88, total loss: 6.694567853754217
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 1.0292, Acc: 0.6954, AUC: 0.8651, F1: 0.5435
[VIT] val - Epoch: 88, Loss: 0.7556, Acc: 0.7299, AUC: 0.9061, F1: 0.6114
epoch: 89, total loss: 6.872665015133944
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 1.0349, Acc: 0.6695, AUC: 0.8779, F1: 0.5974
[VIT] val - Epoch: 89, Loss: 0.7681, Acc: 0.7098, AUC: 0.9028, F1: 0.6163
epoch: 90, total loss: 6.848431803963401
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 1.2025, Acc: 0.6437, AUC: 0.8454, F1: 0.4916
[VIT] val - Epoch: 90, Loss: 0.8003, Acc: 0.7011, AUC: 0.9075, F1: 0.6270
epoch: 91, total loss: 6.62575067173351
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 1.1511, Acc: 0.6466, AUC: 0.8659, F1: 0.5905
[VIT] val - Epoch: 91, Loss: 0.7927, Acc: 0.6839, AUC: 0.9079, F1: 0.6464
epoch: 92, total loss: 6.78793308951638
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 1.1096, Acc: 0.7184, AUC: 0.8611, F1: 0.5316
[VIT] val - Epoch: 92, Loss: 0.7610, Acc: 0.7040, AUC: 0.9073, F1: 0.6171
epoch: 93, total loss: 6.831577170978893
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.9449, Acc: 0.6839, AUC: 0.8886, F1: 0.5915
[VIT] val - Epoch: 93, Loss: 0.7734, Acc: 0.6925, AUC: 0.9036, F1: 0.6179
epoch: 94, total loss: 6.909786571155895
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 1.1234, Acc: 0.6782, AUC: 0.8410, F1: 0.5226
[VIT] val - Epoch: 94, Loss: 0.7676, Acc: 0.7270, AUC: 0.9044, F1: 0.6019
epoch: 95, total loss: 7.0629495707425205
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 1.1728, Acc: 0.7356, AUC: 0.8796, F1: 0.5892
[VIT] val - Epoch: 95, Loss: 0.7613, Acc: 0.6954, AUC: 0.9008, F1: 0.6283
epoch: 96, total loss: 6.720838633450595
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 1.1171, Acc: 0.7040, AUC: 0.8474, F1: 0.4873
[VIT] val - Epoch: 96, Loss: 0.7794, Acc: 0.7040, AUC: 0.9097, F1: 0.6092
epoch: 97, total loss: 6.322420640425249
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 1.0496, Acc: 0.6753, AUC: 0.8767, F1: 0.5708
[VIT] val - Epoch: 97, Loss: 0.7488, Acc: 0.7241, AUC: 0.9083, F1: 0.6136
epoch: 98, total loss: 6.4212234236977315
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 1.0179, Acc: 0.7098, AUC: 0.8739, F1: 0.6186
[VIT] val - Epoch: 98, Loss: 0.7402, Acc: 0.7299, AUC: 0.9046, F1: 0.6245
epoch: 99, total loss: 6.816832585768267
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 1.1548, Acc: 0.6925, AUC: 0.8655, F1: 0.5903
[VIT] val - Epoch: 99, Loss: 0.7715, Acc: 0.7069, AUC: 0.9094, F1: 0.6213
epoch: 100, total loss: 6.702376972545277
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 1.0666, Acc: 0.7241, AUC: 0.8715, F1: 0.6304
[VIT] val - Epoch: 100, Loss: 0.7552, Acc: 0.7011, AUC: 0.9052, F1: 0.5919
[17:22:41][Rank 2] Training Finished. Starting Final Testing...
[17:22:41][Rank 1] Training Finished. Starting Final Testing...
[17:22:41][Rank 3] Training Finished. Starting Final Testing...
[17:22:41][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test_cnn - Epoch: 100, Loss: 1.4988, Acc: 0.5823, AUC: 0.7647, F1: 0.3984
[VIT] test_cnn - Epoch: 100, Loss: 1.1596, Acc: 0.6536, AUC: 0.7954, F1: 0.4498
üöÄ Final Test Results [CNN] - AUC: 0.7647, Acc: 0.5823, F1: 0.3984
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
‚ùå [ÈîôËØØ] Ê∫êÂüü MESSIDOR ËÆ≠ÁªÉÂ§±Ë¥•ÔºÅ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: RLDR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='RLDR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 32
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['RLDR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_RLDR
OUT_DIR: ./output_esdg_h100/RLDR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA/output_esdg_h100/RLDR/CASS_GDRNet_ESDG_RLDR
[17:38:40][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
[17:38:40][Rank 3] Loading datasets...
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
[17:38:40][Rank 2] Loading datasets...
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
[17:38:40][Rank 1] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Injecting LoRA into: layer.2.mlp.down_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.11.attention.k_projInjecting LoRA into: layer.8.attention.q_proj

Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_projInjecting LoRA into: layer.9.attention.v_proj

Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
Total Params: 118.19M, Trainable (LoRA+CNN): 32.53M
epoch: 1, total loss: 10.233146667480469
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 1.4291, Acc: 0.3805, AUC: 0.6772, F1: 0.2567
[VIT] val - Epoch: 1, Loss: 1.2103, Acc: 0.5629, AUC: 0.6847, F1: 0.1444
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6772
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6847
Saving vit model...
epoch: 2, total loss: 9.177748584747315
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.1548, Acc: 0.5535, AUC: 0.7676, F1: 0.3410
[VIT] val - Epoch: 2, Loss: 1.1737, Acc: 0.6038, AUC: 0.7624, F1: 0.2224
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7676
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7624
Saving vit model...
epoch: 3, total loss: 9.038383388519287
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 1.2456, Acc: 0.4654, AUC: 0.8101, F1: 0.3321
[VIT] val - Epoch: 3, Loss: 0.9999, Acc: 0.6164, AUC: 0.8149, F1: 0.3775
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8101
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8149
Saving vit model...
epoch: 4, total loss: 8.778340339660645
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.9385, Acc: 0.6258, AUC: 0.8264, F1: 0.4403
[VIT] val - Epoch: 4, Loss: 1.0099, Acc: 0.5786, AUC: 0.8246, F1: 0.4019
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8264
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8246
Saving vit model...
epoch: 5, total loss: 8.564537715911865
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.0890, Acc: 0.5157, AUC: 0.8069, F1: 0.3518
[VIT] val - Epoch: 5, Loss: 1.0110, Acc: 0.5660, AUC: 0.8297, F1: 0.4168
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8297
Saving vit model...
epoch: 6, total loss: 8.686477375030517
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.0321, Acc: 0.5566, AUC: 0.8218, F1: 0.3273
[VIT] val - Epoch: 6, Loss: 0.9373, Acc: 0.6258, AUC: 0.8400, F1: 0.4474
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8400
Saving vit model...
epoch: 7, total loss: 8.62394027709961
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.2197, Acc: 0.5000, AUC: 0.7951, F1: 0.3433
[VIT] val - Epoch: 7, Loss: 0.9634, Acc: 0.5786, AUC: 0.8394, F1: 0.4299
epoch: 8, total loss: 8.500018167495728
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.9946, Acc: 0.5786, AUC: 0.8150, F1: 0.3907
[VIT] val - Epoch: 8, Loss: 0.9103, Acc: 0.6321, AUC: 0.8399, F1: 0.4686
epoch: 9, total loss: 8.478361082077026
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.9842, Acc: 0.5692, AUC: 0.8310, F1: 0.3957
[VIT] val - Epoch: 9, Loss: 1.0007, Acc: 0.5314, AUC: 0.8449, F1: 0.3607
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8310
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8449
Saving vit model...
epoch: 10, total loss: 8.325394010543823
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.9498, Acc: 0.6069, AUC: 0.8112, F1: 0.4218
[VIT] val - Epoch: 10, Loss: 0.9175, Acc: 0.6069, AUC: 0.8490, F1: 0.4584
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8490
Saving vit model...
epoch: 11, total loss: 8.333037567138671
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.9366, Acc: 0.6258, AUC: 0.8420, F1: 0.5374
[VIT] val - Epoch: 11, Loss: 0.8843, Acc: 0.6478, AUC: 0.8515, F1: 0.4727
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8420
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8515
Saving vit model...
epoch: 12, total loss: 8.268190479278564
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.0285, Acc: 0.5440, AUC: 0.8174, F1: 0.3927
[VIT] val - Epoch: 12, Loss: 0.9372, Acc: 0.5818, AUC: 0.8492, F1: 0.4451
epoch: 13, total loss: 8.281063985824584
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.0359, Acc: 0.5409, AUC: 0.8357, F1: 0.4139
[VIT] val - Epoch: 13, Loss: 0.9176, Acc: 0.6289, AUC: 0.8526, F1: 0.4908
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8526
Saving vit model...
epoch: 14, total loss: 8.207495546340942
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.9598, Acc: 0.6006, AUC: 0.8125, F1: 0.3837
[VIT] val - Epoch: 14, Loss: 0.9070, Acc: 0.6164, AUC: 0.8567, F1: 0.4414
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8567
Saving vit model...
epoch: 15, total loss: 8.247904109954835
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 1.0753, Acc: 0.5189, AUC: 0.8284, F1: 0.4234
[VIT] val - Epoch: 15, Loss: 0.9896, Acc: 0.5786, AUC: 0.8594, F1: 0.4428
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8594
Saving vit model...
epoch: 16, total loss: 8.422570705413818
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.9363, Acc: 0.6101, AUC: 0.8293, F1: 0.4628
[VIT] val - Epoch: 16, Loss: 0.9075, Acc: 0.6164, AUC: 0.8615, F1: 0.4830
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8615
Saving vit model...
epoch: 17, total loss: 8.324592018127442
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 1.0757, Acc: 0.5409, AUC: 0.8284, F1: 0.3488
[VIT] val - Epoch: 17, Loss: 0.9279, Acc: 0.5881, AUC: 0.8620, F1: 0.4478
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8620
Saving vit model...
epoch: 18, total loss: 8.357452964782714
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.9543, Acc: 0.5755, AUC: 0.8424, F1: 0.4814
[VIT] val - Epoch: 18, Loss: 0.9451, Acc: 0.5912, AUC: 0.8559, F1: 0.4416
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8424
Saving cnn model...
epoch: 19, total loss: 8.143359041213989
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.0104, Acc: 0.5849, AUC: 0.8343, F1: 0.4265
[VIT] val - Epoch: 19, Loss: 0.9518, Acc: 0.5849, AUC: 0.8614, F1: 0.4700
epoch: 20, total loss: 7.967652082443237
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 1.0437, Acc: 0.5723, AUC: 0.8374, F1: 0.4265
[VIT] val - Epoch: 20, Loss: 0.8834, Acc: 0.6164, AUC: 0.8592, F1: 0.4584
epoch: 21, total loss: 8.196440839767456
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 1.0102, Acc: 0.5755, AUC: 0.8193, F1: 0.4163
[VIT] val - Epoch: 21, Loss: 0.9464, Acc: 0.5692, AUC: 0.8545, F1: 0.4111
epoch: 22, total loss: 8.285029458999634
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 1.1101, Acc: 0.5409, AUC: 0.8270, F1: 0.4665
[VIT] val - Epoch: 22, Loss: 0.8618, Acc: 0.6384, AUC: 0.8595, F1: 0.4940
epoch: 23, total loss: 8.019159269332885
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.9745, Acc: 0.6101, AUC: 0.8407, F1: 0.4533
[VIT] val - Epoch: 23, Loss: 0.8898, Acc: 0.6289, AUC: 0.8660, F1: 0.4818
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8660
Saving vit model...
epoch: 24, total loss: 8.029557514190675
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 1.0020, Acc: 0.5818, AUC: 0.8363, F1: 0.4290
[VIT] val - Epoch: 24, Loss: 1.0203, Acc: 0.5535, AUC: 0.8592, F1: 0.4488
epoch: 25, total loss: 8.049023103713989
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.9686, Acc: 0.5975, AUC: 0.8461, F1: 0.5205
[VIT] val - Epoch: 25, Loss: 0.8741, Acc: 0.6195, AUC: 0.8599, F1: 0.4964
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8461
Saving cnn model...
epoch: 26, total loss: 8.168167400360108
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 1.0040, Acc: 0.6132, AUC: 0.8308, F1: 0.3653
[VIT] val - Epoch: 26, Loss: 0.8746, Acc: 0.6258, AUC: 0.8574, F1: 0.4506
epoch: 27, total loss: 8.07285771369934
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 1.0245, Acc: 0.5975, AUC: 0.8271, F1: 0.4435
[VIT] val - Epoch: 27, Loss: 0.8751, Acc: 0.6226, AUC: 0.8594, F1: 0.5173
epoch: 28, total loss: 7.799258470535278
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 1.1779, Acc: 0.5755, AUC: 0.8225, F1: 0.3897
[VIT] val - Epoch: 28, Loss: 0.9369, Acc: 0.5755, AUC: 0.8543, F1: 0.4521
epoch: 29, total loss: 8.055566501617431
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.9471, Acc: 0.6101, AUC: 0.8411, F1: 0.4601
[VIT] val - Epoch: 29, Loss: 0.8523, Acc: 0.6478, AUC: 0.8626, F1: 0.5168
epoch: 30, total loss: 8.073983860015868
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 1.1173, Acc: 0.5409, AUC: 0.8296, F1: 0.4485
[VIT] val - Epoch: 30, Loss: 0.8733, Acc: 0.6226, AUC: 0.8606, F1: 0.5062
epoch: 31, total loss: 7.790386438369751
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 1.0522, Acc: 0.5818, AUC: 0.8321, F1: 0.4510
[VIT] val - Epoch: 31, Loss: 0.8791, Acc: 0.6321, AUC: 0.8653, F1: 0.5044
epoch: 32, total loss: 7.888907146453858
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 1.1311, Acc: 0.5220, AUC: 0.8449, F1: 0.4592
[VIT] val - Epoch: 32, Loss: 0.8958, Acc: 0.6258, AUC: 0.8466, F1: 0.5155
epoch: 33, total loss: 7.843321323394775
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 1.0224, Acc: 0.6226, AUC: 0.8364, F1: 0.4328
[VIT] val - Epoch: 33, Loss: 0.8776, Acc: 0.6447, AUC: 0.8590, F1: 0.5200
epoch: 34, total loss: 7.954792451858521
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.9641, Acc: 0.5786, AUC: 0.8376, F1: 0.4808
[VIT] val - Epoch: 34, Loss: 0.8602, Acc: 0.6289, AUC: 0.8641, F1: 0.5107
epoch: 35, total loss: 7.98099308013916
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 1.0218, Acc: 0.5786, AUC: 0.8394, F1: 0.4739
[VIT] val - Epoch: 35, Loss: 1.0234, Acc: 0.5818, AUC: 0.8665, F1: 0.4974
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8665
Saving vit model...
epoch: 36, total loss: 7.914695930480957
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 1.0942, Acc: 0.5912, AUC: 0.8428, F1: 0.4280
[VIT] val - Epoch: 36, Loss: 0.9451, Acc: 0.6164, AUC: 0.8612, F1: 0.5056
epoch: 37, total loss: 7.795072793960571
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.9790, Acc: 0.5943, AUC: 0.8478, F1: 0.4848
[VIT] val - Epoch: 37, Loss: 0.9533, Acc: 0.6101, AUC: 0.8614, F1: 0.4903
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8478
Saving cnn model...
epoch: 38, total loss: 7.760318660736084
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.9966, Acc: 0.6384, AUC: 0.8425, F1: 0.4735
[VIT] val - Epoch: 38, Loss: 0.9392, Acc: 0.6226, AUC: 0.8653, F1: 0.5009
epoch: 39, total loss: 7.840068054199219
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 1.3142, Acc: 0.4906, AUC: 0.7857, F1: 0.3599
[VIT] val - Epoch: 39, Loss: 0.8600, Acc: 0.6572, AUC: 0.8465, F1: 0.5416
epoch: 40, total loss: 7.5092192649841305
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 1.0579, Acc: 0.5692, AUC: 0.8339, F1: 0.4541
[VIT] val - Epoch: 40, Loss: 1.0240, Acc: 0.5535, AUC: 0.8583, F1: 0.4430
epoch: 41, total loss: 7.647156810760498
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 1.0181, Acc: 0.5755, AUC: 0.8338, F1: 0.4956
[VIT] val - Epoch: 41, Loss: 0.9107, Acc: 0.6101, AUC: 0.8573, F1: 0.5145
epoch: 42, total loss: 7.475885057449341
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 1.0760, Acc: 0.5755, AUC: 0.8417, F1: 0.5098
[VIT] val - Epoch: 42, Loss: 0.8712, Acc: 0.6415, AUC: 0.8501, F1: 0.4918
epoch: 43, total loss: 7.519534635543823
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 1.1523, Acc: 0.5755, AUC: 0.8253, F1: 0.4177
[VIT] val - Epoch: 43, Loss: 0.8829, Acc: 0.6226, AUC: 0.8522, F1: 0.4990
epoch: 44, total loss: 7.592973899841309
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 1.3665, Acc: 0.5409, AUC: 0.8181, F1: 0.4371
[VIT] val - Epoch: 44, Loss: 0.9365, Acc: 0.6038, AUC: 0.8381, F1: 0.4605
epoch: 45, total loss: 7.573716497421264
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 1.3244, Acc: 0.5094, AUC: 0.8074, F1: 0.3974
[VIT] val - Epoch: 45, Loss: 0.9138, Acc: 0.6384, AUC: 0.8265, F1: 0.4710
epoch: 46, total loss: 7.582694053649902
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 1.0990, Acc: 0.5755, AUC: 0.8405, F1: 0.4153
[VIT] val - Epoch: 46, Loss: 0.9134, Acc: 0.6415, AUC: 0.8434, F1: 0.4912
epoch: 47, total loss: 7.71769380569458
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 1.1443, Acc: 0.5849, AUC: 0.8388, F1: 0.4512
[VIT] val - Epoch: 47, Loss: 1.0808, Acc: 0.5566, AUC: 0.8534, F1: 0.4573
epoch: 48, total loss: 7.802124738693237
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 1.2725, Acc: 0.5629, AUC: 0.8164, F1: 0.4127
[VIT] val - Epoch: 48, Loss: 0.9447, Acc: 0.6164, AUC: 0.8469, F1: 0.4795
epoch: 49, total loss: 7.316601133346557
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 1.1621, Acc: 0.5818, AUC: 0.8234, F1: 0.4614
[VIT] val - Epoch: 49, Loss: 0.9735, Acc: 0.6321, AUC: 0.8431, F1: 0.4504
epoch: 50, total loss: 7.299437618255615
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 1.3317, Acc: 0.5409, AUC: 0.8340, F1: 0.4664
[VIT] val - Epoch: 50, Loss: 0.9630, Acc: 0.6226, AUC: 0.8440, F1: 0.4999
epoch: 51, total loss: 7.436932134628296
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 1.2249, Acc: 0.5377, AUC: 0.8044, F1: 0.4339
[VIT] val - Epoch: 51, Loss: 1.0601, Acc: 0.5912, AUC: 0.8310, F1: 0.4808
epoch: 52, total loss: 7.404416465759278
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 1.0526, Acc: 0.6006, AUC: 0.8335, F1: 0.4385
[VIT] val - Epoch: 52, Loss: 0.9483, Acc: 0.6352, AUC: 0.8444, F1: 0.4992
epoch: 53, total loss: 7.4602128028869625
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 1.1567, Acc: 0.5849, AUC: 0.8132, F1: 0.4574
[VIT] val - Epoch: 53, Loss: 0.9450, Acc: 0.6226, AUC: 0.8453, F1: 0.5292
epoch: 54, total loss: 7.36347074508667
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 1.1753, Acc: 0.6038, AUC: 0.8245, F1: 0.4867
[VIT] val - Epoch: 54, Loss: 0.9685, Acc: 0.6226, AUC: 0.8503, F1: 0.5162
epoch: 55, total loss: 7.354878711700439
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 1.4672, Acc: 0.5535, AUC: 0.8039, F1: 0.4061
[VIT] val - Epoch: 55, Loss: 1.0457, Acc: 0.5943, AUC: 0.8496, F1: 0.4870
epoch: 56, total loss: 7.491848182678223
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 1.0934, Acc: 0.6226, AUC: 0.8324, F1: 0.4839
[VIT] val - Epoch: 56, Loss: 0.9416, Acc: 0.6447, AUC: 0.8457, F1: 0.4916
epoch: 57, total loss: 7.124345779418945
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 1.4263, Acc: 0.4843, AUC: 0.8268, F1: 0.3720
[VIT] val - Epoch: 57, Loss: 1.0048, Acc: 0.6447, AUC: 0.8503, F1: 0.4974
epoch: 58, total loss: 7.050117444992066
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 1.1429, Acc: 0.6415, AUC: 0.8197, F1: 0.4924
[VIT] val - Epoch: 58, Loss: 0.9833, Acc: 0.6321, AUC: 0.8460, F1: 0.5015
epoch: 59, total loss: 7.04598445892334
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 1.5553, Acc: 0.5283, AUC: 0.8354, F1: 0.4719
[VIT] val - Epoch: 59, Loss: 0.9888, Acc: 0.6415, AUC: 0.8459, F1: 0.5181
epoch: 60, total loss: 7.090964365005493
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 1.1576, Acc: 0.6069, AUC: 0.8372, F1: 0.4962
[VIT] val - Epoch: 60, Loss: 0.9953, Acc: 0.6195, AUC: 0.8468, F1: 0.4997
epoch: 61, total loss: 7.1076561450958256
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 1.6125, Acc: 0.6069, AUC: 0.8201, F1: 0.4727
[VIT] val - Epoch: 61, Loss: 0.9851, Acc: 0.6321, AUC: 0.8486, F1: 0.4917
epoch: 62, total loss: 7.073157501220703
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 1.3337, Acc: 0.5786, AUC: 0.8100, F1: 0.4531
[VIT] val - Epoch: 62, Loss: 0.9903, Acc: 0.6226, AUC: 0.8327, F1: 0.4447
epoch: 63, total loss: 7.0270792007446286
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 1.4884, Acc: 0.5943, AUC: 0.7865, F1: 0.4557
[VIT] val - Epoch: 63, Loss: 1.0228, Acc: 0.6226, AUC: 0.8325, F1: 0.4930
epoch: 64, total loss: 6.8781548023223875
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 1.2318, Acc: 0.5975, AUC: 0.8178, F1: 0.4739
[VIT] val - Epoch: 64, Loss: 1.0088, Acc: 0.6509, AUC: 0.8376, F1: 0.5151
epoch: 65, total loss: 6.708894157409668
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 1.2092, Acc: 0.5849, AUC: 0.8124, F1: 0.4756
[VIT] val - Epoch: 65, Loss: 0.9878, Acc: 0.6447, AUC: 0.8281, F1: 0.4983
epoch: 66, total loss: 6.935536336898804
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 1.6231, Acc: 0.5094, AUC: 0.8295, F1: 0.4165
[VIT] val - Epoch: 66, Loss: 1.0062, Acc: 0.6289, AUC: 0.8460, F1: 0.4972
epoch: 67, total loss: 6.777092170715332
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 1.3521, Acc: 0.5629, AUC: 0.8251, F1: 0.4306
[VIT] val - Epoch: 67, Loss: 1.1522, Acc: 0.6195, AUC: 0.8423, F1: 0.4714
epoch: 68, total loss: 7.220308876037597
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 1.3040, Acc: 0.5597, AUC: 0.8104, F1: 0.4236
[VIT] val - Epoch: 68, Loss: 1.0573, Acc: 0.6415, AUC: 0.8366, F1: 0.4914
epoch: 69, total loss: 6.804794836044311
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 1.6011, Acc: 0.5409, AUC: 0.8036, F1: 0.4422
[VIT] val - Epoch: 69, Loss: 0.9953, Acc: 0.6352, AUC: 0.8414, F1: 0.4622
epoch: 70, total loss: 6.687863969802857
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 1.5299, Acc: 0.6006, AUC: 0.8233, F1: 0.4673
[VIT] val - Epoch: 70, Loss: 1.1014, Acc: 0.6258, AUC: 0.8351, F1: 0.4847
epoch: 71, total loss: 6.575810146331787
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 1.3198, Acc: 0.5818, AUC: 0.8180, F1: 0.4696
[VIT] val - Epoch: 71, Loss: 1.1206, Acc: 0.6289, AUC: 0.8425, F1: 0.4724
epoch: 72, total loss: 6.729324150085449
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 1.6949, Acc: 0.5031, AUC: 0.8103, F1: 0.4222
[VIT] val - Epoch: 72, Loss: 1.0303, Acc: 0.6352, AUC: 0.8400, F1: 0.5122
epoch: 73, total loss: 6.8671458721160885
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 1.4456, Acc: 0.5818, AUC: 0.8141, F1: 0.4369
[VIT] val - Epoch: 73, Loss: 1.0004, Acc: 0.6226, AUC: 0.8387, F1: 0.5010
epoch: 74, total loss: 6.706486558914184
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 1.5602, Acc: 0.5440, AUC: 0.8055, F1: 0.4598
[VIT] val - Epoch: 74, Loss: 1.0408, Acc: 0.6289, AUC: 0.8425, F1: 0.4889
epoch: 75, total loss: 6.5935122013092045
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 1.4043, Acc: 0.6258, AUC: 0.8104, F1: 0.4431
[VIT] val - Epoch: 75, Loss: 1.0639, Acc: 0.6352, AUC: 0.8332, F1: 0.4897
epoch: 76, total loss: 6.632941198348999
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 1.5454, Acc: 0.5692, AUC: 0.8161, F1: 0.4392
[VIT] val - Epoch: 76, Loss: 1.0650, Acc: 0.6447, AUC: 0.8299, F1: 0.4718
epoch: 77, total loss: 6.61013355255127
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 1.4095, Acc: 0.5566, AUC: 0.7981, F1: 0.4563
[VIT] val - Epoch: 77, Loss: 1.0875, Acc: 0.6509, AUC: 0.8371, F1: 0.4612
epoch: 78, total loss: 6.305085945129394
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 1.6708, Acc: 0.5975, AUC: 0.8027, F1: 0.4595
[VIT] val - Epoch: 78, Loss: 1.0763, Acc: 0.6195, AUC: 0.8372, F1: 0.4827
epoch: 79, total loss: 6.374500846862793
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 1.5333, Acc: 0.5975, AUC: 0.8097, F1: 0.4612
[VIT] val - Epoch: 79, Loss: 1.1028, Acc: 0.6604, AUC: 0.8362, F1: 0.5061
epoch: 80, total loss: 6.675355386734009
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 1.8074, Acc: 0.4906, AUC: 0.8040, F1: 0.3962
[VIT] val - Epoch: 80, Loss: 1.0959, Acc: 0.6698, AUC: 0.8285, F1: 0.4768
epoch: 81, total loss: 6.387334871292114
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 1.4048, Acc: 0.5629, AUC: 0.8255, F1: 0.4645
[VIT] val - Epoch: 81, Loss: 1.0622, Acc: 0.6572, AUC: 0.8342, F1: 0.5195
epoch: 82, total loss: 6.4386414051055905
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 1.6124, Acc: 0.6006, AUC: 0.8110, F1: 0.4415
[VIT] val - Epoch: 82, Loss: 1.0350, Acc: 0.6415, AUC: 0.8380, F1: 0.4998
epoch: 83, total loss: 6.6652271270751955
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 1.5258, Acc: 0.5975, AUC: 0.8149, F1: 0.4985
[VIT] val - Epoch: 83, Loss: 0.9951, Acc: 0.6667, AUC: 0.8343, F1: 0.4923
epoch: 84, total loss: 6.515408134460449
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 1.4855, Acc: 0.5849, AUC: 0.8407, F1: 0.5063
[VIT] val - Epoch: 84, Loss: 1.1077, Acc: 0.6478, AUC: 0.8297, F1: 0.4435
epoch: 85, total loss: 6.12255220413208
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 1.5452, Acc: 0.6164, AUC: 0.8048, F1: 0.4581
[VIT] val - Epoch: 85, Loss: 1.0506, Acc: 0.6541, AUC: 0.8334, F1: 0.4751
epoch: 86, total loss: 6.320518255233765
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 1.7009, Acc: 0.5881, AUC: 0.8166, F1: 0.4392
[VIT] val - Epoch: 86, Loss: 1.0579, Acc: 0.6509, AUC: 0.8349, F1: 0.4789
epoch: 87, total loss: 6.532208013534546
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 1.7115, Acc: 0.5472, AUC: 0.8174, F1: 0.4420
[VIT] val - Epoch: 87, Loss: 1.0635, Acc: 0.6258, AUC: 0.8290, F1: 0.4967
epoch: 88, total loss: 6.328803968429566
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 1.8320, Acc: 0.5912, AUC: 0.8122, F1: 0.4407
[VIT] val - Epoch: 88, Loss: 1.0687, Acc: 0.6289, AUC: 0.8296, F1: 0.4551
epoch: 89, total loss: 6.499602365493774
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 1.9775, Acc: 0.5755, AUC: 0.8169, F1: 0.4148
[VIT] val - Epoch: 89, Loss: 1.0660, Acc: 0.6478, AUC: 0.8264, F1: 0.4621
epoch: 90, total loss: 6.17786169052124
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 1.5494, Acc: 0.5503, AUC: 0.8082, F1: 0.3935
[VIT] val - Epoch: 90, Loss: 1.0972, Acc: 0.6289, AUC: 0.8330, F1: 0.5009
epoch: 91, total loss: 6.136048603057861
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 1.4343, Acc: 0.5943, AUC: 0.8164, F1: 0.4386
[VIT] val - Epoch: 91, Loss: 1.0840, Acc: 0.6698, AUC: 0.8289, F1: 0.5118
epoch: 92, total loss: 6.354332971572876
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 2.1993, Acc: 0.5126, AUC: 0.8080, F1: 0.4085
[VIT] val - Epoch: 92, Loss: 1.1467, Acc: 0.6509, AUC: 0.8291, F1: 0.4804
epoch: 93, total loss: 6.103758859634399
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 1.8226, Acc: 0.5723, AUC: 0.8033, F1: 0.4394
[VIT] val - Epoch: 93, Loss: 1.0682, Acc: 0.6604, AUC: 0.8217, F1: 0.4670
epoch: 94, total loss: 6.007119512557983
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 1.5486, Acc: 0.6069, AUC: 0.8195, F1: 0.4955
[VIT] val - Epoch: 94, Loss: 1.1905, Acc: 0.6415, AUC: 0.8378, F1: 0.4863
epoch: 95, total loss: 6.205807065963745
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 1.7780, Acc: 0.5818, AUC: 0.8234, F1: 0.4846
[VIT] val - Epoch: 95, Loss: 1.0966, Acc: 0.6478, AUC: 0.8323, F1: 0.5024
epoch: 96, total loss: 6.217867994308472
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 1.7764, Acc: 0.5755, AUC: 0.8088, F1: 0.4414
[VIT] val - Epoch: 96, Loss: 1.0944, Acc: 0.6384, AUC: 0.8344, F1: 0.5180
epoch: 97, total loss: 6.080133104324341
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 1.7194, Acc: 0.5849, AUC: 0.8035, F1: 0.4612
[VIT] val - Epoch: 97, Loss: 1.1392, Acc: 0.6447, AUC: 0.8236, F1: 0.5197
epoch: 98, total loss: 6.16845293045044
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 1.6744, Acc: 0.5692, AUC: 0.8147, F1: 0.4613
[VIT] val - Epoch: 98, Loss: 1.1534, Acc: 0.6447, AUC: 0.8331, F1: 0.4983
epoch: 99, total loss: 6.400241565704346
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 1.8278, Acc: 0.5912, AUC: 0.8102, F1: 0.4553
[VIT] val - Epoch: 99, Loss: 1.1278, Acc: 0.6604, AUC: 0.8262, F1: 0.5012
epoch: 100, total loss: 6.2633466720581055
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 1.8688, Acc: 0.5881, AUC: 0.8025, F1: 0.4587
[VIT] val - Epoch: 100, Loss: 1.0996, Acc: 0.6635, AUC: 0.8321, F1: 0.5269
[17:47:21][Rank 3] Training Finished. Starting Final Testing...
[17:47:21][Rank 2] Training Finished. Starting Final Testing...
[17:47:21][Rank 1] Training Finished. Starting Final Testing...
[17:47:21][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test_cnn - Epoch: 100, Loss: 2.4695, Acc: 0.3523, AUC: 0.7363, F1: 0.3375
[VIT] test_cnn - Epoch: 100, Loss: 1.5885, Acc: 0.3852, AUC: 0.7874, F1: 0.4065
üöÄ Final Test Results [CNN] - AUC: 0.7363, Acc: 0.3523, F1: 0.3375
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
‚ùå [ÈîôËØØ] Ê∫êÂüü RLDR ËÆ≠ÁªÉÂ§±Ë¥•ÔºÅ

########################################################
üìä ÊúÄÁªàÁªìÊûúÊ±áÊÄª (Running collect_results.py)
########################################################

------------------------------------------------------------
Domain       | Test AUC   | Test Acc   | Test F1   
------------------------------------------------------------
APTOS        | Not Finished (No done file)
DEEPDR       | Not Finished (No done file)
FGADR        | Not Finished (No done file)
IDRID        | Not Finished (No done file)
MESSIDOR     | Not Finished (No done file)
RLDR         | Not Finished (No done file)
------------------------------------------------------------
‚ùå Ê≤°ÊúâÊâæÂà∞ÊúâÊïàÁöÑÊµãËØïÁªìÊûú„ÄÇ
========================================================
üéâ ÊâÄÊúâ‰ªªÂä°ÊâßË°åÂÆåÊØï
========================================================
Python script finished. Checking checkpoints for status...
Checking directory: ./output_esdg_h100/GDRNet_ESDG_MESSIDOR
‚ùå Neither final nor latest checkpoint found.
   This implies the job failed before the first epoch or path is wrong.
