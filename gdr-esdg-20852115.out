Job 20852115 started at Fri 27 Feb 2026 06:05:08 AM AEDT
========================================================
üöÄ ÂêØÂä® ESDG ÊâπÈáèÂÆûÈ™å (Bash Âæ™ÁéØÊ®°Âºè)
GPU Êï∞Èáè: 4
ÂæÖËøêË°åÊ∫êÂüü: APTOS DEEPDR FGADR IDRID MESSIDOR RLDR
Âü∫Á°ÄËæìÂá∫ÁõÆÂΩï: ./output_esdg_h100
========================================================

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: APTOS
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='APTOS', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['APTOS']
  TARGET_DOMAINS: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_APTOS
OUT_DIR: ./output_esdg_h100/APTOS
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified2/output_esdg_h100/APTOS/CASS_GDRNet_ESDG_APTOS
[06:08:15][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
================ [Auto Config] ================
Source: ['APTOS']
Targets: ['DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/APTOS
===============================================
[06:08:15][Rank 1] Loading datasets...
[06:08:15][Rank 3] Loading datasets...
[06:08:15][Rank 2] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
üßä [DINOv3] All base parameters frozen.
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_projInjecting LoRA into: layer.7.attention.k_proj

Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_projInjecting LoRA into: layer.9.attention.k_proj

Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.o_projInjecting LoRA into: layer.9.attention.v_proj

Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_projInjecting LoRA into: layer.10.mlp.up_proj

Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 3.5426979039026345
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 0.9905, Acc: 0.6653, AUC: 0.6414, F1: 0.2949
[VIT] val - Epoch: 1, Loss: 1.2267, Acc: 0.5738, AUC: 0.6833, F1: 0.2284
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6414
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6833
Saving vit model...
epoch: 2, total loss: 2.785182885501696
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.0241, Acc: 0.5464, AUC: 0.6809, F1: 0.2407
[VIT] val - Epoch: 2, Loss: 1.0882, Acc: 0.6653, AUC: 0.7537, F1: 0.2891
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6809
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7537
Saving vit model...
epoch: 3, total loss: 2.699911190115887
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 0.8069, Acc: 0.7145, AUC: 0.7049, F1: 0.3239
[VIT] val - Epoch: 3, Loss: 1.0002, Acc: 0.6831, AUC: 0.7658, F1: 0.3284
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7049
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7658
Saving vit model...
epoch: 4, total loss: 2.339745653712231
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 0.8116, Acc: 0.7131, AUC: 0.7339, F1: 0.3539
[VIT] val - Epoch: 4, Loss: 0.9424, Acc: 0.7199, AUC: 0.7965, F1: 0.3670
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7339
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7965
Saving vit model...
epoch: 5, total loss: 2.3467854028162747
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 0.7817, Acc: 0.7049, AUC: 0.7396, F1: 0.3409
[VIT] val - Epoch: 5, Loss: 0.8674, Acc: 0.7104, AUC: 0.8272, F1: 0.3246
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7396
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8272
Saving vit model...
epoch: 6, total loss: 2.460103343362394
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 0.7415, Acc: 0.7104, AUC: 0.7626, F1: 0.3922
[VIT] val - Epoch: 6, Loss: 0.8774, Acc: 0.7432, AUC: 0.8173, F1: 0.4328
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7626
Saving cnn model...
epoch: 7, total loss: 2.356597224007482
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 0.7654, Acc: 0.6981, AUC: 0.7659, F1: 0.3750
[VIT] val - Epoch: 7, Loss: 0.8315, Acc: 0.7117, AUC: 0.8153, F1: 0.4262
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7659
Saving cnn model...
epoch: 8, total loss: 2.192890789197839
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.7338, Acc: 0.7500, AUC: 0.7895, F1: 0.4533
[VIT] val - Epoch: 8, Loss: 0.8131, Acc: 0.7445, AUC: 0.8227, F1: 0.5130
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7895
Saving cnn model...
epoch: 9, total loss: 2.238307457903157
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 0.7413, Acc: 0.7172, AUC: 0.7786, F1: 0.4543
[VIT] val - Epoch: 9, Loss: 0.7923, Acc: 0.7473, AUC: 0.8188, F1: 0.5173
epoch: 10, total loss: 2.2586485911970553
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.6898, Acc: 0.7527, AUC: 0.7939, F1: 0.4533
[VIT] val - Epoch: 10, Loss: 0.7600, Acc: 0.7568, AUC: 0.8258, F1: 0.4758
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7939
Saving cnn model...
epoch: 11, total loss: 2.218102812767029
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 0.7124, Acc: 0.7158, AUC: 0.7961, F1: 0.4551
[VIT] val - Epoch: 11, Loss: 0.7522, Acc: 0.7500, AUC: 0.8515, F1: 0.4699
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7961
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8515
Saving vit model...
epoch: 12, total loss: 2.1599565200183704
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 0.7231, Acc: 0.7254, AUC: 0.7946, F1: 0.4707
[VIT] val - Epoch: 12, Loss: 0.7702, Acc: 0.7254, AUC: 0.8382, F1: 0.4870
epoch: 13, total loss: 1.9130380218443663
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.6401, Acc: 0.7473, AUC: 0.8181, F1: 0.4662
[VIT] val - Epoch: 13, Loss: 0.7416, Acc: 0.7500, AUC: 0.8312, F1: 0.5193
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8181
Saving cnn model...
epoch: 14, total loss: 2.1431933265665304
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.6436, Acc: 0.7596, AUC: 0.8183, F1: 0.4619
[VIT] val - Epoch: 14, Loss: 0.7227, Acc: 0.7527, AUC: 0.8549, F1: 0.5322
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8183
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8549
Saving vit model...
epoch: 15, total loss: 2.1275495990462927
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.6569, Acc: 0.7404, AUC: 0.8193, F1: 0.4728
[VIT] val - Epoch: 15, Loss: 0.6976, Acc: 0.7637, AUC: 0.8580, F1: 0.5125
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8193
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8580
Saving vit model...
epoch: 16, total loss: 2.1573281676872917
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.6732, Acc: 0.7377, AUC: 0.8221, F1: 0.4738
[VIT] val - Epoch: 16, Loss: 0.7155, Acc: 0.7527, AUC: 0.8339, F1: 0.5140
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8221
Saving cnn model...
epoch: 17, total loss: 2.015159673017004
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.6464, Acc: 0.7500, AUC: 0.8298, F1: 0.5198
[VIT] val - Epoch: 17, Loss: 0.7068, Acc: 0.7678, AUC: 0.8608, F1: 0.5692
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8298
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8608
Saving vit model...
epoch: 18, total loss: 2.1294154485930568
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 0.6777, Acc: 0.7281, AUC: 0.8178, F1: 0.4864
[VIT] val - Epoch: 18, Loss: 0.6804, Acc: 0.7623, AUC: 0.8468, F1: 0.5285
epoch: 19, total loss: 2.028065642584925
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.5948, Acc: 0.6926, AUC: 0.7228, F1: 0.4622
[VIT] val - Epoch: 19, Loss: 0.6757, Acc: 0.7719, AUC: 0.8665, F1: 0.5712
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8665
Saving vit model...
epoch: 20, total loss: 2.0221872329711914
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.6476, Acc: 0.7445, AUC: 0.8259, F1: 0.5133
[VIT] val - Epoch: 20, Loss: 0.6728, Acc: 0.7719, AUC: 0.8642, F1: 0.5812
epoch: 21, total loss: 2.0306396484375
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.6978, Acc: 0.7199, AUC: 0.8176, F1: 0.5072
[VIT] val - Epoch: 21, Loss: 0.6602, Acc: 0.7691, AUC: 0.8591, F1: 0.5503
epoch: 22, total loss: 2.085525129152381
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.6625, Acc: 0.7377, AUC: 0.8316, F1: 0.4542
[VIT] val - Epoch: 22, Loss: 0.6476, Acc: 0.7787, AUC: 0.8709, F1: 0.5520
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8316
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8709
Saving vit model...
epoch: 23, total loss: 1.9435255553411401
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.5913, Acc: 0.7582, AUC: 0.8435, F1: 0.5061
[VIT] val - Epoch: 23, Loss: 0.6307, Acc: 0.7869, AUC: 0.8678, F1: 0.5489
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8435
Saving cnn model...
epoch: 24, total loss: 2.0613448244074117
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.6115, Acc: 0.7486, AUC: 0.8334, F1: 0.5075
[VIT] val - Epoch: 24, Loss: 0.6451, Acc: 0.7869, AUC: 0.8622, F1: 0.5788
epoch: 25, total loss: 1.949143930621769
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.5920, Acc: 0.7596, AUC: 0.8529, F1: 0.5595
[VIT] val - Epoch: 25, Loss: 0.6469, Acc: 0.7842, AUC: 0.8746, F1: 0.5900
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8529
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8746
Saving vit model...
epoch: 26, total loss: 2.2129500352818035
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.6308, Acc: 0.7486, AUC: 0.8424, F1: 0.5492
[VIT] val - Epoch: 26, Loss: 0.6510, Acc: 0.7555, AUC: 0.8702, F1: 0.5410
epoch: 27, total loss: 1.9313460562540137
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.5787, Acc: 0.7691, AUC: 0.8480, F1: 0.5491
[VIT] val - Epoch: 27, Loss: 0.6224, Acc: 0.7842, AUC: 0.8665, F1: 0.5611
epoch: 28, total loss: 2.0332417060499606
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.6021, Acc: 0.7596, AUC: 0.8339, F1: 0.5477
[VIT] val - Epoch: 28, Loss: 0.6301, Acc: 0.7773, AUC: 0.8771, F1: 0.5854
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8771
Saving vit model...
epoch: 29, total loss: 1.8351363954336748
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.6042, Acc: 0.7609, AUC: 0.8479, F1: 0.5609
[VIT] val - Epoch: 29, Loss: 0.6318, Acc: 0.7842, AUC: 0.8785, F1: 0.5948
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8785
Saving vit model...
epoch: 30, total loss: 1.9884759032207986
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.5984, Acc: 0.7404, AUC: 0.8522, F1: 0.5410
[VIT] val - Epoch: 30, Loss: 0.6222, Acc: 0.7828, AUC: 0.8780, F1: 0.6001
epoch: 31, total loss: 1.9548678942348645
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.5733, Acc: 0.7596, AUC: 0.8588, F1: 0.5651
[VIT] val - Epoch: 31, Loss: 0.6103, Acc: 0.7951, AUC: 0.8792, F1: 0.6010
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8588
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8792
Saving vit model...
epoch: 32, total loss: 1.9186344729817433
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.6179, Acc: 0.7404, AUC: 0.8511, F1: 0.5353
[VIT] val - Epoch: 32, Loss: 0.6218, Acc: 0.7910, AUC: 0.8661, F1: 0.6006
epoch: 33, total loss: 2.018256239269091
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.5700, Acc: 0.7828, AUC: 0.8615, F1: 0.5859
[VIT] val - Epoch: 33, Loss: 0.6013, Acc: 0.7883, AUC: 0.8783, F1: 0.5872
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8615
Saving cnn model...
epoch: 34, total loss: 2.097036141416301
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.6594, Acc: 0.7377, AUC: 0.8381, F1: 0.5349
[VIT] val - Epoch: 34, Loss: 0.6137, Acc: 0.7814, AUC: 0.8769, F1: 0.6017
epoch: 35, total loss: 1.9857775283896404
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.6251, Acc: 0.7445, AUC: 0.8509, F1: 0.5438
[VIT] val - Epoch: 35, Loss: 0.6158, Acc: 0.8019, AUC: 0.8765, F1: 0.6287
epoch: 36, total loss: 1.9220072989878447
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.5744, Acc: 0.7719, AUC: 0.8589, F1: 0.5628
[VIT] val - Epoch: 36, Loss: 0.6076, Acc: 0.7992, AUC: 0.8753, F1: 0.5952
epoch: 37, total loss: 1.8744739203349403
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.5494, Acc: 0.7814, AUC: 0.8642, F1: 0.5707
[VIT] val - Epoch: 37, Loss: 0.5983, Acc: 0.7964, AUC: 0.8793, F1: 0.5950
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8642
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8793
Saving vit model...
epoch: 38, total loss: 1.9881460329760676
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.5896, Acc: 0.7555, AUC: 0.8613, F1: 0.5682
[VIT] val - Epoch: 38, Loss: 0.6186, Acc: 0.7787, AUC: 0.8784, F1: 0.5896
epoch: 39, total loss: 1.9532068566135738
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.5734, Acc: 0.7719, AUC: 0.8636, F1: 0.5782
[VIT] val - Epoch: 39, Loss: 0.5983, Acc: 0.8005, AUC: 0.8829, F1: 0.6100
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8829
Saving vit model...
epoch: 40, total loss: 1.9331974594489387
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.6058, Acc: 0.7527, AUC: 0.8543, F1: 0.5463
[VIT] val - Epoch: 40, Loss: 0.6061, Acc: 0.7801, AUC: 0.8808, F1: 0.5997
epoch: 41, total loss: 1.8614526909330618
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.5686, Acc: 0.7609, AUC: 0.8565, F1: 0.5576
[VIT] val - Epoch: 41, Loss: 0.5882, Acc: 0.7978, AUC: 0.8829, F1: 0.6113
epoch: 42, total loss: 1.9244285251783289
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.5879, Acc: 0.7527, AUC: 0.8472, F1: 0.5256
[VIT] val - Epoch: 42, Loss: 0.6026, Acc: 0.7978, AUC: 0.8797, F1: 0.6235
epoch: 43, total loss: 1.825999792503274
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.5768, Acc: 0.7787, AUC: 0.8576, F1: 0.5883
[VIT] val - Epoch: 43, Loss: 0.5873, Acc: 0.8060, AUC: 0.8834, F1: 0.6407
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8834
Saving vit model...
epoch: 44, total loss: 1.8886443259923353
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.6254, Acc: 0.7527, AUC: 0.8512, F1: 0.5605
[VIT] val - Epoch: 44, Loss: 0.5933, Acc: 0.7842, AUC: 0.8789, F1: 0.6010
epoch: 45, total loss: 1.7509499969689741
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.5672, Acc: 0.7746, AUC: 0.8611, F1: 0.5607
[VIT] val - Epoch: 45, Loss: 0.5761, Acc: 0.7896, AUC: 0.8813, F1: 0.5815
epoch: 46, total loss: 1.8993526878564253
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.5789, Acc: 0.7691, AUC: 0.8612, F1: 0.5737
[VIT] val - Epoch: 46, Loss: 0.5887, Acc: 0.7951, AUC: 0.8809, F1: 0.6063
epoch: 47, total loss: 1.7517804218375164
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.5610, Acc: 0.7814, AUC: 0.8613, F1: 0.5644
[VIT] val - Epoch: 47, Loss: 0.5791, Acc: 0.7951, AUC: 0.8834, F1: 0.5976
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8834
Saving vit model...
epoch: 48, total loss: 1.8099491375943888
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.5631, Acc: 0.7719, AUC: 0.8593, F1: 0.5650
[VIT] val - Epoch: 48, Loss: 0.5818, Acc: 0.8046, AUC: 0.8850, F1: 0.6290
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8850
Saving vit model...
epoch: 49, total loss: 1.8195705219455387
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.5580, Acc: 0.7951, AUC: 0.8597, F1: 0.6047
[VIT] val - Epoch: 49, Loss: 0.5825, Acc: 0.8142, AUC: 0.8809, F1: 0.6491
epoch: 50, total loss: 1.950869603001553
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.5610, Acc: 0.7609, AUC: 0.8616, F1: 0.5594
[VIT] val - Epoch: 50, Loss: 0.5755, Acc: 0.8046, AUC: 0.8851, F1: 0.6212
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8851
Saving vit model...
epoch: 51, total loss: 1.6742701543414074
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.5502, Acc: 0.7814, AUC: 0.8689, F1: 0.5930
[VIT] val - Epoch: 51, Loss: 0.5782, Acc: 0.8060, AUC: 0.8822, F1: 0.6330
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8689
Saving cnn model...
epoch: 52, total loss: 1.8907697395138119
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.5571, Acc: 0.7760, AUC: 0.8644, F1: 0.5744
[VIT] val - Epoch: 52, Loss: 0.5725, Acc: 0.8101, AUC: 0.8864, F1: 0.6327
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8864
Saving vit model...
epoch: 53, total loss: 1.7761555832365286
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.5621, Acc: 0.7801, AUC: 0.8629, F1: 0.5570
[VIT] val - Epoch: 53, Loss: 0.5714, Acc: 0.8156, AUC: 0.8840, F1: 0.6491
epoch: 54, total loss: 1.746283287587373
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.5565, Acc: 0.7828, AUC: 0.8676, F1: 0.5893
[VIT] val - Epoch: 54, Loss: 0.5725, Acc: 0.8033, AUC: 0.8830, F1: 0.6347
epoch: 55, total loss: 1.9866482770961265
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.5833, Acc: 0.7596, AUC: 0.8677, F1: 0.5779
[VIT] val - Epoch: 55, Loss: 0.5780, Acc: 0.7923, AUC: 0.8889, F1: 0.6181
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8889
Saving vit model...
epoch: 56, total loss: 1.8958217022211656
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.5457, Acc: 0.7842, AUC: 0.8721, F1: 0.5985
[VIT] val - Epoch: 56, Loss: 0.5820, Acc: 0.7923, AUC: 0.8826, F1: 0.6099
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8721
Saving cnn model...
epoch: 57, total loss: 1.987855751229369
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.5847, Acc: 0.7650, AUC: 0.8694, F1: 0.5753
[VIT] val - Epoch: 57, Loss: 0.5799, Acc: 0.7869, AUC: 0.8831, F1: 0.6264
epoch: 58, total loss: 1.766253747369932
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.5235, Acc: 0.8128, AUC: 0.8800, F1: 0.6472
[VIT] val - Epoch: 58, Loss: 0.5691, Acc: 0.7992, AUC: 0.8876, F1: 0.6108
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8800
Saving cnn model...
epoch: 59, total loss: 1.9825977566449537
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.5269, Acc: 0.7951, AUC: 0.8764, F1: 0.6152
[VIT] val - Epoch: 59, Loss: 0.5636, Acc: 0.8033, AUC: 0.8819, F1: 0.6138
epoch: 60, total loss: 1.7760479838951775
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.5371, Acc: 0.7801, AUC: 0.8737, F1: 0.5874
[VIT] val - Epoch: 60, Loss: 0.5758, Acc: 0.7992, AUC: 0.8890, F1: 0.6340
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8890
Saving vit model...
epoch: 61, total loss: 1.689586597940196
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.5308, Acc: 0.8019, AUC: 0.8805, F1: 0.6244
[VIT] val - Epoch: 61, Loss: 0.5690, Acc: 0.8074, AUC: 0.8847, F1: 0.6344
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8805
Saving cnn model...
epoch: 62, total loss: 1.7807690604873325
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.5526, Acc: 0.7842, AUC: 0.8761, F1: 0.6187
[VIT] val - Epoch: 62, Loss: 0.5735, Acc: 0.8074, AUC: 0.8889, F1: 0.6442
epoch: 63, total loss: 1.840202273234077
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.5063, Acc: 0.8033, AUC: 0.8850, F1: 0.6227
[VIT] val - Epoch: 63, Loss: 0.5615, Acc: 0.8169, AUC: 0.8877, F1: 0.6510
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8850
Saving cnn model...
epoch: 64, total loss: 1.7750515251056007
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.5340, Acc: 0.7814, AUC: 0.8780, F1: 0.5967
[VIT] val - Epoch: 64, Loss: 0.5604, Acc: 0.8033, AUC: 0.8897, F1: 0.6200
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8897
Saving vit model...
epoch: 65, total loss: 1.7956634060196255
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.5282, Acc: 0.7801, AUC: 0.8798, F1: 0.5893
[VIT] val - Epoch: 65, Loss: 0.5671, Acc: 0.8019, AUC: 0.8834, F1: 0.6182
epoch: 66, total loss: 1.9772364432397096
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.5476, Acc: 0.7910, AUC: 0.8715, F1: 0.6194
[VIT] val - Epoch: 66, Loss: 0.5559, Acc: 0.8087, AUC: 0.8839, F1: 0.6324
epoch: 67, total loss: 1.8228005067161892
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.5339, Acc: 0.8005, AUC: 0.8771, F1: 0.6179
[VIT] val - Epoch: 67, Loss: 0.5617, Acc: 0.8101, AUC: 0.8864, F1: 0.6553
epoch: 68, total loss: 1.6692613687204279
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.5296, Acc: 0.7842, AUC: 0.8767, F1: 0.6006
[VIT] val - Epoch: 68, Loss: 0.5581, Acc: 0.8115, AUC: 0.8899, F1: 0.6409
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8899
Saving vit model...
epoch: 69, total loss: 1.7351156965545986
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.5138, Acc: 0.7883, AUC: 0.8863, F1: 0.6090
[VIT] val - Epoch: 69, Loss: 0.5684, Acc: 0.8046, AUC: 0.8838, F1: 0.6366
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8863
Saving cnn model...
epoch: 70, total loss: 1.7223204892614614
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.5189, Acc: 0.7910, AUC: 0.8812, F1: 0.6081
[VIT] val - Epoch: 70, Loss: 0.5599, Acc: 0.8115, AUC: 0.8900, F1: 0.6419
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8900
Saving vit model...
epoch: 71, total loss: 1.7610402612582496
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.5174, Acc: 0.7923, AUC: 0.8850, F1: 0.6208
[VIT] val - Epoch: 71, Loss: 0.5616, Acc: 0.8087, AUC: 0.8880, F1: 0.6448
epoch: 72, total loss: 1.682396665863369
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.5251, Acc: 0.7978, AUC: 0.8813, F1: 0.6302
[VIT] val - Epoch: 72, Loss: 0.5528, Acc: 0.8101, AUC: 0.8869, F1: 0.6282
epoch: 73, total loss: 1.6250461637973785
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.5168, Acc: 0.8087, AUC: 0.8835, F1: 0.6533
[VIT] val - Epoch: 73, Loss: 0.5578, Acc: 0.8060, AUC: 0.8902, F1: 0.6374
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8902
Saving vit model...
epoch: 74, total loss: 1.8382828624352165
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.5320, Acc: 0.7951, AUC: 0.8800, F1: 0.6210
[VIT] val - Epoch: 74, Loss: 0.5530, Acc: 0.8074, AUC: 0.8920, F1: 0.6289
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8920
Saving vit model...
epoch: 75, total loss: 1.728687380966933
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.5180, Acc: 0.7883, AUC: 0.8857, F1: 0.6124
[VIT] val - Epoch: 75, Loss: 0.5643, Acc: 0.8101, AUC: 0.8865, F1: 0.6400
epoch: 76, total loss: 1.6964326131602991
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.5562, Acc: 0.7787, AUC: 0.8723, F1: 0.6022
[VIT] val - Epoch: 76, Loss: 0.5574, Acc: 0.8087, AUC: 0.8913, F1: 0.6383
epoch: 77, total loss: 1.7028939438902813
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.5278, Acc: 0.7801, AUC: 0.8753, F1: 0.5916
[VIT] val - Epoch: 77, Loss: 0.5568, Acc: 0.8074, AUC: 0.8840, F1: 0.6354
epoch: 78, total loss: 1.7585770163847052
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.5211, Acc: 0.7910, AUC: 0.8829, F1: 0.6280
[VIT] val - Epoch: 78, Loss: 0.5580, Acc: 0.7992, AUC: 0.8940, F1: 0.6306
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8940
Saving vit model...
epoch: 79, total loss: 1.8414611453595369
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.5196, Acc: 0.7951, AUC: 0.8815, F1: 0.6154
[VIT] val - Epoch: 79, Loss: 0.5553, Acc: 0.8046, AUC: 0.8916, F1: 0.6246
epoch: 80, total loss: 1.5951117821361707
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.5200, Acc: 0.7896, AUC: 0.8826, F1: 0.6186
[VIT] val - Epoch: 80, Loss: 0.5509, Acc: 0.8087, AUC: 0.8900, F1: 0.6407
epoch: 81, total loss: 1.76485359798307
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.5281, Acc: 0.7869, AUC: 0.8807, F1: 0.6116
[VIT] val - Epoch: 81, Loss: 0.5567, Acc: 0.8128, AUC: 0.8871, F1: 0.6417
epoch: 82, total loss: 1.5940315438353496
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.5268, Acc: 0.7923, AUC: 0.8820, F1: 0.6258
[VIT] val - Epoch: 82, Loss: 0.5606, Acc: 0.8046, AUC: 0.8887, F1: 0.6382
epoch: 83, total loss: 1.7254129907359248
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.5084, Acc: 0.7992, AUC: 0.8824, F1: 0.6291
[VIT] val - Epoch: 83, Loss: 0.5479, Acc: 0.8197, AUC: 0.8879, F1: 0.6513
epoch: 84, total loss: 1.558592721172001
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.5375, Acc: 0.7869, AUC: 0.8766, F1: 0.6181
[VIT] val - Epoch: 84, Loss: 0.5539, Acc: 0.8060, AUC: 0.8896, F1: 0.6397
epoch: 85, total loss: 1.6270196969094484
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.5135, Acc: 0.8019, AUC: 0.8818, F1: 0.6290
[VIT] val - Epoch: 85, Loss: 0.5441, Acc: 0.8142, AUC: 0.8908, F1: 0.6442
epoch: 86, total loss: 1.663231760263443
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.5187, Acc: 0.7964, AUC: 0.8808, F1: 0.6219
[VIT] val - Epoch: 86, Loss: 0.5452, Acc: 0.8142, AUC: 0.8915, F1: 0.6478
epoch: 87, total loss: 1.7725712786550107
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.5156, Acc: 0.7992, AUC: 0.8829, F1: 0.6200
[VIT] val - Epoch: 87, Loss: 0.5447, Acc: 0.8156, AUC: 0.8924, F1: 0.6473
epoch: 88, total loss: 1.7760060740553814
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.5563, Acc: 0.7773, AUC: 0.8790, F1: 0.6079
[VIT] val - Epoch: 88, Loss: 0.5541, Acc: 0.8142, AUC: 0.8872, F1: 0.6533
epoch: 89, total loss: 1.6423984727133876
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.5469, Acc: 0.7828, AUC: 0.8771, F1: 0.6107
[VIT] val - Epoch: 89, Loss: 0.5479, Acc: 0.8101, AUC: 0.8870, F1: 0.6429
epoch: 90, total loss: 1.7731590569019318
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.5169, Acc: 0.7883, AUC: 0.8837, F1: 0.6134
[VIT] val - Epoch: 90, Loss: 0.5567, Acc: 0.8060, AUC: 0.8920, F1: 0.6463
epoch: 91, total loss: 1.7018213116604348
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.5058, Acc: 0.8101, AUC: 0.8816, F1: 0.6319
[VIT] val - Epoch: 91, Loss: 0.5477, Acc: 0.8087, AUC: 0.8918, F1: 0.6241
epoch: 92, total loss: 1.5688913840314616
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.5271, Acc: 0.7896, AUC: 0.8782, F1: 0.6142
[VIT] val - Epoch: 92, Loss: 0.5525, Acc: 0.8060, AUC: 0.8913, F1: 0.6322
epoch: 93, total loss: 1.6645184923773226
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.4910, Acc: 0.8019, AUC: 0.8900, F1: 0.6227
[VIT] val - Epoch: 93, Loss: 0.5474, Acc: 0.8115, AUC: 0.8933, F1: 0.6444
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8900
Saving cnn model...
epoch: 94, total loss: 1.7065372920554618
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.5225, Acc: 0.7951, AUC: 0.8832, F1: 0.6263
[VIT] val - Epoch: 94, Loss: 0.5501, Acc: 0.8115, AUC: 0.8908, F1: 0.6416
epoch: 95, total loss: 1.738040596894596
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.5024, Acc: 0.8087, AUC: 0.8870, F1: 0.6323
[VIT] val - Epoch: 95, Loss: 0.5463, Acc: 0.8101, AUC: 0.8916, F1: 0.6365
epoch: 96, total loss: 1.6169289240370626
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.5083, Acc: 0.8128, AUC: 0.8854, F1: 0.6361
[VIT] val - Epoch: 96, Loss: 0.5453, Acc: 0.8087, AUC: 0.8952, F1: 0.6394
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8952
Saving vit model...
epoch: 97, total loss: 1.7778011016223743
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.5134, Acc: 0.7883, AUC: 0.8866, F1: 0.6094
[VIT] val - Epoch: 97, Loss: 0.5464, Acc: 0.8142, AUC: 0.8908, F1: 0.6514
epoch: 98, total loss: 1.701489456321882
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.4889, Acc: 0.8033, AUC: 0.8929, F1: 0.6306
[VIT] val - Epoch: 98, Loss: 0.5463, Acc: 0.8210, AUC: 0.8934, F1: 0.6620
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8929
Saving cnn model...
epoch: 99, total loss: 1.7007535838562509
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.5267, Acc: 0.7896, AUC: 0.8847, F1: 0.6308
[VIT] val - Epoch: 99, Loss: 0.5445, Acc: 0.8142, AUC: 0.8956, F1: 0.6522
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8956
Saving vit model...
epoch: 100, total loss: 1.7207799825979315
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.4914, Acc: 0.8060, AUC: 0.8932, F1: 0.6324
[VIT] val - Epoch: 100, Loss: 0.5437, Acc: 0.8169, AUC: 0.8931, F1: 0.6506
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8932
Saving cnn model...
[06:50:33][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
[06:50:33][Rank 2] Training Finished. Starting Final Testing...
[06:50:33][Rank 3] Training Finished. Starting Final Testing...
[06:50:33][Rank 1] Training Finished. Starting Final Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.3912, Acc: 0.6018, AUC: 0.7180, F1: 0.3843
[VIT] test - Epoch: 100, Loss: 1.0429, Acc: 0.6510, AUC: 0.7130, F1: 0.3558
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.3138, Acc: 0.5996, AUC: 0.7232, F1: 0.3768
[VIT] test - Epoch: 100, Loss: 1.0643, Acc: 0.6508, AUC: 0.7072, F1: 0.3527
‚úÖ [ÂÆåÊàê] Ê∫êÂüü APTOS ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: DEEPDR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='DEEPDR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['DEEPDR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_DEEPDR
OUT_DIR: ./output_esdg_h100/DEEPDR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified2/output_esdg_h100/DEEPDR/CASS_GDRNet_ESDG_DEEPDR
[06:58:48][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
[06:58:48][Rank 1] Loading datasets...
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
[06:58:48][Rank 2] Loading datasets...
================ [Auto Config] ================
Source: ['DEEPDR']
Targets: ['APTOS', 'DDR', 'FGADR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/DEEPDR
===============================================
[06:58:48][Rank 3] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.3.attention.o_proj
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 4.314975547790527
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 38.0627, Acc: 0.4906, AUC: 0.5239, F1: 0.1317
[VIT] val - Epoch: 1, Loss: 1.3894, Acc: 0.5000, AUC: 0.5856, F1: 0.2023
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5239
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5856
Saving vit model...
epoch: 2, total loss: 3.7303815364837645
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 111.1884, Acc: 0.5437, AUC: 0.5444, F1: 0.2403
[VIT] val - Epoch: 2, Loss: 1.3633, Acc: 0.5406, AUC: 0.6775, F1: 0.3369
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5444
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6775
Saving vit model...
epoch: 3, total loss: 3.3700802087783814
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 2.1765, Acc: 0.5344, AUC: 0.6589, F1: 0.2207
[VIT] val - Epoch: 3, Loss: 1.2857, Acc: 0.5750, AUC: 0.7335, F1: 0.3864
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6589
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7335
Saving vit model...
epoch: 4, total loss: 3.1190009832382204
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 1.4178, Acc: 0.5188, AUC: 0.6051, F1: 0.2540
[VIT] val - Epoch: 4, Loss: 1.2748, Acc: 0.5750, AUC: 0.7432, F1: 0.4226
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7432
Saving vit model...
epoch: 5, total loss: 3.365927243232727
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.2363, Acc: 0.5188, AUC: 0.6804, F1: 0.2999
[VIT] val - Epoch: 5, Loss: 1.2225, Acc: 0.5906, AUC: 0.7616, F1: 0.4610
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6804
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7616
Saving vit model...
epoch: 6, total loss: 3.158382606506348
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.1792, Acc: 0.5656, AUC: 0.7108, F1: 0.2921
[VIT] val - Epoch: 6, Loss: 1.1634, Acc: 0.6125, AUC: 0.7769, F1: 0.4310
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7108
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7769
Saving vit model...
epoch: 7, total loss: 3.0955710887908934
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.1237, Acc: 0.5969, AUC: 0.7058, F1: 0.3080
[VIT] val - Epoch: 7, Loss: 1.1657, Acc: 0.6562, AUC: 0.7673, F1: 0.4669
epoch: 8, total loss: 2.9576600015163423
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 1.0856, Acc: 0.6156, AUC: 0.7385, F1: 0.3523
[VIT] val - Epoch: 8, Loss: 1.1209, Acc: 0.6656, AUC: 0.8130, F1: 0.4952
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7385
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8130
Saving vit model...
epoch: 9, total loss: 2.927433931827545
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 1.1204, Acc: 0.5656, AUC: 0.7139, F1: 0.3090
[VIT] val - Epoch: 9, Loss: 1.1049, Acc: 0.6875, AUC: 0.8099, F1: 0.5094
epoch: 10, total loss: 2.9000829696655273
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 1.1595, Acc: 0.5719, AUC: 0.6891, F1: 0.3253
[VIT] val - Epoch: 10, Loss: 1.0890, Acc: 0.6875, AUC: 0.7936, F1: 0.5330
epoch: 11, total loss: 2.956338119506836
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 1.1300, Acc: 0.5750, AUC: 0.7131, F1: 0.3001
[VIT] val - Epoch: 11, Loss: 1.0488, Acc: 0.6687, AUC: 0.8357, F1: 0.5094
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8357
Saving vit model...
epoch: 12, total loss: 3.021977925300598
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.0883, Acc: 0.5563, AUC: 0.7397, F1: 0.3129
[VIT] val - Epoch: 12, Loss: 1.0626, Acc: 0.6594, AUC: 0.8179, F1: 0.4876
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7397
Saving cnn model...
epoch: 13, total loss: 2.8421335339546205
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.0514, Acc: 0.6375, AUC: 0.7661, F1: 0.3662
[VIT] val - Epoch: 13, Loss: 1.0321, Acc: 0.6687, AUC: 0.8272, F1: 0.5085
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7661
Saving cnn model...
epoch: 14, total loss: 2.9393773555755613
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 1.1157, Acc: 0.5844, AUC: 0.7214, F1: 0.3102
[VIT] val - Epoch: 14, Loss: 1.0269, Acc: 0.6813, AUC: 0.8409, F1: 0.4822
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8409
Saving vit model...
epoch: 15, total loss: 2.9853325366973875
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 1.3075, Acc: 0.5656, AUC: 0.6776, F1: 0.3402
[VIT] val - Epoch: 15, Loss: 1.0496, Acc: 0.6625, AUC: 0.8325, F1: 0.5255
epoch: 16, total loss: 2.7330100536346436
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 1.0683, Acc: 0.5625, AUC: 0.7431, F1: 0.3725
[VIT] val - Epoch: 16, Loss: 1.0041, Acc: 0.6781, AUC: 0.8438, F1: 0.5573
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8438
Saving vit model...
epoch: 17, total loss: 3.040966200828552
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.9486, Acc: 0.6156, AUC: 0.7968, F1: 0.3723
[VIT] val - Epoch: 17, Loss: 1.0029, Acc: 0.6656, AUC: 0.8491, F1: 0.5570
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7968
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8491
Saving vit model...
epoch: 18, total loss: 2.8173579931259156
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 1.0308, Acc: 0.6156, AUC: 0.7732, F1: 0.3668
[VIT] val - Epoch: 18, Loss: 0.9695, Acc: 0.6906, AUC: 0.8571, F1: 0.5648
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8571
Saving vit model...
epoch: 19, total loss: 3.0054794430732725
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 0.9548, Acc: 0.6344, AUC: 0.7929, F1: 0.4220
[VIT] val - Epoch: 19, Loss: 1.0110, Acc: 0.6594, AUC: 0.8447, F1: 0.5458
epoch: 20, total loss: 2.855667304992676
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.9580, Acc: 0.6344, AUC: 0.7755, F1: 0.4197
[VIT] val - Epoch: 20, Loss: 0.9637, Acc: 0.6750, AUC: 0.8603, F1: 0.5404
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8603
Saving vit model...
epoch: 21, total loss: 2.6970172822475433
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.9203, Acc: 0.6656, AUC: 0.7990, F1: 0.4262
[VIT] val - Epoch: 21, Loss: 0.9604, Acc: 0.6937, AUC: 0.8574, F1: 0.5435
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7990
Saving cnn model...
epoch: 22, total loss: 2.8310752153396606
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.9947, Acc: 0.6125, AUC: 0.7783, F1: 0.3938
[VIT] val - Epoch: 22, Loss: 0.9291, Acc: 0.7000, AUC: 0.8529, F1: 0.5349
epoch: 23, total loss: 2.7688928604125977
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 1.0221, Acc: 0.5844, AUC: 0.8001, F1: 0.3916
[VIT] val - Epoch: 23, Loss: 0.9372, Acc: 0.7063, AUC: 0.8647, F1: 0.5774
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8001
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8647
Saving vit model...
epoch: 24, total loss: 2.6105505406856535
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.9387, Acc: 0.6562, AUC: 0.7989, F1: 0.4117
[VIT] val - Epoch: 24, Loss: 0.9180, Acc: 0.6937, AUC: 0.8669, F1: 0.5260
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8669
Saving vit model...
epoch: 25, total loss: 2.8056663393974306
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.8957, Acc: 0.6562, AUC: 0.8343, F1: 0.5168
[VIT] val - Epoch: 25, Loss: 0.9238, Acc: 0.6813, AUC: 0.8666, F1: 0.5518
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8343
Saving cnn model...
epoch: 26, total loss: 2.741026771068573
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.9285, Acc: 0.6219, AUC: 0.8117, F1: 0.4261
[VIT] val - Epoch: 26, Loss: 0.9234, Acc: 0.6906, AUC: 0.8630, F1: 0.5475
epoch: 27, total loss: 2.6011571526527404
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.9224, Acc: 0.6687, AUC: 0.8048, F1: 0.4947
[VIT] val - Epoch: 27, Loss: 0.9387, Acc: 0.7094, AUC: 0.8595, F1: 0.5972
epoch: 28, total loss: 2.6803359925746917
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.9727, Acc: 0.6219, AUC: 0.7896, F1: 0.4347
[VIT] val - Epoch: 28, Loss: 0.9247, Acc: 0.7156, AUC: 0.8598, F1: 0.6164
epoch: 29, total loss: 2.6287706077098845
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.9636, Acc: 0.6219, AUC: 0.8185, F1: 0.4492
[VIT] val - Epoch: 29, Loss: 0.9087, Acc: 0.7125, AUC: 0.8647, F1: 0.5804
epoch: 30, total loss: 2.4423766493797303
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.9704, Acc: 0.6406, AUC: 0.8022, F1: 0.4647
[VIT] val - Epoch: 30, Loss: 0.9108, Acc: 0.6937, AUC: 0.8623, F1: 0.5471
epoch: 31, total loss: 2.7748912930488587
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.9234, Acc: 0.6094, AUC: 0.8214, F1: 0.4354
[VIT] val - Epoch: 31, Loss: 0.9002, Acc: 0.6906, AUC: 0.8670, F1: 0.5434
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8670
Saving vit model...
epoch: 32, total loss: 2.632999473810196
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.8532, Acc: 0.6750, AUC: 0.8327, F1: 0.5049
[VIT] val - Epoch: 32, Loss: 0.8848, Acc: 0.7031, AUC: 0.8713, F1: 0.5853
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8713
Saving vit model...
epoch: 33, total loss: 2.7333705186843873
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.9554, Acc: 0.6156, AUC: 0.8226, F1: 0.4678
[VIT] val - Epoch: 33, Loss: 0.8912, Acc: 0.7031, AUC: 0.8670, F1: 0.5933
epoch: 34, total loss: 2.5583394825458527
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.8809, Acc: 0.6562, AUC: 0.8145, F1: 0.4579
[VIT] val - Epoch: 34, Loss: 0.8926, Acc: 0.6750, AUC: 0.8680, F1: 0.5367
epoch: 35, total loss: 2.7151552200317384
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.8751, Acc: 0.6781, AUC: 0.8282, F1: 0.4790
[VIT] val - Epoch: 35, Loss: 0.8916, Acc: 0.6875, AUC: 0.8638, F1: 0.5554
epoch: 36, total loss: 2.5950294494628907
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.8378, Acc: 0.6625, AUC: 0.8431, F1: 0.5129
[VIT] val - Epoch: 36, Loss: 0.8745, Acc: 0.7063, AUC: 0.8710, F1: 0.5774
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8431
Saving cnn model...
epoch: 37, total loss: 2.452816718816757
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8940, Acc: 0.6375, AUC: 0.8429, F1: 0.4972
[VIT] val - Epoch: 37, Loss: 0.8989, Acc: 0.6937, AUC: 0.8717, F1: 0.5908
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8717
Saving vit model...
epoch: 38, total loss: 2.7151303708553316
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.8717, Acc: 0.6594, AUC: 0.8331, F1: 0.4646
[VIT] val - Epoch: 38, Loss: 0.8869, Acc: 0.7063, AUC: 0.8596, F1: 0.5835
epoch: 39, total loss: 2.4821173310279847
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.8033, Acc: 0.6875, AUC: 0.8535, F1: 0.5175
[VIT] val - Epoch: 39, Loss: 0.8577, Acc: 0.7219, AUC: 0.8734, F1: 0.5980
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8535
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8734
Saving vit model...
epoch: 40, total loss: 2.4768459141254424
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.8881, Acc: 0.6344, AUC: 0.8439, F1: 0.4853
[VIT] val - Epoch: 40, Loss: 0.8650, Acc: 0.7250, AUC: 0.8713, F1: 0.6100
epoch: 41, total loss: 2.5398131191730497
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.8940, Acc: 0.6594, AUC: 0.8435, F1: 0.5269
[VIT] val - Epoch: 41, Loss: 0.8640, Acc: 0.7188, AUC: 0.8714, F1: 0.6075
epoch: 42, total loss: 2.6082614421844483
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.8270, Acc: 0.6781, AUC: 0.8394, F1: 0.5154
[VIT] val - Epoch: 42, Loss: 0.8600, Acc: 0.6937, AUC: 0.8730, F1: 0.5767
epoch: 43, total loss: 2.4855100870132447
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.8788, Acc: 0.6625, AUC: 0.8455, F1: 0.5554
[VIT] val - Epoch: 43, Loss: 0.8695, Acc: 0.7094, AUC: 0.8651, F1: 0.5884
epoch: 44, total loss: 2.47006556391716
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.9236, Acc: 0.6438, AUC: 0.8313, F1: 0.5212
[VIT] val - Epoch: 44, Loss: 0.8446, Acc: 0.7063, AUC: 0.8729, F1: 0.5747
epoch: 45, total loss: 2.387499678134918
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.8470, Acc: 0.6594, AUC: 0.8573, F1: 0.5157
[VIT] val - Epoch: 45, Loss: 0.8737, Acc: 0.7125, AUC: 0.8794, F1: 0.6151
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8573
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8794
Saving vit model...
epoch: 46, total loss: 2.6206112265586854
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.8249, Acc: 0.6656, AUC: 0.8573, F1: 0.5296
[VIT] val - Epoch: 46, Loss: 0.8563, Acc: 0.7031, AUC: 0.8700, F1: 0.5672
epoch: 47, total loss: 2.5364997744560243
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.8991, Acc: 0.6188, AUC: 0.8514, F1: 0.4935
[VIT] val - Epoch: 47, Loss: 0.8590, Acc: 0.6844, AUC: 0.8749, F1: 0.5614
epoch: 48, total loss: 2.4569435119628906
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.8136, Acc: 0.6906, AUC: 0.8676, F1: 0.5980
[VIT] val - Epoch: 48, Loss: 0.8610, Acc: 0.7125, AUC: 0.8758, F1: 0.6171
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8676
Saving cnn model...
epoch: 49, total loss: 2.433440262079239
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.8063, Acc: 0.6813, AUC: 0.8630, F1: 0.5393
[VIT] val - Epoch: 49, Loss: 0.8306, Acc: 0.7281, AUC: 0.8795, F1: 0.6068
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8795
Saving vit model...
epoch: 50, total loss: 2.6049500942230224
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.8209, Acc: 0.7219, AUC: 0.8564, F1: 0.6260
[VIT] val - Epoch: 50, Loss: 0.8458, Acc: 0.7063, AUC: 0.8755, F1: 0.6048
epoch: 51, total loss: 2.3871367454528807
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.8285, Acc: 0.7000, AUC: 0.8613, F1: 0.5757
[VIT] val - Epoch: 51, Loss: 0.8499, Acc: 0.6906, AUC: 0.8680, F1: 0.5587
epoch: 52, total loss: 2.4759192287921907
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.8102, Acc: 0.7000, AUC: 0.8730, F1: 0.6170
[VIT] val - Epoch: 52, Loss: 0.8490, Acc: 0.6906, AUC: 0.8781, F1: 0.5605
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8730
Saving cnn model...
epoch: 53, total loss: 2.6756043136119843
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.8025, Acc: 0.6813, AUC: 0.8734, F1: 0.5845
[VIT] val - Epoch: 53, Loss: 0.8311, Acc: 0.7000, AUC: 0.8797, F1: 0.5927
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8734
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8797
Saving vit model...
epoch: 54, total loss: 2.350760155916214
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.7898, Acc: 0.6781, AUC: 0.8697, F1: 0.5577
[VIT] val - Epoch: 54, Loss: 0.8285, Acc: 0.7094, AUC: 0.8824, F1: 0.5872
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8824
Saving vit model...
epoch: 55, total loss: 2.3223625302314757
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.8093, Acc: 0.6906, AUC: 0.8622, F1: 0.5758
[VIT] val - Epoch: 55, Loss: 0.8262, Acc: 0.7344, AUC: 0.8820, F1: 0.6381
epoch: 56, total loss: 2.47007697224617
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.8461, Acc: 0.6594, AUC: 0.8588, F1: 0.5471
[VIT] val - Epoch: 56, Loss: 0.8418, Acc: 0.6875, AUC: 0.8715, F1: 0.5907
epoch: 57, total loss: 2.5248178005218507
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.7859, Acc: 0.7000, AUC: 0.8583, F1: 0.5965
[VIT] val - Epoch: 57, Loss: 0.8395, Acc: 0.6844, AUC: 0.8764, F1: 0.5719
epoch: 58, total loss: 2.507131415605545
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.8622, Acc: 0.6594, AUC: 0.8582, F1: 0.5854
[VIT] val - Epoch: 58, Loss: 0.8368, Acc: 0.7000, AUC: 0.8792, F1: 0.5657
epoch: 59, total loss: 2.4856099843978883
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.8228, Acc: 0.6844, AUC: 0.8560, F1: 0.5624
[VIT] val - Epoch: 59, Loss: 0.8430, Acc: 0.7125, AUC: 0.8736, F1: 0.5966
epoch: 60, total loss: 2.4292880594730377
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.8143, Acc: 0.6625, AUC: 0.8686, F1: 0.5704
[VIT] val - Epoch: 60, Loss: 0.8309, Acc: 0.7063, AUC: 0.8765, F1: 0.5811
epoch: 61, total loss: 2.4043217599391937
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.8137, Acc: 0.6719, AUC: 0.8679, F1: 0.5443
[VIT] val - Epoch: 61, Loss: 0.8434, Acc: 0.6937, AUC: 0.8745, F1: 0.5770
epoch: 62, total loss: 2.347111701965332
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.7866, Acc: 0.7031, AUC: 0.8632, F1: 0.5740
[VIT] val - Epoch: 62, Loss: 0.8326, Acc: 0.6969, AUC: 0.8795, F1: 0.5762
epoch: 63, total loss: 2.382165426015854
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.7526, Acc: 0.7000, AUC: 0.8782, F1: 0.5919
[VIT] val - Epoch: 63, Loss: 0.8321, Acc: 0.6906, AUC: 0.8767, F1: 0.5645
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8782
Saving cnn model...
epoch: 64, total loss: 2.4074913024902345
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.8368, Acc: 0.6844, AUC: 0.8570, F1: 0.5901
[VIT] val - Epoch: 64, Loss: 0.8318, Acc: 0.7000, AUC: 0.8730, F1: 0.5818
epoch: 65, total loss: 2.3962861359119416
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.7865, Acc: 0.6875, AUC: 0.8744, F1: 0.5774
[VIT] val - Epoch: 65, Loss: 0.8066, Acc: 0.7281, AUC: 0.8833, F1: 0.6123
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8833
Saving vit model...
epoch: 66, total loss: 2.354840409755707
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.7668, Acc: 0.6875, AUC: 0.8742, F1: 0.6045
[VIT] val - Epoch: 66, Loss: 0.8248, Acc: 0.7000, AUC: 0.8824, F1: 0.5819
epoch: 67, total loss: 2.3925819873809813
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.8239, Acc: 0.6687, AUC: 0.8758, F1: 0.5659
[VIT] val - Epoch: 67, Loss: 0.8073, Acc: 0.7188, AUC: 0.8851, F1: 0.5983
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8851
Saving vit model...
epoch: 68, total loss: 2.380574029684067
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.8429, Acc: 0.6781, AUC: 0.8561, F1: 0.6030
[VIT] val - Epoch: 68, Loss: 0.8194, Acc: 0.6937, AUC: 0.8777, F1: 0.5725
epoch: 69, total loss: 2.200230759382248
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.8392, Acc: 0.6750, AUC: 0.8732, F1: 0.5977
[VIT] val - Epoch: 69, Loss: 0.8179, Acc: 0.7063, AUC: 0.8783, F1: 0.5902
epoch: 70, total loss: 2.2884924829006197
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.8147, Acc: 0.6656, AUC: 0.8718, F1: 0.5640
[VIT] val - Epoch: 70, Loss: 0.8231, Acc: 0.7094, AUC: 0.8837, F1: 0.5867
epoch: 71, total loss: 2.6050826609134674
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.8396, Acc: 0.6750, AUC: 0.8669, F1: 0.5802
[VIT] val - Epoch: 71, Loss: 0.8253, Acc: 0.7063, AUC: 0.8847, F1: 0.6039
epoch: 72, total loss: 2.437712574005127
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.8002, Acc: 0.6937, AUC: 0.8767, F1: 0.6226
[VIT] val - Epoch: 72, Loss: 0.8340, Acc: 0.6969, AUC: 0.8761, F1: 0.5963
epoch: 73, total loss: 2.4964925706386567
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.8033, Acc: 0.6656, AUC: 0.8791, F1: 0.5958
[VIT] val - Epoch: 73, Loss: 0.8313, Acc: 0.6969, AUC: 0.8831, F1: 0.5959
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8791
Saving cnn model...
epoch: 74, total loss: 2.4940616309642794
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.7862, Acc: 0.6906, AUC: 0.8755, F1: 0.6125
[VIT] val - Epoch: 74, Loss: 0.8142, Acc: 0.7125, AUC: 0.8852, F1: 0.5971
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8852
Saving vit model...
epoch: 75, total loss: 2.4615622222423554
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.8444, Acc: 0.6625, AUC: 0.8754, F1: 0.6029
[VIT] val - Epoch: 75, Loss: 0.8202, Acc: 0.7031, AUC: 0.8833, F1: 0.5922
epoch: 76, total loss: 2.3119368314743043
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.8614, Acc: 0.6500, AUC: 0.8510, F1: 0.5603
[VIT] val - Epoch: 76, Loss: 0.8237, Acc: 0.6875, AUC: 0.8844, F1: 0.5723
epoch: 77, total loss: 2.2585858702659607
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.8053, Acc: 0.6781, AUC: 0.8769, F1: 0.6109
[VIT] val - Epoch: 77, Loss: 0.8142, Acc: 0.6969, AUC: 0.8820, F1: 0.5815
epoch: 78, total loss: 2.6828052520751955
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.8994, Acc: 0.6781, AUC: 0.8352, F1: 0.5771
[VIT] val - Epoch: 78, Loss: 0.8156, Acc: 0.6875, AUC: 0.8756, F1: 0.5778
epoch: 79, total loss: 2.6055286645889284
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.7917, Acc: 0.6750, AUC: 0.8810, F1: 0.6021
[VIT] val - Epoch: 79, Loss: 0.8103, Acc: 0.6844, AUC: 0.8821, F1: 0.5702
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8810
Saving cnn model...
epoch: 80, total loss: 2.372667872905731
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.7760, Acc: 0.6656, AUC: 0.8886, F1: 0.5905
[VIT] val - Epoch: 80, Loss: 0.8068, Acc: 0.6906, AUC: 0.8858, F1: 0.5849
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8886
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8858
Saving vit model...
epoch: 81, total loss: 2.2860036551952363
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.7672, Acc: 0.6937, AUC: 0.8805, F1: 0.6206
[VIT] val - Epoch: 81, Loss: 0.8026, Acc: 0.7094, AUC: 0.8890, F1: 0.6075
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8890
Saving vit model...
epoch: 82, total loss: 2.3589780390262605
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.7824, Acc: 0.6906, AUC: 0.8809, F1: 0.6127
[VIT] val - Epoch: 82, Loss: 0.8109, Acc: 0.7063, AUC: 0.8871, F1: 0.6009
epoch: 83, total loss: 2.4329668641090394
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.7662, Acc: 0.6875, AUC: 0.8828, F1: 0.6053
[VIT] val - Epoch: 83, Loss: 0.8132, Acc: 0.6937, AUC: 0.8866, F1: 0.5706
epoch: 84, total loss: 2.3155424118041994
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.7412, Acc: 0.6969, AUC: 0.8964, F1: 0.6197
[VIT] val - Epoch: 84, Loss: 0.8069, Acc: 0.7063, AUC: 0.8857, F1: 0.5880
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8964
Saving cnn model...
epoch: 85, total loss: 2.2779467701911926
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.7900, Acc: 0.6813, AUC: 0.8750, F1: 0.5969
[VIT] val - Epoch: 85, Loss: 0.8074, Acc: 0.6969, AUC: 0.8874, F1: 0.6137
epoch: 86, total loss: 2.5132554709911346
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.7539, Acc: 0.6969, AUC: 0.8957, F1: 0.6371
[VIT] val - Epoch: 86, Loss: 0.8084, Acc: 0.6906, AUC: 0.8902, F1: 0.5878
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8902
Saving vit model...
epoch: 87, total loss: 2.29446702003479
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.7694, Acc: 0.6750, AUC: 0.8937, F1: 0.6005
[VIT] val - Epoch: 87, Loss: 0.8168, Acc: 0.6844, AUC: 0.8869, F1: 0.5769
epoch: 88, total loss: 2.2118405282497404
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.7447, Acc: 0.6875, AUC: 0.9042, F1: 0.6158
[VIT] val - Epoch: 88, Loss: 0.8169, Acc: 0.6875, AUC: 0.8868, F1: 0.5772
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9042
Saving cnn model...
epoch: 89, total loss: 2.3856815338134765
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.7837, Acc: 0.6781, AUC: 0.8901, F1: 0.5945
[VIT] val - Epoch: 89, Loss: 0.8099, Acc: 0.6969, AUC: 0.8848, F1: 0.5895
epoch: 90, total loss: 2.185190737247467
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.7648, Acc: 0.6844, AUC: 0.8789, F1: 0.6002
[VIT] val - Epoch: 90, Loss: 0.8111, Acc: 0.7156, AUC: 0.8900, F1: 0.6157
epoch: 91, total loss: 2.237429529428482
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.7659, Acc: 0.6875, AUC: 0.8992, F1: 0.6384
[VIT] val - Epoch: 91, Loss: 0.7968, Acc: 0.7344, AUC: 0.8898, F1: 0.6176
epoch: 92, total loss: 2.2890343844890593
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.7286, Acc: 0.7188, AUC: 0.8982, F1: 0.6416
[VIT] val - Epoch: 92, Loss: 0.7997, Acc: 0.7063, AUC: 0.8874, F1: 0.6015
epoch: 93, total loss: 2.4247912585735323
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.7422, Acc: 0.6906, AUC: 0.9058, F1: 0.6211
[VIT] val - Epoch: 93, Loss: 0.8093, Acc: 0.6844, AUC: 0.8878, F1: 0.5829
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9058
Saving cnn model...
epoch: 94, total loss: 2.2632860839366913
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.7503, Acc: 0.6937, AUC: 0.8899, F1: 0.6175
[VIT] val - Epoch: 94, Loss: 0.8172, Acc: 0.6813, AUC: 0.8884, F1: 0.5900
epoch: 95, total loss: 2.2603311061859133
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.7029, Acc: 0.7063, AUC: 0.9091, F1: 0.6463
[VIT] val - Epoch: 95, Loss: 0.8035, Acc: 0.7000, AUC: 0.8869, F1: 0.5901
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9091
Saving cnn model...
epoch: 96, total loss: 2.2674457848072054
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.7287, Acc: 0.6906, AUC: 0.9064, F1: 0.6148
[VIT] val - Epoch: 96, Loss: 0.8053, Acc: 0.7000, AUC: 0.8859, F1: 0.5880
epoch: 97, total loss: 2.464644706249237
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.7256, Acc: 0.6875, AUC: 0.9116, F1: 0.6284
[VIT] val - Epoch: 97, Loss: 0.8013, Acc: 0.6844, AUC: 0.8896, F1: 0.5808
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.9116
Saving cnn model...
epoch: 98, total loss: 2.1626284778118134
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.7394, Acc: 0.6969, AUC: 0.9109, F1: 0.6261
[VIT] val - Epoch: 98, Loss: 0.8109, Acc: 0.6813, AUC: 0.8871, F1: 0.5862
epoch: 99, total loss: 2.3744913935661316
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.7245, Acc: 0.7312, AUC: 0.9036, F1: 0.6558
[VIT] val - Epoch: 99, Loss: 0.8064, Acc: 0.6906, AUC: 0.8865, F1: 0.5785
epoch: 100, total loss: 2.305830478668213
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.7670, Acc: 0.7063, AUC: 0.9020, F1: 0.6489
[VIT] val - Epoch: 100, Loss: 0.8133, Acc: 0.7000, AUC: 0.8826, F1: 0.5984
[07:15:57][Rank 2] Training Finished. Starting Final Testing...
[07:15:57][Rank 3] Training Finished. Starting Final Testing...
[07:15:57][Rank 1] Training Finished. Starting Final Testing...
[07:15:57][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.2500, Acc: 0.3036, AUC: 0.8056, F1: 0.3276
[VIT] test - Epoch: 100, Loss: 1.2691, Acc: 0.4245, AUC: 0.7607, F1: 0.3465
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.1625, Acc: 0.3561, AUC: 0.7965, F1: 0.3316
[VIT] test - Epoch: 100, Loss: 1.2921, Acc: 0.3982, AUC: 0.7604, F1: 0.3397
‚úÖ [ÂÆåÊàê] Ê∫êÂüü DEEPDR ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: FGADR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='FGADR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['FGADR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_FGADR
OUT_DIR: ./output_esdg_h100/FGADR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified2/output_esdg_h100/FGADR/CASS_GDRNet_ESDG_FGADR
[07:23:08][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
[07:23:09][Rank 3] Loading datasets...
[07:23:09][Rank 2] Loading datasets...
================ [Auto Config] ================
Source: ['FGADR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'IDRID', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/FGADR
===============================================
[07:23:09][Rank 1] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_projInjecting LoRA into: layer.0.attention.v_proj

üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.q_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_projInjecting LoRA into: layer.1.attention.k_proj

Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_projInjecting LoRA into: layer.1.attention.v_proj

Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.o_projInjecting LoRA into: layer.2.attention.o_proj

Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_projInjecting LoRA into: layer.5.mlp.up_proj

Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_projInjecting LoRA into: layer.6.attention.o_proj

Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_projInjecting LoRA into: layer.9.mlp.up_proj

Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.attention.q_projInjecting LoRA into: layer.11.attention.k_proj

Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 3.845257600148519
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 3.5103, Acc: 0.4239, AUC: 0.6422, F1: 0.2689
[VIT] val - Epoch: 1, Loss: 1.3843, Acc: 0.5054, AUC: 0.5706, F1: 0.2400
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6422
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5706
Saving vit model...
epoch: 2, total loss: 3.4097959796587625
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 3.6658, Acc: 0.3696, AUC: 0.6703, F1: 0.2729
[VIT] val - Epoch: 2, Loss: 1.2991, Acc: 0.5571, AUC: 0.7107, F1: 0.2860
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6703
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7107
Saving vit model...
epoch: 3, total loss: 3.18851305047671
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 1.0623, Acc: 0.5516, AUC: 0.7996, F1: 0.3729
[VIT] val - Epoch: 3, Loss: 1.2408, Acc: 0.6087, AUC: 0.7743, F1: 0.3897
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7996
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7743
Saving vit model...
epoch: 4, total loss: 3.378675878047943
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 1.1707, Acc: 0.4701, AUC: 0.7695, F1: 0.3328
[VIT] val - Epoch: 4, Loss: 1.2163, Acc: 0.6223, AUC: 0.7691, F1: 0.3518
epoch: 5, total loss: 2.9698241849740348
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.1148, Acc: 0.5136, AUC: 0.7822, F1: 0.3353
[VIT] val - Epoch: 5, Loss: 1.1640, Acc: 0.6576, AUC: 0.7747, F1: 0.3525
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7747
Saving vit model...
epoch: 6, total loss: 2.642569233973821
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.1874, Acc: 0.5299, AUC: 0.7649, F1: 0.3160
[VIT] val - Epoch: 6, Loss: 1.1808, Acc: 0.6141, AUC: 0.7688, F1: 0.4189
epoch: 7, total loss: 2.6992686788241067
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.2788, Acc: 0.4728, AUC: 0.7570, F1: 0.3335
[VIT] val - Epoch: 7, Loss: 1.1370, Acc: 0.5951, AUC: 0.7754, F1: 0.3792
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7754
Saving vit model...
epoch: 8, total loss: 3.09503964583079
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 0.9821, Acc: 0.5815, AUC: 0.8090, F1: 0.3708
[VIT] val - Epoch: 8, Loss: 1.0734, Acc: 0.6603, AUC: 0.8264, F1: 0.3956
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8090
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8264
Saving vit model...
epoch: 9, total loss: 2.709745466709137
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 1.0261, Acc: 0.5462, AUC: 0.7943, F1: 0.4148
[VIT] val - Epoch: 9, Loss: 1.0820, Acc: 0.6685, AUC: 0.7957, F1: 0.4456
epoch: 10, total loss: 2.7205952207247415
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 0.9749, Acc: 0.5625, AUC: 0.8087, F1: 0.3663
[VIT] val - Epoch: 10, Loss: 1.0181, Acc: 0.6658, AUC: 0.8302, F1: 0.4127
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8302
Saving vit model...
epoch: 11, total loss: 2.817788541316986
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 1.0182, Acc: 0.5924, AUC: 0.8084, F1: 0.3846
[VIT] val - Epoch: 11, Loss: 1.0245, Acc: 0.6902, AUC: 0.8285, F1: 0.4412
epoch: 12, total loss: 2.785240868727366
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.0068, Acc: 0.5652, AUC: 0.8030, F1: 0.3720
[VIT] val - Epoch: 12, Loss: 1.0278, Acc: 0.6522, AUC: 0.8123, F1: 0.4043
epoch: 13, total loss: 2.6558868885040283
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 0.9430, Acc: 0.6005, AUC: 0.8250, F1: 0.4196
[VIT] val - Epoch: 13, Loss: 0.9926, Acc: 0.6984, AUC: 0.8363, F1: 0.4568
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8250
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8363
Saving vit model...
epoch: 14, total loss: 2.775617609421412
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 0.9966, Acc: 0.5951, AUC: 0.8098, F1: 0.3399
[VIT] val - Epoch: 14, Loss: 1.0180, Acc: 0.6793, AUC: 0.8025, F1: 0.4567
epoch: 15, total loss: 2.641373192270597
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 0.9622, Acc: 0.6168, AUC: 0.8170, F1: 0.4002
[VIT] val - Epoch: 15, Loss: 1.0072, Acc: 0.6712, AUC: 0.8380, F1: 0.4113
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8380
Saving vit model...
epoch: 16, total loss: 2.595896561940511
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 0.8753, Acc: 0.5978, AUC: 0.8294, F1: 0.3817
[VIT] val - Epoch: 16, Loss: 0.9683, Acc: 0.6848, AUC: 0.8318, F1: 0.4531
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8294
Saving cnn model...
epoch: 17, total loss: 2.479313184817632
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 0.9290, Acc: 0.5734, AUC: 0.8185, F1: 0.3847
[VIT] val - Epoch: 17, Loss: 1.0138, Acc: 0.6386, AUC: 0.8225, F1: 0.4593
epoch: 18, total loss: 2.598069275418917
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 1.1585, Acc: 0.5245, AUC: 0.7998, F1: 0.3579
[VIT] val - Epoch: 18, Loss: 0.9717, Acc: 0.6576, AUC: 0.8299, F1: 0.4629
epoch: 19, total loss: 2.648157387971878
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.0420, Acc: 0.5489, AUC: 0.8147, F1: 0.4040
[VIT] val - Epoch: 19, Loss: 0.9852, Acc: 0.6658, AUC: 0.8254, F1: 0.4633
epoch: 20, total loss: 2.5724546909332275
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 0.9015, Acc: 0.6250, AUC: 0.8261, F1: 0.4333
[VIT] val - Epoch: 20, Loss: 0.9475, Acc: 0.7038, AUC: 0.8385, F1: 0.4652
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8385
Saving vit model...
epoch: 21, total loss: 2.4990697701772056
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 0.9187, Acc: 0.6386, AUC: 0.8163, F1: 0.3917
[VIT] val - Epoch: 21, Loss: 0.9659, Acc: 0.6712, AUC: 0.8374, F1: 0.4485
epoch: 22, total loss: 2.4176044364770255
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 0.9184, Acc: 0.6196, AUC: 0.8251, F1: 0.4121
[VIT] val - Epoch: 22, Loss: 0.9616, Acc: 0.6576, AUC: 0.8400, F1: 0.4759
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8400
Saving vit model...
epoch: 23, total loss: 2.4707337468862534
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 0.9298, Acc: 0.6087, AUC: 0.8314, F1: 0.4364
[VIT] val - Epoch: 23, Loss: 0.9416, Acc: 0.6848, AUC: 0.8362, F1: 0.4565
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8314
Saving cnn model...
epoch: 24, total loss: 3.095098912715912
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 0.8693, Acc: 0.6087, AUC: 0.8425, F1: 0.4560
[VIT] val - Epoch: 24, Loss: 0.9212, Acc: 0.7201, AUC: 0.8576, F1: 0.5151
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8425
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8576
Saving vit model...
epoch: 25, total loss: 2.6370617796977363
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 0.9048, Acc: 0.6196, AUC: 0.8354, F1: 0.4589
[VIT] val - Epoch: 25, Loss: 0.9594, Acc: 0.6630, AUC: 0.8283, F1: 0.4897
epoch: 26, total loss: 2.320290187994639
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 0.8733, Acc: 0.5897, AUC: 0.8354, F1: 0.4432
[VIT] val - Epoch: 26, Loss: 0.9301, Acc: 0.6984, AUC: 0.8478, F1: 0.4919
epoch: 27, total loss: 2.463707357645035
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 0.9111, Acc: 0.5951, AUC: 0.8317, F1: 0.4342
[VIT] val - Epoch: 27, Loss: 0.9532, Acc: 0.6440, AUC: 0.8518, F1: 0.4534
epoch: 28, total loss: 2.434870888789495
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 0.9930, Acc: 0.5625, AUC: 0.8276, F1: 0.4362
[VIT] val - Epoch: 28, Loss: 0.9223, Acc: 0.6766, AUC: 0.8581, F1: 0.4648
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8581
Saving vit model...
epoch: 29, total loss: 2.5086295952399573
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.8977, Acc: 0.6196, AUC: 0.8333, F1: 0.4301
[VIT] val - Epoch: 29, Loss: 0.9289, Acc: 0.6902, AUC: 0.8410, F1: 0.4935
epoch: 30, total loss: 2.4039874176184335
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.9182, Acc: 0.6250, AUC: 0.8372, F1: 0.4703
[VIT] val - Epoch: 30, Loss: 0.9285, Acc: 0.6821, AUC: 0.8460, F1: 0.5080
epoch: 31, total loss: 2.619178613026937
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.8704, Acc: 0.6386, AUC: 0.8529, F1: 0.4517
[VIT] val - Epoch: 31, Loss: 0.9064, Acc: 0.6902, AUC: 0.8507, F1: 0.4938
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8529
Saving cnn model...
epoch: 32, total loss: 2.38776067396005
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 0.8999, Acc: 0.6087, AUC: 0.8307, F1: 0.4535
[VIT] val - Epoch: 32, Loss: 0.9046, Acc: 0.7092, AUC: 0.8453, F1: 0.4973
epoch: 33, total loss: 2.4423501044511795
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 0.8602, Acc: 0.6495, AUC: 0.8399, F1: 0.4553
[VIT] val - Epoch: 33, Loss: 0.8945, Acc: 0.6848, AUC: 0.8574, F1: 0.4635
epoch: 34, total loss: 2.7442909826835
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.8702, Acc: 0.6413, AUC: 0.8376, F1: 0.4179
[VIT] val - Epoch: 34, Loss: 0.8813, Acc: 0.6957, AUC: 0.8576, F1: 0.4902
epoch: 35, total loss: 2.4642764379580817
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.8984, Acc: 0.6250, AUC: 0.8324, F1: 0.4612
[VIT] val - Epoch: 35, Loss: 0.9159, Acc: 0.6848, AUC: 0.8416, F1: 0.5154
epoch: 36, total loss: 2.43793061375618
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.8467, Acc: 0.6522, AUC: 0.8506, F1: 0.4794
[VIT] val - Epoch: 36, Loss: 0.8933, Acc: 0.6848, AUC: 0.8513, F1: 0.4605
epoch: 37, total loss: 2.300759514172872
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 0.8755, Acc: 0.6114, AUC: 0.8410, F1: 0.4456
[VIT] val - Epoch: 37, Loss: 0.9272, Acc: 0.6603, AUC: 0.8363, F1: 0.5074
epoch: 38, total loss: 2.4802294423182807
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 0.8900, Acc: 0.6141, AUC: 0.8466, F1: 0.4552
[VIT] val - Epoch: 38, Loss: 0.9082, Acc: 0.6576, AUC: 0.8473, F1: 0.4690
epoch: 39, total loss: 2.363081475098928
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 0.9205, Acc: 0.5897, AUC: 0.8374, F1: 0.4527
[VIT] val - Epoch: 39, Loss: 0.9080, Acc: 0.6712, AUC: 0.8537, F1: 0.4900
epoch: 40, total loss: 2.4908756564060845
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 0.8273, Acc: 0.6576, AUC: 0.8531, F1: 0.4457
[VIT] val - Epoch: 40, Loss: 0.8816, Acc: 0.7065, AUC: 0.8503, F1: 0.5206
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8531
Saving cnn model...
epoch: 41, total loss: 2.3113548010587692
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.9361, Acc: 0.6250, AUC: 0.8342, F1: 0.4766
[VIT] val - Epoch: 41, Loss: 0.9122, Acc: 0.6685, AUC: 0.8522, F1: 0.4884
epoch: 42, total loss: 2.4485979278882346
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 0.8559, Acc: 0.5978, AUC: 0.8423, F1: 0.4708
[VIT] val - Epoch: 42, Loss: 0.9004, Acc: 0.6821, AUC: 0.8470, F1: 0.4901
epoch: 43, total loss: 2.34061199426651
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.8264, Acc: 0.6576, AUC: 0.8629, F1: 0.5164
[VIT] val - Epoch: 43, Loss: 0.8903, Acc: 0.6929, AUC: 0.8557, F1: 0.5069
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8629
Saving cnn model...
epoch: 44, total loss: 2.34579831858476
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.8415, Acc: 0.6359, AUC: 0.8609, F1: 0.5090
[VIT] val - Epoch: 44, Loss: 0.9097, Acc: 0.6630, AUC: 0.8337, F1: 0.4903
epoch: 45, total loss: 2.613565052549044
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.8462, Acc: 0.6386, AUC: 0.8478, F1: 0.4552
[VIT] val - Epoch: 45, Loss: 0.8857, Acc: 0.6712, AUC: 0.8506, F1: 0.4743
epoch: 46, total loss: 2.435684541861216
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.9011, Acc: 0.6359, AUC: 0.8305, F1: 0.4691
[VIT] val - Epoch: 46, Loss: 0.9007, Acc: 0.6739, AUC: 0.8364, F1: 0.5304
epoch: 47, total loss: 2.389392167329788
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 0.8200, Acc: 0.6576, AUC: 0.8580, F1: 0.4910
[VIT] val - Epoch: 47, Loss: 0.8654, Acc: 0.7011, AUC: 0.8592, F1: 0.4695
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8592
Saving vit model...
epoch: 48, total loss: 2.2647819916407266
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.9110, Acc: 0.5761, AUC: 0.8447, F1: 0.4712
[VIT] val - Epoch: 48, Loss: 0.9080, Acc: 0.6630, AUC: 0.8432, F1: 0.5102
epoch: 49, total loss: 2.3450870364904404
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.8509, Acc: 0.6359, AUC: 0.8548, F1: 0.4687
[VIT] val - Epoch: 49, Loss: 0.8939, Acc: 0.6739, AUC: 0.8468, F1: 0.5064
epoch: 50, total loss: 2.2497159987688065
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.8851, Acc: 0.6168, AUC: 0.8437, F1: 0.4662
[VIT] val - Epoch: 50, Loss: 0.8894, Acc: 0.6821, AUC: 0.8574, F1: 0.4906
epoch: 51, total loss: 2.22784457107385
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.8553, Acc: 0.6304, AUC: 0.8593, F1: 0.4707
[VIT] val - Epoch: 51, Loss: 0.8864, Acc: 0.6902, AUC: 0.8534, F1: 0.5060
epoch: 52, total loss: 2.4731165021657944
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.8640, Acc: 0.6630, AUC: 0.8450, F1: 0.4684
[VIT] val - Epoch: 52, Loss: 0.8811, Acc: 0.6875, AUC: 0.8562, F1: 0.5144
epoch: 53, total loss: 2.282965530951818
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.8306, Acc: 0.6522, AUC: 0.8621, F1: 0.4867
[VIT] val - Epoch: 53, Loss: 0.8710, Acc: 0.7120, AUC: 0.8577, F1: 0.5244
epoch: 54, total loss: 2.4488030821084976
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.8680, Acc: 0.6277, AUC: 0.8562, F1: 0.5155
[VIT] val - Epoch: 54, Loss: 0.8647, Acc: 0.6984, AUC: 0.8629, F1: 0.4919
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8629
Saving vit model...
epoch: 55, total loss: 2.3683301707108817
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.8086, Acc: 0.6522, AUC: 0.8610, F1: 0.4970
[VIT] val - Epoch: 55, Loss: 0.8815, Acc: 0.6848, AUC: 0.8605, F1: 0.5238
epoch: 56, total loss: 2.334921653072039
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.8863, Acc: 0.6277, AUC: 0.8510, F1: 0.4470
[VIT] val - Epoch: 56, Loss: 0.8711, Acc: 0.6984, AUC: 0.8588, F1: 0.5152
epoch: 57, total loss: 2.32120111087958
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.9049, Acc: 0.6250, AUC: 0.8431, F1: 0.4950
[VIT] val - Epoch: 57, Loss: 0.8864, Acc: 0.6739, AUC: 0.8506, F1: 0.5108
epoch: 58, total loss: 2.2325502087672553
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.9743, Acc: 0.5842, AUC: 0.8225, F1: 0.4414
[VIT] val - Epoch: 58, Loss: 0.8819, Acc: 0.6902, AUC: 0.8556, F1: 0.5197
epoch: 59, total loss: 2.2083070228497186
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.8422, Acc: 0.6413, AUC: 0.8578, F1: 0.4842
[VIT] val - Epoch: 59, Loss: 0.8876, Acc: 0.6848, AUC: 0.8493, F1: 0.5239
epoch: 60, total loss: 2.327599768837293
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.8432, Acc: 0.6467, AUC: 0.8602, F1: 0.5071
[VIT] val - Epoch: 60, Loss: 0.8607, Acc: 0.7038, AUC: 0.8618, F1: 0.5221
epoch: 61, total loss: 2.565446545680364
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.8241, Acc: 0.6522, AUC: 0.8694, F1: 0.5011
[VIT] val - Epoch: 61, Loss: 0.8763, Acc: 0.6685, AUC: 0.8590, F1: 0.4999
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8694
Saving cnn model...
epoch: 62, total loss: 2.056671053171158
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.8681, Acc: 0.6467, AUC: 0.8507, F1: 0.5137
[VIT] val - Epoch: 62, Loss: 0.8812, Acc: 0.6848, AUC: 0.8523, F1: 0.5018
epoch: 63, total loss: 2.400429278612137
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.8638, Acc: 0.6440, AUC: 0.8495, F1: 0.4566
[VIT] val - Epoch: 63, Loss: 0.8757, Acc: 0.6848, AUC: 0.8514, F1: 0.5140
epoch: 64, total loss: 2.26176126797994
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.8671, Acc: 0.6440, AUC: 0.8577, F1: 0.5116
[VIT] val - Epoch: 64, Loss: 0.8849, Acc: 0.6848, AUC: 0.8465, F1: 0.5088
epoch: 65, total loss: 2.4454817473888397
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.8624, Acc: 0.6277, AUC: 0.8613, F1: 0.4967
[VIT] val - Epoch: 65, Loss: 0.8694, Acc: 0.6821, AUC: 0.8556, F1: 0.5053
epoch: 66, total loss: 2.2769329994916916
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.8594, Acc: 0.6168, AUC: 0.8504, F1: 0.4723
[VIT] val - Epoch: 66, Loss: 0.8762, Acc: 0.6603, AUC: 0.8494, F1: 0.4935
epoch: 67, total loss: 2.109833707412084
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.8599, Acc: 0.6304, AUC: 0.8495, F1: 0.4877
[VIT] val - Epoch: 67, Loss: 0.8682, Acc: 0.6766, AUC: 0.8464, F1: 0.4809
epoch: 68, total loss: 2.2022857517004013
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.8412, Acc: 0.6630, AUC: 0.8596, F1: 0.5176
[VIT] val - Epoch: 68, Loss: 0.8629, Acc: 0.6848, AUC: 0.8624, F1: 0.4799
epoch: 69, total loss: 2.245162233710289
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.7997, Acc: 0.6766, AUC: 0.8642, F1: 0.5434
[VIT] val - Epoch: 69, Loss: 0.8654, Acc: 0.6793, AUC: 0.8586, F1: 0.5031
epoch: 70, total loss: 2.252442345023155
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.8495, Acc: 0.6576, AUC: 0.8655, F1: 0.5421
[VIT] val - Epoch: 70, Loss: 0.8825, Acc: 0.6658, AUC: 0.8544, F1: 0.4890
epoch: 71, total loss: 2.243014474709829
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.8287, Acc: 0.6549, AUC: 0.8640, F1: 0.5262
[VIT] val - Epoch: 71, Loss: 0.8615, Acc: 0.6984, AUC: 0.8582, F1: 0.5215
epoch: 72, total loss: 2.3385378569364548
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.8516, Acc: 0.6658, AUC: 0.8635, F1: 0.5078
[VIT] val - Epoch: 72, Loss: 0.8535, Acc: 0.6902, AUC: 0.8617, F1: 0.4932
epoch: 73, total loss: 2.0897182623545327
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.8455, Acc: 0.6141, AUC: 0.8638, F1: 0.5107
[VIT] val - Epoch: 73, Loss: 0.8517, Acc: 0.7038, AUC: 0.8608, F1: 0.5155
epoch: 74, total loss: 2.1968891074260077
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.8092, Acc: 0.6821, AUC: 0.8663, F1: 0.5152
[VIT] val - Epoch: 74, Loss: 0.8638, Acc: 0.6848, AUC: 0.8594, F1: 0.5191
epoch: 75, total loss: 2.401597782969475
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.8334, Acc: 0.6658, AUC: 0.8612, F1: 0.4941
[VIT] val - Epoch: 75, Loss: 0.8542, Acc: 0.6902, AUC: 0.8568, F1: 0.5057
epoch: 76, total loss: 2.3292542894681296
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.8248, Acc: 0.6685, AUC: 0.8669, F1: 0.5262
[VIT] val - Epoch: 76, Loss: 0.8629, Acc: 0.6984, AUC: 0.8576, F1: 0.5229
epoch: 77, total loss: 2.265885834892591
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.8162, Acc: 0.6712, AUC: 0.8699, F1: 0.5266
[VIT] val - Epoch: 77, Loss: 0.8520, Acc: 0.6957, AUC: 0.8663, F1: 0.5225
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8699
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8663
Saving vit model...
epoch: 78, total loss: 2.0503724763790765
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.8391, Acc: 0.6603, AUC: 0.8651, F1: 0.5409
[VIT] val - Epoch: 78, Loss: 0.8620, Acc: 0.6793, AUC: 0.8583, F1: 0.5110
epoch: 79, total loss: 2.2519804338614144
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.8804, Acc: 0.6440, AUC: 0.8588, F1: 0.5325
[VIT] val - Epoch: 79, Loss: 0.8513, Acc: 0.6875, AUC: 0.8603, F1: 0.5110
epoch: 80, total loss: 2.2914845844109855
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.8310, Acc: 0.6495, AUC: 0.8631, F1: 0.5090
[VIT] val - Epoch: 80, Loss: 0.8599, Acc: 0.6766, AUC: 0.8606, F1: 0.4963
epoch: 81, total loss: 2.1661433428525925
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.8547, Acc: 0.6467, AUC: 0.8690, F1: 0.5166
[VIT] val - Epoch: 81, Loss: 0.8494, Acc: 0.6875, AUC: 0.8629, F1: 0.5051
epoch: 82, total loss: 2.0006420264641442
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.8606, Acc: 0.6440, AUC: 0.8601, F1: 0.5192
[VIT] val - Epoch: 82, Loss: 0.8597, Acc: 0.6848, AUC: 0.8614, F1: 0.5017
epoch: 83, total loss: 2.2142981688181558
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.8486, Acc: 0.6603, AUC: 0.8602, F1: 0.5424
[VIT] val - Epoch: 83, Loss: 0.8628, Acc: 0.6902, AUC: 0.8551, F1: 0.5193
epoch: 84, total loss: 2.203570236762365
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.8002, Acc: 0.6467, AUC: 0.8726, F1: 0.5207
[VIT] val - Epoch: 84, Loss: 0.8672, Acc: 0.6766, AUC: 0.8576, F1: 0.5219
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8726
Saving cnn model...
epoch: 85, total loss: 2.18935489654541
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.8137, Acc: 0.6630, AUC: 0.8627, F1: 0.5302
[VIT] val - Epoch: 85, Loss: 0.8670, Acc: 0.6603, AUC: 0.8549, F1: 0.5011
epoch: 86, total loss: 2.053227831919988
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.8173, Acc: 0.6766, AUC: 0.8678, F1: 0.5484
[VIT] val - Epoch: 86, Loss: 0.8576, Acc: 0.6875, AUC: 0.8581, F1: 0.5170
epoch: 87, total loss: 2.313528448343277
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.8244, Acc: 0.6033, AUC: 0.8658, F1: 0.4979
[VIT] val - Epoch: 87, Loss: 0.8536, Acc: 0.6902, AUC: 0.8645, F1: 0.5265
epoch: 88, total loss: 2.1275361329317093
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.8386, Acc: 0.6658, AUC: 0.8643, F1: 0.5192
[VIT] val - Epoch: 88, Loss: 0.8636, Acc: 0.6739, AUC: 0.8579, F1: 0.5031
epoch: 89, total loss: 2.226715808113416
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.8375, Acc: 0.6522, AUC: 0.8604, F1: 0.5237
[VIT] val - Epoch: 89, Loss: 0.8710, Acc: 0.6739, AUC: 0.8545, F1: 0.5148
epoch: 90, total loss: 2.358615646759669
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.8369, Acc: 0.6603, AUC: 0.8672, F1: 0.5553
[VIT] val - Epoch: 90, Loss: 0.8565, Acc: 0.6902, AUC: 0.8613, F1: 0.5296
epoch: 91, total loss: 2.165770798921585
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.8127, Acc: 0.6848, AUC: 0.8648, F1: 0.5605
[VIT] val - Epoch: 91, Loss: 0.8633, Acc: 0.6902, AUC: 0.8564, F1: 0.5228
epoch: 92, total loss: 2.1287960931658745
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.8418, Acc: 0.6712, AUC: 0.8636, F1: 0.5173
[VIT] val - Epoch: 92, Loss: 0.8549, Acc: 0.6902, AUC: 0.8610, F1: 0.5309
epoch: 93, total loss: 2.1253447184960046
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.8081, Acc: 0.6848, AUC: 0.8679, F1: 0.5250
[VIT] val - Epoch: 93, Loss: 0.8567, Acc: 0.6821, AUC: 0.8549, F1: 0.4881
epoch: 94, total loss: 2.308348918954531
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.8179, Acc: 0.6712, AUC: 0.8732, F1: 0.5482
[VIT] val - Epoch: 94, Loss: 0.8583, Acc: 0.6848, AUC: 0.8619, F1: 0.5267
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8732
Saving cnn model...
epoch: 95, total loss: 2.013335863749186
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.8101, Acc: 0.6685, AUC: 0.8691, F1: 0.5405
[VIT] val - Epoch: 95, Loss: 0.8572, Acc: 0.6875, AUC: 0.8596, F1: 0.5187
epoch: 96, total loss: 2.342994287610054
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.7990, Acc: 0.6821, AUC: 0.8671, F1: 0.5385
[VIT] val - Epoch: 96, Loss: 0.8447, Acc: 0.6984, AUC: 0.8651, F1: 0.5414
epoch: 97, total loss: 2.0504376689592996
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.7951, Acc: 0.6821, AUC: 0.8801, F1: 0.5587
[VIT] val - Epoch: 97, Loss: 0.8562, Acc: 0.6848, AUC: 0.8632, F1: 0.5324
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8801
Saving cnn model...
epoch: 98, total loss: 2.0116902515292168
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.8354, Acc: 0.6359, AUC: 0.8690, F1: 0.5023
[VIT] val - Epoch: 98, Loss: 0.8552, Acc: 0.7011, AUC: 0.8579, F1: 0.5438
epoch: 99, total loss: 2.211340313156446
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.7904, Acc: 0.6821, AUC: 0.8760, F1: 0.5507
[VIT] val - Epoch: 99, Loss: 0.8428, Acc: 0.6957, AUC: 0.8583, F1: 0.5295
epoch: 100, total loss: 2.0712806383768716
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.8155, Acc: 0.6793, AUC: 0.8782, F1: 0.5563
[VIT] val - Epoch: 100, Loss: 0.8459, Acc: 0.6902, AUC: 0.8640, F1: 0.5150
[07:42:22][Rank 0] Training Finished. Starting Final Testing...[07:42:22][Rank 1] Training Finished. Starting Final Testing...

[07:42:22][Rank 3] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
[07:42:22][Rank 2] Training Finished. Starting Final Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 3.1877, Acc: 0.0839, AUC: 0.6914, F1: 0.0923
[VIT] test - Epoch: 100, Loss: 2.0570, Acc: 0.1441, AUC: 0.7174, F1: 0.1184
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 2.8423, Acc: 0.2072, AUC: 0.7171, F1: 0.1706
[VIT] test - Epoch: 100, Loss: 1.8908, Acc: 0.2017, AUC: 0.7202, F1: 0.1337
‚úÖ [ÂÆåÊàê] Ê∫êÂüü FGADR ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: IDRID
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='IDRID', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['IDRID']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_IDRID
OUT_DIR: ./output_esdg_h100/IDRID
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified2/output_esdg_h100/IDRID/CASS_GDRNet_ESDG_IDRID
[07:49:33][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
[07:49:33][Rank 2] Loading datasets...
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
[07:49:33][Rank 3] Loading datasets...
================ [Auto Config] ================
Source: ['IDRID']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'MESSIDOR', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/IDRID
===============================================
[07:49:33][Rank 1] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 4.2553079809461325
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 407.6776, Acc: 0.2039, AUC: 0.4815, F1: 0.1552
[VIT] val - Epoch: 1, Loss: 1.5475, Acc: 0.2913, AUC: 0.5460, F1: 0.1367
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.4815
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5460
Saving vit model...
epoch: 2, total loss: 3.8958101953778947
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 6.7429, Acc: 0.2039, AUC: 0.4612, F1: 0.1552
[VIT] val - Epoch: 2, Loss: 1.4642, Acc: 0.3398, AUC: 0.5541, F1: 0.1540
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5541
Saving vit model...
epoch: 3, total loss: 3.7952373708997453
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 3.0220, Acc: 0.3592, AUC: 0.5201, F1: 0.2047
[VIT] val - Epoch: 3, Loss: 1.3995, Acc: 0.4369, AUC: 0.6522, F1: 0.2578
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5201
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6522
Saving vit model...
epoch: 4, total loss: 3.5474324566977367
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 11.1738, Acc: 0.3883, AUC: 0.5178, F1: 0.1972
[VIT] val - Epoch: 4, Loss: 1.3900, Acc: 0.4078, AUC: 0.7082, F1: 0.2385
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7082
Saving vit model...
epoch: 5, total loss: 3.570179428373064
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 3.1262, Acc: 0.3592, AUC: 0.5229, F1: 0.2112
[VIT] val - Epoch: 5, Loss: 1.3765, Acc: 0.4272, AUC: 0.7339, F1: 0.2019
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5229
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7339
Saving vit model...
epoch: 6, total loss: 3.5089210782732283
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.4934, Acc: 0.4078, AUC: 0.6058, F1: 0.2338
[VIT] val - Epoch: 6, Loss: 1.3330, Acc: 0.5146, AUC: 0.7712, F1: 0.2547
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6058
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7712
Saving vit model...
epoch: 7, total loss: 3.847125768661499
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.4727, Acc: 0.3689, AUC: 0.5739, F1: 0.1774
[VIT] val - Epoch: 7, Loss: 1.3159, Acc: 0.5340, AUC: 0.7439, F1: 0.2939
epoch: 8, total loss: 3.2143309797559465
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 1.3578, Acc: 0.4175, AUC: 0.6610, F1: 0.1931
[VIT] val - Epoch: 8, Loss: 1.3177, Acc: 0.5340, AUC: 0.7361, F1: 0.2820
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6610
Saving cnn model...
epoch: 9, total loss: 3.316539628165109
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 1.4354, Acc: 0.4272, AUC: 0.6682, F1: 0.1802
[VIT] val - Epoch: 9, Loss: 1.3146, Acc: 0.4951, AUC: 0.7363, F1: 0.3400
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6682
Saving cnn model...
epoch: 10, total loss: 3.5742780481066023
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 2.6788, Acc: 0.3786, AUC: 0.5691, F1: 0.1985
[VIT] val - Epoch: 10, Loss: 1.2509, Acc: 0.5728, AUC: 0.8085, F1: 0.3650
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8085
Saving vit model...
epoch: 11, total loss: 3.264934710093907
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 1.2900, Acc: 0.4078, AUC: 0.6510, F1: 0.2462
[VIT] val - Epoch: 11, Loss: 1.2208, Acc: 0.5631, AUC: 0.8239, F1: 0.3699
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8239
Saving vit model...
epoch: 12, total loss: 3.4368458134787425
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.1731, Acc: 0.4854, AUC: 0.7310, F1: 0.3735
[VIT] val - Epoch: 12, Loss: 1.2109, Acc: 0.5631, AUC: 0.8037, F1: 0.4056
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7310
Saving cnn model...
epoch: 13, total loss: 3.074781826564244
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.2258, Acc: 0.4369, AUC: 0.7104, F1: 0.3234
[VIT] val - Epoch: 13, Loss: 1.2182, Acc: 0.5534, AUC: 0.7778, F1: 0.3708
epoch: 14, total loss: 3.137619835989816
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 1.3506, Acc: 0.4563, AUC: 0.6846, F1: 0.3207
[VIT] val - Epoch: 14, Loss: 1.2226, Acc: 0.5825, AUC: 0.7768, F1: 0.3507
epoch: 15, total loss: 3.3256704807281494
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 1.1086, Acc: 0.5340, AUC: 0.7696, F1: 0.3397
[VIT] val - Epoch: 15, Loss: 1.2102, Acc: 0.5825, AUC: 0.7854, F1: 0.4017
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7696
Saving cnn model...
epoch: 16, total loss: 2.949899332863944
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 1.1732, Acc: 0.5340, AUC: 0.7676, F1: 0.3887
[VIT] val - Epoch: 16, Loss: 1.2066, Acc: 0.5825, AUC: 0.8087, F1: 0.4235
epoch: 17, total loss: 3.2995553697858537
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 1.2886, Acc: 0.4175, AUC: 0.7402, F1: 0.2606
[VIT] val - Epoch: 17, Loss: 1.1942, Acc: 0.5437, AUC: 0.8112, F1: 0.3660
epoch: 18, total loss: 3.060839823314122
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 1.1505, Acc: 0.5146, AUC: 0.7762, F1: 0.2780
[VIT] val - Epoch: 18, Loss: 1.1765, Acc: 0.5437, AUC: 0.8254, F1: 0.3644
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7762
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8254
Saving vit model...
epoch: 19, total loss: 3.1393304552350725
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.2396, Acc: 0.4369, AUC: 0.6675, F1: 0.2654
[VIT] val - Epoch: 19, Loss: 1.1788, Acc: 0.5437, AUC: 0.8265, F1: 0.3801
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8265
Saving vit model...
epoch: 20, total loss: 3.09654096194676
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 1.4930, Acc: 0.3883, AUC: 0.6178, F1: 0.2574
[VIT] val - Epoch: 20, Loss: 1.1902, Acc: 0.6019, AUC: 0.8231, F1: 0.4127
epoch: 21, total loss: 3.3405775002070834
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 1.3517, Acc: 0.3981, AUC: 0.7294, F1: 0.2940
[VIT] val - Epoch: 21, Loss: 1.1754, Acc: 0.6117, AUC: 0.8243, F1: 0.4276
epoch: 22, total loss: 2.830496753965105
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 1.6144, Acc: 0.4563, AUC: 0.6511, F1: 0.2741
[VIT] val - Epoch: 22, Loss: 1.1585, Acc: 0.5631, AUC: 0.8232, F1: 0.3787
epoch: 23, total loss: 3.163973161152431
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 1.1842, Acc: 0.4660, AUC: 0.7539, F1: 0.2551
[VIT] val - Epoch: 23, Loss: 1.1520, Acc: 0.5922, AUC: 0.8221, F1: 0.4105
epoch: 24, total loss: 3.2404124396187917
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 1.2208, Acc: 0.5340, AUC: 0.7321, F1: 0.4254
[VIT] val - Epoch: 24, Loss: 1.1476, Acc: 0.5825, AUC: 0.8325, F1: 0.4188
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8325
Saving vit model...
epoch: 25, total loss: 3.1473571913582936
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 1.3097, Acc: 0.4660, AUC: 0.7460, F1: 0.3013
[VIT] val - Epoch: 25, Loss: 1.1360, Acc: 0.5728, AUC: 0.8444, F1: 0.4433
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8444
Saving vit model...
epoch: 26, total loss: 3.252168451036726
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 1.1515, Acc: 0.5340, AUC: 0.8025, F1: 0.3725
[VIT] val - Epoch: 26, Loss: 1.1040, Acc: 0.6117, AUC: 0.8570, F1: 0.4510
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8025
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8570
Saving vit model...
epoch: 27, total loss: 3.11974835395813
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 1.1429, Acc: 0.4854, AUC: 0.7951, F1: 0.2969
[VIT] val - Epoch: 27, Loss: 1.1192, Acc: 0.5728, AUC: 0.8405, F1: 0.4131
epoch: 28, total loss: 3.2194872243063792
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 1.2394, Acc: 0.5146, AUC: 0.7578, F1: 0.3295
[VIT] val - Epoch: 28, Loss: 1.1257, Acc: 0.5437, AUC: 0.8334, F1: 0.4102
epoch: 29, total loss: 2.744844606944493
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 1.1834, Acc: 0.4660, AUC: 0.7682, F1: 0.3106
[VIT] val - Epoch: 29, Loss: 1.1296, Acc: 0.5728, AUC: 0.8345, F1: 0.4279
epoch: 30, total loss: 3.026609625135149
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 1.1659, Acc: 0.4854, AUC: 0.7940, F1: 0.3776
[VIT] val - Epoch: 30, Loss: 1.1451, Acc: 0.5146, AUC: 0.8298, F1: 0.3931
epoch: 31, total loss: 3.209261792046683
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 1.2080, Acc: 0.4757, AUC: 0.7798, F1: 0.3351
[VIT] val - Epoch: 31, Loss: 1.1365, Acc: 0.4854, AUC: 0.8287, F1: 0.3474
epoch: 32, total loss: 3.1112617424556186
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 1.1460, Acc: 0.4951, AUC: 0.8028, F1: 0.3382
[VIT] val - Epoch: 32, Loss: 1.1092, Acc: 0.5243, AUC: 0.8378, F1: 0.3574
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8028
Saving cnn model...
epoch: 33, total loss: 3.349893263408116
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 1.1545, Acc: 0.5146, AUC: 0.7781, F1: 0.3443
[VIT] val - Epoch: 33, Loss: 1.0865, Acc: 0.5728, AUC: 0.8471, F1: 0.4630
epoch: 34, total loss: 3.134974241256714
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 1.1569, Acc: 0.4660, AUC: 0.7606, F1: 0.3061
[VIT] val - Epoch: 34, Loss: 1.0699, Acc: 0.6117, AUC: 0.8504, F1: 0.4393
epoch: 35, total loss: 2.7291676657540456
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 1.1654, Acc: 0.4757, AUC: 0.7795, F1: 0.3031
[VIT] val - Epoch: 35, Loss: 1.0906, Acc: 0.5728, AUC: 0.8385, F1: 0.4255
epoch: 36, total loss: 3.3469795158931186
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 1.1448, Acc: 0.4854, AUC: 0.7961, F1: 0.3764
[VIT] val - Epoch: 36, Loss: 1.1021, Acc: 0.5728, AUC: 0.8449, F1: 0.4291
epoch: 37, total loss: 3.0536836215427945
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 1.1795, Acc: 0.4757, AUC: 0.7622, F1: 0.3229
[VIT] val - Epoch: 37, Loss: 1.1102, Acc: 0.5631, AUC: 0.8532, F1: 0.4358
epoch: 38, total loss: 3.0368023940495084
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 1.2432, Acc: 0.4951, AUC: 0.6721, F1: 0.3257
[VIT] val - Epoch: 38, Loss: 1.1123, Acc: 0.5534, AUC: 0.8390, F1: 0.4147
epoch: 39, total loss: 2.766246489116124
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 1.2127, Acc: 0.4854, AUC: 0.7396, F1: 0.3485
[VIT] val - Epoch: 39, Loss: 1.1145, Acc: 0.5146, AUC: 0.8315, F1: 0.3712
epoch: 40, total loss: 2.7891320841653005
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 1.1381, Acc: 0.5243, AUC: 0.7541, F1: 0.3722
[VIT] val - Epoch: 40, Loss: 1.1007, Acc: 0.5437, AUC: 0.8285, F1: 0.3879
epoch: 41, total loss: 2.654193605695452
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 1.2043, Acc: 0.5049, AUC: 0.7256, F1: 0.3090
[VIT] val - Epoch: 41, Loss: 1.0969, Acc: 0.5825, AUC: 0.8178, F1: 0.4447
epoch: 42, total loss: 3.1487723078046526
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 1.1664, Acc: 0.5146, AUC: 0.7565, F1: 0.3889
[VIT] val - Epoch: 42, Loss: 1.0904, Acc: 0.6019, AUC: 0.8190, F1: 0.4403
epoch: 43, total loss: 2.6222963333129883
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 1.3205, Acc: 0.4369, AUC: 0.6693, F1: 0.2936
[VIT] val - Epoch: 43, Loss: 1.0806, Acc: 0.6019, AUC: 0.8296, F1: 0.4519
epoch: 44, total loss: 2.7442209039415633
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 1.2913, Acc: 0.4757, AUC: 0.6946, F1: 0.3304
[VIT] val - Epoch: 44, Loss: 1.0916, Acc: 0.6311, AUC: 0.8184, F1: 0.4653
epoch: 45, total loss: 2.637866871697562
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 1.2177, Acc: 0.5243, AUC: 0.7117, F1: 0.3657
[VIT] val - Epoch: 45, Loss: 1.0872, Acc: 0.6311, AUC: 0.8161, F1: 0.4792
epoch: 46, total loss: 3.002409969057356
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 1.0951, Acc: 0.5534, AUC: 0.7902, F1: 0.4093
[VIT] val - Epoch: 46, Loss: 1.0756, Acc: 0.6408, AUC: 0.8201, F1: 0.4868
epoch: 47, total loss: 2.6826207297188893
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 1.2247, Acc: 0.5049, AUC: 0.7537, F1: 0.3834
[VIT] val - Epoch: 47, Loss: 1.0542, Acc: 0.6019, AUC: 0.8347, F1: 0.4570
epoch: 48, total loss: 2.9946490015302385
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 1.2158, Acc: 0.4951, AUC: 0.6926, F1: 0.2999
[VIT] val - Epoch: 48, Loss: 1.0380, Acc: 0.6019, AUC: 0.8398, F1: 0.4463
epoch: 49, total loss: 2.6423910685947964
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 1.1559, Acc: 0.5340, AUC: 0.7426, F1: 0.3651
[VIT] val - Epoch: 49, Loss: 1.0373, Acc: 0.6408, AUC: 0.8289, F1: 0.4684
epoch: 50, total loss: 2.5313389471599033
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 1.0783, Acc: 0.5631, AUC: 0.7183, F1: 0.3922
[VIT] val - Epoch: 50, Loss: 1.0601, Acc: 0.6214, AUC: 0.8004, F1: 0.4245
epoch: 51, total loss: 3.0268770456314087
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 1.1023, Acc: 0.5146, AUC: 0.7094, F1: 0.3343
[VIT] val - Epoch: 51, Loss: 1.0639, Acc: 0.6019, AUC: 0.7903, F1: 0.4243
epoch: 52, total loss: 2.6643679482596263
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 1.1595, Acc: 0.5146, AUC: 0.7334, F1: 0.3450
[VIT] val - Epoch: 52, Loss: 1.0613, Acc: 0.5728, AUC: 0.8005, F1: 0.4075
epoch: 53, total loss: 3.2585289478302
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 1.2082, Acc: 0.5049, AUC: 0.7516, F1: 0.3313
[VIT] val - Epoch: 53, Loss: 1.0677, Acc: 0.6214, AUC: 0.8120, F1: 0.4801
epoch: 54, total loss: 2.956416538783482
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 1.1524, Acc: 0.4854, AUC: 0.7940, F1: 0.3035
[VIT] val - Epoch: 54, Loss: 1.0736, Acc: 0.5922, AUC: 0.8256, F1: 0.4714
epoch: 55, total loss: 2.2502324921744212
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 1.0905, Acc: 0.4660, AUC: 0.8055, F1: 0.3247
[VIT] val - Epoch: 55, Loss: 1.0565, Acc: 0.5534, AUC: 0.8401, F1: 0.4157
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8055
Saving cnn model...
epoch: 56, total loss: 2.96028048651559
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 1.2504, Acc: 0.5340, AUC: 0.7046, F1: 0.3949
[VIT] val - Epoch: 56, Loss: 1.0489, Acc: 0.6311, AUC: 0.8399, F1: 0.4719
epoch: 57, total loss: 3.042487246649606
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 1.0930, Acc: 0.5146, AUC: 0.8010, F1: 0.3436
[VIT] val - Epoch: 57, Loss: 1.0486, Acc: 0.6117, AUC: 0.8381, F1: 0.4596
epoch: 58, total loss: 2.82398670060294
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 1.2273, Acc: 0.4466, AUC: 0.7115, F1: 0.3349
[VIT] val - Epoch: 58, Loss: 1.0506, Acc: 0.6019, AUC: 0.8418, F1: 0.4588
epoch: 59, total loss: 3.104799270629883
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 1.2106, Acc: 0.4854, AUC: 0.7214, F1: 0.3716
[VIT] val - Epoch: 59, Loss: 1.0518, Acc: 0.5534, AUC: 0.8358, F1: 0.4010
epoch: 60, total loss: 2.6270097153527394
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 1.1210, Acc: 0.5243, AUC: 0.7931, F1: 0.3380
[VIT] val - Epoch: 60, Loss: 1.0365, Acc: 0.5922, AUC: 0.8459, F1: 0.4507
epoch: 61, total loss: 2.587924003601074
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 1.1340, Acc: 0.5340, AUC: 0.7993, F1: 0.4097
[VIT] val - Epoch: 61, Loss: 1.0346, Acc: 0.6117, AUC: 0.8476, F1: 0.4686
epoch: 62, total loss: 2.7189840929848805
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 1.0742, Acc: 0.5534, AUC: 0.7941, F1: 0.4464
[VIT] val - Epoch: 62, Loss: 1.0477, Acc: 0.6019, AUC: 0.8429, F1: 0.4455
epoch: 63, total loss: 2.7587255580084666
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 1.1569, Acc: 0.5146, AUC: 0.7204, F1: 0.3624
[VIT] val - Epoch: 63, Loss: 1.0367, Acc: 0.6019, AUC: 0.8406, F1: 0.4567
epoch: 64, total loss: 2.807668379374913
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 1.0611, Acc: 0.5243, AUC: 0.8111, F1: 0.3789
[VIT] val - Epoch: 64, Loss: 1.0196, Acc: 0.6019, AUC: 0.8485, F1: 0.4473
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8111
Saving cnn model...
epoch: 65, total loss: 2.8593074594225203
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 1.1073, Acc: 0.4951, AUC: 0.8173, F1: 0.3504
[VIT] val - Epoch: 65, Loss: 1.0185, Acc: 0.6311, AUC: 0.8543, F1: 0.4702
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8173
Saving cnn model...
epoch: 66, total loss: 2.885232448577881
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 1.0545, Acc: 0.5437, AUC: 0.7978, F1: 0.3790
[VIT] val - Epoch: 66, Loss: 1.0253, Acc: 0.6117, AUC: 0.8549, F1: 0.4556
epoch: 67, total loss: 2.673550912312099
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 1.0947, Acc: 0.5243, AUC: 0.7584, F1: 0.3925
[VIT] val - Epoch: 67, Loss: 1.0200, Acc: 0.6214, AUC: 0.8539, F1: 0.4706
epoch: 68, total loss: 2.848439931869507
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 1.1348, Acc: 0.5728, AUC: 0.7766, F1: 0.3902
[VIT] val - Epoch: 68, Loss: 1.0153, Acc: 0.6117, AUC: 0.8508, F1: 0.4649
epoch: 69, total loss: 2.8122782707214355
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 1.0164, Acc: 0.5631, AUC: 0.8281, F1: 0.4233
[VIT] val - Epoch: 69, Loss: 1.0146, Acc: 0.6408, AUC: 0.8465, F1: 0.4954
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8281
Saving cnn model...
epoch: 70, total loss: 2.4773058891296387
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 1.0413, Acc: 0.5631, AUC: 0.8364, F1: 0.4261
[VIT] val - Epoch: 70, Loss: 1.0195, Acc: 0.6019, AUC: 0.8432, F1: 0.4470
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8364
Saving cnn model...
epoch: 71, total loss: 2.5419623170580183
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 1.0613, Acc: 0.5534, AUC: 0.7920, F1: 0.3873
[VIT] val - Epoch: 71, Loss: 1.0211, Acc: 0.5825, AUC: 0.8488, F1: 0.4308
epoch: 72, total loss: 2.852959360395159
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 1.0487, Acc: 0.5340, AUC: 0.7857, F1: 0.3733
[VIT] val - Epoch: 72, Loss: 1.0192, Acc: 0.5922, AUC: 0.8390, F1: 0.4403
epoch: 73, total loss: 2.835486037390573
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 1.0422, Acc: 0.5049, AUC: 0.8154, F1: 0.3860
[VIT] val - Epoch: 73, Loss: 1.0179, Acc: 0.5825, AUC: 0.8349, F1: 0.4224
epoch: 74, total loss: 2.801941122327532
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 1.0647, Acc: 0.5340, AUC: 0.7835, F1: 0.3689
[VIT] val - Epoch: 74, Loss: 1.0238, Acc: 0.5825, AUC: 0.8430, F1: 0.4283
epoch: 75, total loss: 2.4817473718098233
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 1.0415, Acc: 0.5728, AUC: 0.8118, F1: 0.4073
[VIT] val - Epoch: 75, Loss: 1.0232, Acc: 0.5922, AUC: 0.8499, F1: 0.4414
epoch: 76, total loss: 2.6809138570513045
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 1.0845, Acc: 0.5146, AUC: 0.7989, F1: 0.3680
[VIT] val - Epoch: 76, Loss: 1.0263, Acc: 0.5631, AUC: 0.8487, F1: 0.4318
epoch: 77, total loss: 2.900411980492728
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 1.0315, Acc: 0.5728, AUC: 0.8126, F1: 0.4335
[VIT] val - Epoch: 77, Loss: 1.0237, Acc: 0.5825, AUC: 0.8414, F1: 0.4508
epoch: 78, total loss: 2.785703080041068
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 1.0136, Acc: 0.5146, AUC: 0.8217, F1: 0.3675
[VIT] val - Epoch: 78, Loss: 1.0147, Acc: 0.6019, AUC: 0.8373, F1: 0.4571
epoch: 79, total loss: 2.4387527874537875
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 1.1206, Acc: 0.5049, AUC: 0.7738, F1: 0.3387
[VIT] val - Epoch: 79, Loss: 1.0174, Acc: 0.5825, AUC: 0.8332, F1: 0.4370
epoch: 80, total loss: 2.884384478841509
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 1.0177, Acc: 0.5340, AUC: 0.7872, F1: 0.4029
[VIT] val - Epoch: 80, Loss: 1.0191, Acc: 0.5728, AUC: 0.8261, F1: 0.4210
epoch: 81, total loss: 2.4475412368774414
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.9834, Acc: 0.5437, AUC: 0.8121, F1: 0.4365
[VIT] val - Epoch: 81, Loss: 1.0145, Acc: 0.6214, AUC: 0.8254, F1: 0.4656
epoch: 82, total loss: 2.540462170328413
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 1.0092, Acc: 0.5631, AUC: 0.8185, F1: 0.4257
[VIT] val - Epoch: 82, Loss: 1.0124, Acc: 0.6214, AUC: 0.8265, F1: 0.4551
epoch: 83, total loss: 2.6941844054630826
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 1.0316, Acc: 0.5825, AUC: 0.8190, F1: 0.4285
[VIT] val - Epoch: 83, Loss: 1.0121, Acc: 0.6408, AUC: 0.8292, F1: 0.4986
epoch: 84, total loss: 2.535323909350804
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 1.0379, Acc: 0.5728, AUC: 0.8274, F1: 0.3955
[VIT] val - Epoch: 84, Loss: 1.0091, Acc: 0.6311, AUC: 0.8358, F1: 0.4869
epoch: 85, total loss: 2.240209664617266
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 1.0298, Acc: 0.5437, AUC: 0.8364, F1: 0.4041
[VIT] val - Epoch: 85, Loss: 1.0084, Acc: 0.6117, AUC: 0.8421, F1: 0.4622
epoch: 86, total loss: 2.756369079862322
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 1.0472, Acc: 0.5631, AUC: 0.8163, F1: 0.4036
[VIT] val - Epoch: 86, Loss: 1.0165, Acc: 0.6019, AUC: 0.8418, F1: 0.4502
epoch: 87, total loss: 2.785344362258911
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 1.0195, Acc: 0.5728, AUC: 0.8224, F1: 0.4525
[VIT] val - Epoch: 87, Loss: 1.0132, Acc: 0.5728, AUC: 0.8445, F1: 0.4341
epoch: 88, total loss: 2.5506716115134105
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.9963, Acc: 0.5728, AUC: 0.8350, F1: 0.4482
[VIT] val - Epoch: 88, Loss: 1.0091, Acc: 0.5825, AUC: 0.8453, F1: 0.4505
epoch: 89, total loss: 2.6662857873099193
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 1.1156, Acc: 0.5146, AUC: 0.8075, F1: 0.3613
[VIT] val - Epoch: 89, Loss: 1.0049, Acc: 0.5922, AUC: 0.8454, F1: 0.4598
epoch: 90, total loss: 2.8647607394627164
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.9833, Acc: 0.5631, AUC: 0.8387, F1: 0.3894
[VIT] val - Epoch: 90, Loss: 1.0014, Acc: 0.5922, AUC: 0.8419, F1: 0.4607
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8387
Saving cnn model...
epoch: 91, total loss: 2.8689549139567783
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 1.0068, Acc: 0.5146, AUC: 0.8304, F1: 0.3748
[VIT] val - Epoch: 91, Loss: 0.9974, Acc: 0.6214, AUC: 0.8469, F1: 0.4822
epoch: 92, total loss: 2.855970161301749
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.9975, Acc: 0.5825, AUC: 0.8429, F1: 0.4274
[VIT] val - Epoch: 92, Loss: 0.9955, Acc: 0.6408, AUC: 0.8487, F1: 0.4868
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8429
Saving cnn model...
epoch: 93, total loss: 2.4914014680044994
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 1.0305, Acc: 0.5340, AUC: 0.8301, F1: 0.4003
[VIT] val - Epoch: 93, Loss: 0.9938, Acc: 0.6505, AUC: 0.8457, F1: 0.4919
epoch: 94, total loss: 2.5942020416259766
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 1.0589, Acc: 0.4854, AUC: 0.8318, F1: 0.3470
[VIT] val - Epoch: 94, Loss: 0.9952, Acc: 0.6019, AUC: 0.8447, F1: 0.4407
epoch: 95, total loss: 2.8646647930145264
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.9959, Acc: 0.5728, AUC: 0.8279, F1: 0.4087
[VIT] val - Epoch: 95, Loss: 1.0037, Acc: 0.6117, AUC: 0.8396, F1: 0.4388
epoch: 96, total loss: 2.636336326599121
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 1.0086, Acc: 0.5146, AUC: 0.8337, F1: 0.3515
[VIT] val - Epoch: 96, Loss: 1.0067, Acc: 0.6214, AUC: 0.8406, F1: 0.4565
epoch: 97, total loss: 2.7259977545057024
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 1.0182, Acc: 0.5243, AUC: 0.8317, F1: 0.3665
[VIT] val - Epoch: 97, Loss: 1.0076, Acc: 0.6214, AUC: 0.8433, F1: 0.4575
epoch: 98, total loss: 2.3531703608376637
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.9995, Acc: 0.5437, AUC: 0.8303, F1: 0.3581
[VIT] val - Epoch: 98, Loss: 1.0043, Acc: 0.6408, AUC: 0.8453, F1: 0.4897
epoch: 99, total loss: 2.6334725618362427
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.9455, Acc: 0.5631, AUC: 0.8383, F1: 0.3748
[VIT] val - Epoch: 99, Loss: 1.0000, Acc: 0.6408, AUC: 0.8465, F1: 0.4859
epoch: 100, total loss: 2.3120830740247453
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 1.0072, Acc: 0.5534, AUC: 0.8309, F1: 0.4037
[VIT] val - Epoch: 100, Loss: 1.0045, Acc: 0.5922, AUC: 0.8399, F1: 0.4433
[08:05:47][Rank 1] Training Finished. Starting Final Testing...[08:05:47][Rank 2] Training Finished. Starting Final Testing...

[08:05:47][Rank 3] Training Finished. Starting Final Testing...
[08:05:47][Rank 0] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.0949, Acc: 0.6275, AUC: 0.7911, F1: 0.3990
[VIT] test - Epoch: 100, Loss: 1.0844, Acc: 0.6075, AUC: 0.7498, F1: 0.3751
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.1057, Acc: 0.6552, AUC: 0.7361, F1: 0.2614
[VIT] test - Epoch: 100, Loss: 1.1465, Acc: 0.6399, AUC: 0.7301, F1: 0.3469
‚úÖ [ÂÆåÊàê] Ê∫êÂüü IDRID ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: MESSIDOR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='MESSIDOR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['MESSIDOR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_MESSIDOR
OUT_DIR: ./output_esdg_h100/MESSIDOR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified2/output_esdg_h100/MESSIDOR/CASS_GDRNet_ESDG_MESSIDOR
[08:13:07][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
[08:13:07][Rank 2] Loading datasets...
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
[08:13:07][Rank 1] Loading datasets...
================ [Auto Config] ================
Source: ['MESSIDOR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'RLDR', 'EYEPACS']
Output Dir: ./output_esdg_h100/MESSIDOR
===============================================
[08:13:07][Rank 3] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
üßä [DINOv3] All base parameters frozen.Injecting LoRA into: layer.2.attention.v_proj

Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_projInjecting LoRA into: layer.5.attention.k_proj

Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 3.820482470772483
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 5.3312, Acc: 0.4741, AUC: 0.5529, F1: 0.1773
[VIT] val - Epoch: 1, Loss: 1.2229, Acc: 0.5546, AUC: 0.5783, F1: 0.1427
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5529
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5783
Saving vit model...
epoch: 2, total loss: 3.201002689925107
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 1.4866, Acc: 0.5546, AUC: 0.5367, F1: 0.1427
[VIT] val - Epoch: 2, Loss: 1.2430, Acc: 0.5603, AUC: 0.5857, F1: 0.1627
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5857
Saving vit model...
epoch: 3, total loss: 3.1464631340720435
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 1.1851, Acc: 0.5546, AUC: 0.5834, F1: 0.1427
[VIT] val - Epoch: 3, Loss: 1.2049, Acc: 0.5431, AUC: 0.6086, F1: 0.1545
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5834
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6086
Saving vit model...
epoch: 4, total loss: 2.901564359664917
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 1.2003, Acc: 0.5546, AUC: 0.5937, F1: 0.1427
[VIT] val - Epoch: 4, Loss: 1.1964, Acc: 0.5603, AUC: 0.6879, F1: 0.2286
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5937
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6879
Saving vit model...
epoch: 5, total loss: 3.3063481721011074
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.7151, Acc: 0.6092, AUC: 0.5253, F1: 0.2243
[VIT] val - Epoch: 5, Loss: 1.1758, Acc: 0.5460, AUC: 0.6374, F1: 0.1660
epoch: 6, total loss: 3.0770836635069414
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.2041, Acc: 0.5431, AUC: 0.5562, F1: 0.2005
[VIT] val - Epoch: 6, Loss: 1.1688, Acc: 0.5776, AUC: 0.6321, F1: 0.2642
epoch: 7, total loss: 3.1284782886505127
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.3240, Acc: 0.4799, AUC: 0.5281, F1: 0.2461
[VIT] val - Epoch: 7, Loss: 1.1754, Acc: 0.5603, AUC: 0.6817, F1: 0.2480
epoch: 8, total loss: 3.0683050643314016
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 6.1957, Acc: 0.5747, AUC: 0.6035, F1: 0.2053
[VIT] val - Epoch: 8, Loss: 1.1307, Acc: 0.6034, AUC: 0.7419, F1: 0.2737
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6035
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7419
Saving vit model...
epoch: 9, total loss: 2.633127288384871
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 1.1150, Acc: 0.6178, AUC: 0.6515, F1: 0.2530
[VIT] val - Epoch: 9, Loss: 1.1067, Acc: 0.6092, AUC: 0.7146, F1: 0.3038
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6515
Saving cnn model...
epoch: 10, total loss: 2.661485265601765
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 1.0586, Acc: 0.5862, AUC: 0.6824, F1: 0.2343
[VIT] val - Epoch: 10, Loss: 1.1099, Acc: 0.5977, AUC: 0.7110, F1: 0.2577
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6824
Saving cnn model...
epoch: 11, total loss: 2.904774324460463
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 1.1653, Acc: 0.5920, AUC: 0.5957, F1: 0.2814
[VIT] val - Epoch: 11, Loss: 1.1158, Acc: 0.5948, AUC: 0.7250, F1: 0.2883
epoch: 12, total loss: 2.5538506616245615
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.0660, Acc: 0.6149, AUC: 0.6429, F1: 0.2293
[VIT] val - Epoch: 12, Loss: 1.0859, Acc: 0.6063, AUC: 0.7456, F1: 0.2737
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7456
Saving vit model...
epoch: 13, total loss: 2.7714158350771125
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.1144, Acc: 0.5546, AUC: 0.6382, F1: 0.2678
[VIT] val - Epoch: 13, Loss: 1.0871, Acc: 0.6063, AUC: 0.7831, F1: 0.3424
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7831
Saving vit model...
epoch: 14, total loss: 2.942306935787201
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 1.0357, Acc: 0.6264, AUC: 0.7033, F1: 0.2948
[VIT] val - Epoch: 14, Loss: 1.0976, Acc: 0.6063, AUC: 0.7361, F1: 0.3501
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7033
Saving cnn model...
epoch: 15, total loss: 2.5722611329772254
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 1.8242, Acc: 0.6408, AUC: 0.6017, F1: 0.3251
[VIT] val - Epoch: 15, Loss: 1.0844, Acc: 0.6178, AUC: 0.7800, F1: 0.3415
epoch: 16, total loss: 2.7766130458224905
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 5.5833, Acc: 0.5920, AUC: 0.5599, F1: 0.2427
[VIT] val - Epoch: 16, Loss: 1.0830, Acc: 0.5862, AUC: 0.7277, F1: 0.3408
epoch: 17, total loss: 2.6140602827072144
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 1.1624, Acc: 0.5833, AUC: 0.6433, F1: 0.2844
[VIT] val - Epoch: 17, Loss: 1.0534, Acc: 0.6207, AUC: 0.7397, F1: 0.3350
epoch: 18, total loss: 2.490027676929127
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 1.3027, Acc: 0.6092, AUC: 0.6606, F1: 0.2749
[VIT] val - Epoch: 18, Loss: 1.0632, Acc: 0.6006, AUC: 0.7535, F1: 0.3335
epoch: 19, total loss: 2.587872478094968
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.1822, Acc: 0.6466, AUC: 0.6981, F1: 0.3373
[VIT] val - Epoch: 19, Loss: 1.0673, Acc: 0.5776, AUC: 0.7873, F1: 0.3694
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7873
Saving vit model...
epoch: 20, total loss: 2.681588579307903
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 1.0980, Acc: 0.5776, AUC: 0.7088, F1: 0.3228
[VIT] val - Epoch: 20, Loss: 1.0492, Acc: 0.5891, AUC: 0.7759, F1: 0.3446
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7088
Saving cnn model...
epoch: 21, total loss: 2.357130440798673
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 1.0834, Acc: 0.5805, AUC: 0.7286, F1: 0.2867
[VIT] val - Epoch: 21, Loss: 1.0800, Acc: 0.5833, AUC: 0.7651, F1: 0.3504
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7286
Saving cnn model...
epoch: 22, total loss: 2.838848114013672
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 1.1338, Acc: 0.5431, AUC: 0.7425, F1: 0.3220
[VIT] val - Epoch: 22, Loss: 1.0373, Acc: 0.5747, AUC: 0.7761, F1: 0.3282
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7425
Saving cnn model...
epoch: 23, total loss: 2.51892072504217
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 1.7899, Acc: 0.6293, AUC: 0.6424, F1: 0.3083
[VIT] val - Epoch: 23, Loss: 1.0305, Acc: 0.6264, AUC: 0.7873, F1: 0.4172
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7873
Saving vit model...
epoch: 24, total loss: 2.3352061076597734
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 1.0195, Acc: 0.6207, AUC: 0.7701, F1: 0.3233
[VIT] val - Epoch: 24, Loss: 1.0354, Acc: 0.5805, AUC: 0.7831, F1: 0.3634
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7701
Saving cnn model...
epoch: 25, total loss: 2.474922001361847
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 1.2195, Acc: 0.6351, AUC: 0.6501, F1: 0.3443
[VIT] val - Epoch: 25, Loss: 1.0214, Acc: 0.6207, AUC: 0.7911, F1: 0.4293
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7911
Saving vit model...
epoch: 26, total loss: 2.5812800851735203
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 1.1325, Acc: 0.5201, AUC: 0.6334, F1: 0.2803
[VIT] val - Epoch: 26, Loss: 1.0334, Acc: 0.5977, AUC: 0.7881, F1: 0.3990
epoch: 27, total loss: 2.6482575048099863
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 1.0840, Acc: 0.6149, AUC: 0.7331, F1: 0.2682
[VIT] val - Epoch: 27, Loss: 1.0071, Acc: 0.6063, AUC: 0.7912, F1: 0.3851
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7912
Saving vit model...
epoch: 28, total loss: 2.623132039200176
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 1.1121, Acc: 0.5374, AUC: 0.7242, F1: 0.3144
[VIT] val - Epoch: 28, Loss: 1.0377, Acc: 0.5805, AUC: 0.7798, F1: 0.4033
epoch: 29, total loss: 2.3819885633208533
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 1.0942, Acc: 0.6293, AUC: 0.6617, F1: 0.3319
[VIT] val - Epoch: 29, Loss: 1.0301, Acc: 0.6092, AUC: 0.7822, F1: 0.3840
epoch: 30, total loss: 2.527284123680808
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 0.9791, Acc: 0.6408, AUC: 0.7187, F1: 0.3701
[VIT] val - Epoch: 30, Loss: 1.0425, Acc: 0.5575, AUC: 0.7732, F1: 0.3899
epoch: 31, total loss: 2.5364803563464773
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 0.9589, Acc: 0.5862, AUC: 0.7817, F1: 0.3624
[VIT] val - Epoch: 31, Loss: 1.0200, Acc: 0.6063, AUC: 0.7974, F1: 0.4563
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7817
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7974
Saving vit model...
epoch: 32, total loss: 2.5185602740807966
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 1.5740, Acc: 0.5144, AUC: 0.7126, F1: 0.3233
[VIT] val - Epoch: 32, Loss: 1.0391, Acc: 0.5833, AUC: 0.7833, F1: 0.4282
epoch: 33, total loss: 2.3647206371480767
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 1.3554, Acc: 0.5431, AUC: 0.6717, F1: 0.2697
[VIT] val - Epoch: 33, Loss: 1.0055, Acc: 0.6178, AUC: 0.7906, F1: 0.3616
epoch: 34, total loss: 2.612817406654358
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 1.0834, Acc: 0.6006, AUC: 0.7498, F1: 0.3379
[VIT] val - Epoch: 34, Loss: 1.0318, Acc: 0.5776, AUC: 0.7763, F1: 0.3989
epoch: 35, total loss: 2.4897836392576043
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 0.9728, Acc: 0.6494, AUC: 0.7049, F1: 0.3748
[VIT] val - Epoch: 35, Loss: 1.0081, Acc: 0.5920, AUC: 0.7981, F1: 0.4069
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7981
Saving vit model...
epoch: 36, total loss: 2.6813158013603906
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 0.9810, Acc: 0.6609, AUC: 0.7550, F1: 0.3610
[VIT] val - Epoch: 36, Loss: 1.0079, Acc: 0.6121, AUC: 0.7902, F1: 0.4120
epoch: 37, total loss: 2.714209800416773
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 1.1458, Acc: 0.5402, AUC: 0.7469, F1: 0.3422
[VIT] val - Epoch: 37, Loss: 1.0261, Acc: 0.5833, AUC: 0.7806, F1: 0.4460
epoch: 38, total loss: 2.4349034320224416
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 1.1586, Acc: 0.5747, AUC: 0.7490, F1: 0.3775
[VIT] val - Epoch: 38, Loss: 1.0214, Acc: 0.5661, AUC: 0.7936, F1: 0.4108
epoch: 39, total loss: 2.683089630170302
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 1.0537, Acc: 0.6494, AUC: 0.7308, F1: 0.3673
[VIT] val - Epoch: 39, Loss: 1.0043, Acc: 0.6063, AUC: 0.7955, F1: 0.4394
epoch: 40, total loss: 2.786543228409507
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 1.1562, Acc: 0.6868, AUC: 0.6819, F1: 0.4006
[VIT] val - Epoch: 40, Loss: 0.9977, Acc: 0.5977, AUC: 0.7991, F1: 0.4058
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7991
Saving vit model...
epoch: 41, total loss: 2.3235809044404463
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 0.9893, Acc: 0.6782, AUC: 0.7214, F1: 0.3797
[VIT] val - Epoch: 41, Loss: 1.0102, Acc: 0.6006, AUC: 0.7997, F1: 0.4396
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7997
Saving vit model...
epoch: 42, total loss: 2.5702987530014734
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 1.1134, Acc: 0.5460, AUC: 0.7819, F1: 0.3249
[VIT] val - Epoch: 42, Loss: 1.0119, Acc: 0.5977, AUC: 0.7891, F1: 0.4043
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7819
Saving cnn model...
epoch: 43, total loss: 2.585461388934742
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 0.8841, Acc: 0.6839, AUC: 0.7975, F1: 0.4284
[VIT] val - Epoch: 43, Loss: 1.0159, Acc: 0.6063, AUC: 0.8002, F1: 0.4823
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7975
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8002
Saving vit model...
epoch: 44, total loss: 2.216888064687902
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 0.8395, Acc: 0.6954, AUC: 0.8071, F1: 0.4124
[VIT] val - Epoch: 44, Loss: 0.9976, Acc: 0.6063, AUC: 0.8094, F1: 0.4515
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8071
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8094
Saving vit model...
epoch: 45, total loss: 2.285925653847781
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.8993, Acc: 0.6695, AUC: 0.7917, F1: 0.3950
[VIT] val - Epoch: 45, Loss: 0.9903, Acc: 0.6092, AUC: 0.8117, F1: 0.5087
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8117
Saving vit model...
epoch: 46, total loss: 2.3271719325672495
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.9792, Acc: 0.5977, AUC: 0.7983, F1: 0.3291
[VIT] val - Epoch: 46, Loss: 1.0138, Acc: 0.6006, AUC: 0.8058, F1: 0.4705
epoch: 47, total loss: 2.52565257657658
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 1.1022, Acc: 0.5460, AUC: 0.7601, F1: 0.3145
[VIT] val - Epoch: 47, Loss: 0.9929, Acc: 0.6006, AUC: 0.8100, F1: 0.4421
epoch: 48, total loss: 2.431014743718234
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 0.9166, Acc: 0.6494, AUC: 0.7891, F1: 0.3694
[VIT] val - Epoch: 48, Loss: 1.0278, Acc: 0.5747, AUC: 0.7941, F1: 0.4376
epoch: 49, total loss: 2.4594073512337427
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 0.9699, Acc: 0.6494, AUC: 0.7466, F1: 0.4475
[VIT] val - Epoch: 49, Loss: 1.0007, Acc: 0.5977, AUC: 0.7963, F1: 0.4530
epoch: 50, total loss: 2.5061823454770176
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 1.0215, Acc: 0.5546, AUC: 0.8024, F1: 0.3759
[VIT] val - Epoch: 50, Loss: 1.0105, Acc: 0.6034, AUC: 0.7953, F1: 0.4478
epoch: 51, total loss: 2.537512719631195
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.8340, Acc: 0.7098, AUC: 0.8073, F1: 0.4196
[VIT] val - Epoch: 51, Loss: 0.9895, Acc: 0.5805, AUC: 0.8062, F1: 0.4228
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8073
Saving cnn model...
epoch: 52, total loss: 2.3989562825723127
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 0.8444, Acc: 0.6523, AUC: 0.8253, F1: 0.4366
[VIT] val - Epoch: 52, Loss: 0.9890, Acc: 0.6322, AUC: 0.8149, F1: 0.4851
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8253
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8149
Saving vit model...
epoch: 53, total loss: 2.3260964128104122
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 0.9133, Acc: 0.6954, AUC: 0.7739, F1: 0.4528
[VIT] val - Epoch: 53, Loss: 0.9939, Acc: 0.6006, AUC: 0.8131, F1: 0.4860
epoch: 54, total loss: 2.583947853608565
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 0.9842, Acc: 0.6178, AUC: 0.8075, F1: 0.3567
[VIT] val - Epoch: 54, Loss: 0.9826, Acc: 0.5977, AUC: 0.8071, F1: 0.4611
epoch: 55, total loss: 2.3315235376358032
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.9483, Acc: 0.7069, AUC: 0.7544, F1: 0.4623
[VIT] val - Epoch: 55, Loss: 1.0030, Acc: 0.6006, AUC: 0.7884, F1: 0.4478
epoch: 56, total loss: 2.16838369586251
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.8394, Acc: 0.7040, AUC: 0.8135, F1: 0.4885
[VIT] val - Epoch: 56, Loss: 1.0325, Acc: 0.5517, AUC: 0.7838, F1: 0.4104
epoch: 57, total loss: 2.2569343881173567
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 0.9433, Acc: 0.6897, AUC: 0.7754, F1: 0.4114
[VIT] val - Epoch: 57, Loss: 0.9917, Acc: 0.6178, AUC: 0.8007, F1: 0.4877
epoch: 58, total loss: 2.2314693223346365
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.9587, Acc: 0.6897, AUC: 0.7591, F1: 0.4498
[VIT] val - Epoch: 58, Loss: 0.9886, Acc: 0.5776, AUC: 0.8057, F1: 0.4232
epoch: 59, total loss: 2.306894399903037
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 0.8151, Acc: 0.7270, AUC: 0.8153, F1: 0.4794
[VIT] val - Epoch: 59, Loss: 0.9986, Acc: 0.6207, AUC: 0.8047, F1: 0.4943
epoch: 60, total loss: 2.379768745465712
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.8786, Acc: 0.6580, AUC: 0.8242, F1: 0.4007
[VIT] val - Epoch: 60, Loss: 0.9829, Acc: 0.6264, AUC: 0.8051, F1: 0.4783
epoch: 61, total loss: 2.3234786228700117
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 0.9047, Acc: 0.6580, AUC: 0.8303, F1: 0.4333
[VIT] val - Epoch: 61, Loss: 0.9826, Acc: 0.6236, AUC: 0.8030, F1: 0.4679
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8303
Saving cnn model...
epoch: 62, total loss: 2.467851384119554
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.9441, Acc: 0.6379, AUC: 0.8111, F1: 0.3971
[VIT] val - Epoch: 62, Loss: 0.9984, Acc: 0.6034, AUC: 0.8092, F1: 0.4823
epoch: 63, total loss: 2.4245552203871985
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.9068, Acc: 0.6494, AUC: 0.8298, F1: 0.4033
[VIT] val - Epoch: 63, Loss: 0.9776, Acc: 0.6207, AUC: 0.8113, F1: 0.4928
epoch: 64, total loss: 2.4749072085727346
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 1.1063, Acc: 0.6753, AUC: 0.6762, F1: 0.3951
[VIT] val - Epoch: 64, Loss: 0.9945, Acc: 0.5833, AUC: 0.8066, F1: 0.4748
epoch: 65, total loss: 2.365159197287126
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 1.0590, Acc: 0.6839, AUC: 0.7420, F1: 0.4498
[VIT] val - Epoch: 65, Loss: 0.9862, Acc: 0.6034, AUC: 0.8088, F1: 0.4363
epoch: 66, total loss: 2.2712829492308875
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.7898, Acc: 0.7328, AUC: 0.8211, F1: 0.4791
[VIT] val - Epoch: 66, Loss: 0.9985, Acc: 0.5920, AUC: 0.8077, F1: 0.4394
epoch: 67, total loss: 2.330953148278323
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.7922, Acc: 0.7213, AUC: 0.8442, F1: 0.4761
[VIT] val - Epoch: 67, Loss: 0.9809, Acc: 0.5920, AUC: 0.8109, F1: 0.4414
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8442
Saving cnn model...
epoch: 68, total loss: 2.1305141936648977
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.8719, Acc: 0.6897, AUC: 0.8251, F1: 0.4496
[VIT] val - Epoch: 68, Loss: 0.9931, Acc: 0.5833, AUC: 0.8088, F1: 0.4447
epoch: 69, total loss: 2.351488005031239
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 0.8606, Acc: 0.6925, AUC: 0.8359, F1: 0.4208
[VIT] val - Epoch: 69, Loss: 0.9648, Acc: 0.6178, AUC: 0.8207, F1: 0.4774
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8207
Saving vit model...
epoch: 70, total loss: 2.425214799967679
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.9255, Acc: 0.6437, AUC: 0.8151, F1: 0.3794
[VIT] val - Epoch: 70, Loss: 0.9698, Acc: 0.6121, AUC: 0.8208, F1: 0.4894
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8208
Saving vit model...
epoch: 71, total loss: 2.332886739210649
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.9047, Acc: 0.6925, AUC: 0.8113, F1: 0.4434
[VIT] val - Epoch: 71, Loss: 0.9827, Acc: 0.6207, AUC: 0.8089, F1: 0.4988
epoch: 72, total loss: 2.2309167059985073
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.8279, Acc: 0.7241, AUC: 0.8177, F1: 0.4783
[VIT] val - Epoch: 72, Loss: 0.9692, Acc: 0.6092, AUC: 0.8132, F1: 0.4702
epoch: 73, total loss: 2.221048122102564
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.8150, Acc: 0.7126, AUC: 0.8164, F1: 0.4462
[VIT] val - Epoch: 73, Loss: 0.9790, Acc: 0.6092, AUC: 0.8126, F1: 0.4673
epoch: 74, total loss: 2.0638299584388733
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 1.1034, Acc: 0.6638, AUC: 0.7860, F1: 0.4079
[VIT] val - Epoch: 74, Loss: 0.9758, Acc: 0.6121, AUC: 0.8095, F1: 0.4363
epoch: 75, total loss: 2.2190979448231785
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.9975, Acc: 0.6034, AUC: 0.8409, F1: 0.3912
[VIT] val - Epoch: 75, Loss: 0.9821, Acc: 0.5948, AUC: 0.8036, F1: 0.4319
epoch: 76, total loss: 2.3297577771273525
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.8131, Acc: 0.7184, AUC: 0.8161, F1: 0.4903
[VIT] val - Epoch: 76, Loss: 0.9721, Acc: 0.6178, AUC: 0.8112, F1: 0.4614
epoch: 77, total loss: 2.084317597475919
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.9136, Acc: 0.6667, AUC: 0.7926, F1: 0.4276
[VIT] val - Epoch: 77, Loss: 0.9656, Acc: 0.6178, AUC: 0.8129, F1: 0.4832
epoch: 78, total loss: 2.434424411166798
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.9761, Acc: 0.6351, AUC: 0.8068, F1: 0.4238
[VIT] val - Epoch: 78, Loss: 0.9761, Acc: 0.6121, AUC: 0.8070, F1: 0.4859
epoch: 79, total loss: 2.315738943490115
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.8595, Acc: 0.6782, AUC: 0.8047, F1: 0.4418
[VIT] val - Epoch: 79, Loss: 0.9801, Acc: 0.6207, AUC: 0.8058, F1: 0.4998
epoch: 80, total loss: 2.2750342975963247
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.9164, Acc: 0.6523, AUC: 0.8307, F1: 0.4218
[VIT] val - Epoch: 80, Loss: 0.9709, Acc: 0.5891, AUC: 0.8114, F1: 0.4542
epoch: 81, total loss: 2.472524103793231
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.8165, Acc: 0.7155, AUC: 0.8376, F1: 0.4946
[VIT] val - Epoch: 81, Loss: 0.9844, Acc: 0.5920, AUC: 0.8135, F1: 0.4654
epoch: 82, total loss: 2.5763527859341013
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.8383, Acc: 0.6724, AUC: 0.8237, F1: 0.4701
[VIT] val - Epoch: 82, Loss: 0.9784, Acc: 0.5948, AUC: 0.8161, F1: 0.4904
epoch: 83, total loss: 2.4416742487387224
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.9026, Acc: 0.6954, AUC: 0.7795, F1: 0.4408
[VIT] val - Epoch: 83, Loss: 0.9699, Acc: 0.6149, AUC: 0.8130, F1: 0.4917
epoch: 84, total loss: 2.3815834413875234
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.7760, Acc: 0.7299, AUC: 0.8406, F1: 0.5171
[VIT] val - Epoch: 84, Loss: 0.9753, Acc: 0.6121, AUC: 0.8079, F1: 0.4879
epoch: 85, total loss: 2.3674342903223904
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.8654, Acc: 0.6868, AUC: 0.8258, F1: 0.4549
[VIT] val - Epoch: 85, Loss: 0.9748, Acc: 0.6178, AUC: 0.8131, F1: 0.4757
epoch: 86, total loss: 2.345861879262057
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.7983, Acc: 0.7011, AUC: 0.8487, F1: 0.4608
[VIT] val - Epoch: 86, Loss: 0.9656, Acc: 0.6236, AUC: 0.8206, F1: 0.4867
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8487
Saving cnn model...
epoch: 87, total loss: 2.1694570292126047
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.8157, Acc: 0.6954, AUC: 0.8425, F1: 0.4990
[VIT] val - Epoch: 87, Loss: 0.9663, Acc: 0.6264, AUC: 0.8170, F1: 0.4955
epoch: 88, total loss: 2.4723760160532864
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 1.0074, Acc: 0.7011, AUC: 0.7674, F1: 0.4670
[VIT] val - Epoch: 88, Loss: 0.9779, Acc: 0.6149, AUC: 0.8121, F1: 0.4809
epoch: 89, total loss: 2.1298001354390923
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.9659, Acc: 0.7155, AUC: 0.7593, F1: 0.5172
[VIT] val - Epoch: 89, Loss: 0.9735, Acc: 0.6063, AUC: 0.8106, F1: 0.4731
epoch: 90, total loss: 2.3532890569080007
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.9494, Acc: 0.7011, AUC: 0.7499, F1: 0.4943
[VIT] val - Epoch: 90, Loss: 0.9624, Acc: 0.6006, AUC: 0.8162, F1: 0.4741
epoch: 91, total loss: 2.0797830278223213
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.8914, Acc: 0.6437, AUC: 0.8453, F1: 0.4707
[VIT] val - Epoch: 91, Loss: 0.9720, Acc: 0.6092, AUC: 0.8190, F1: 0.4822
epoch: 92, total loss: 2.299199353564869
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.8934, Acc: 0.6695, AUC: 0.8245, F1: 0.4972
[VIT] val - Epoch: 92, Loss: 0.9746, Acc: 0.6063, AUC: 0.8122, F1: 0.4799
epoch: 93, total loss: 2.086595497348092
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.8878, Acc: 0.7098, AUC: 0.7744, F1: 0.4862
[VIT] val - Epoch: 93, Loss: 0.9708, Acc: 0.6236, AUC: 0.8141, F1: 0.5026
epoch: 94, total loss: 2.3562470241026445
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.8341, Acc: 0.7155, AUC: 0.7989, F1: 0.4937
[VIT] val - Epoch: 94, Loss: 0.9688, Acc: 0.5977, AUC: 0.8166, F1: 0.4716
epoch: 95, total loss: 2.0450603691014377
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.7819, Acc: 0.7213, AUC: 0.8550, F1: 0.5092
[VIT] val - Epoch: 95, Loss: 0.9622, Acc: 0.6207, AUC: 0.8208, F1: 0.5117
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8550
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8208
Saving vit model...
epoch: 96, total loss: 2.1623750870878045
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.7969, Acc: 0.7184, AUC: 0.8466, F1: 0.4908
[VIT] val - Epoch: 96, Loss: 0.9687, Acc: 0.5948, AUC: 0.8146, F1: 0.4744
epoch: 97, total loss: 2.20871058919213
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.7877, Acc: 0.7385, AUC: 0.8239, F1: 0.5230
[VIT] val - Epoch: 97, Loss: 0.9771, Acc: 0.6034, AUC: 0.8107, F1: 0.4823
epoch: 98, total loss: 2.3194911886345255
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.8129, Acc: 0.7098, AUC: 0.8091, F1: 0.4771
[VIT] val - Epoch: 98, Loss: 0.9688, Acc: 0.6178, AUC: 0.8165, F1: 0.5040
epoch: 99, total loss: 2.093953761187467
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.8761, Acc: 0.6408, AUC: 0.8504, F1: 0.4728
[VIT] val - Epoch: 99, Loss: 0.9622, Acc: 0.6236, AUC: 0.8175, F1: 0.5010
epoch: 100, total loss: 2.3465757153250952
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 1.0348, Acc: 0.6121, AUC: 0.8132, F1: 0.3897
[VIT] val - Epoch: 100, Loss: 0.9641, Acc: 0.6006, AUC: 0.8219, F1: 0.4736
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8219
Saving vit model...
[08:29:06][Rank 0] Training Finished. Starting Final Testing...
[08:29:06][Rank 2] Training Finished. Starting Final Testing...
[08:29:06][Rank 3] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
[08:29:06][Rank 1] Training Finished. Starting Final Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.0585, Acc: 0.6264, AUC: 0.7777, F1: 0.3682
[VIT] test - Epoch: 100, Loss: 0.9633, Acc: 0.6446, AUC: 0.7582, F1: 0.3654
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.0423, Acc: 0.6415, AUC: 0.7741, F1: 0.3868
[VIT] test - Epoch: 100, Loss: 1.0276, Acc: 0.6261, AUC: 0.7647, F1: 0.3827
‚úÖ [ÂÆåÊàê] Ê∫êÂüü MESSIDOR ËÆ≠ÁªÉÁªìÊùü„ÄÇ

----------------------------------------------------------------
‚ñ∂Ô∏è  [ËøõÂ∫¶] Ê≠£Âú®ÂêØÂä®Ê∫êÂüü: RLDR
----------------------------------------------------------------
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
0 iterations per epoch
We have 0 images in train set, 0 images in val set, and 0 images in test set.
Namespace(local_rank=0, time_limit=36000, output='./output_esdg_h100', source_domain='RLDR', device=device(type='cuda', index=0))
ALGORITHM: CASS_GDRNet
BACKBONE: resnet50
BATCH_SIZE: 16
DATASET:
  NUM_CLASSES: 5
  ROOT: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/GDR_Formatted_Data
  SOURCE_DOMAINS: ['RLDR']
  TARGET_DOMAINS: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
DG_MODE: ESDG
DROP_LAST: False
DROP_OUT: 0.0
EPOCHS: 100
GDRNET:
  BETA: 0.5
  DINOV3_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
  GAMMA: 1.0
  LORA_ALPHA: 16.0
  LORA_R: 8
  SCALING_FACTOR: 4.0
  TEMPERATURE: 0.01
LEARNING_RATE: 0.00045
LOG_STEP: 5
MODEL:
  PRETRAINED: True
  PRETRAINED_PATH: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
MOMENTUM: 0.9
OPTIM:
  NAME: 
OUTPUT_PATH: CASS_GDRNet_ESDG_RLDR
OUT_DIR: ./output_esdg_h100/RLDR
OVERRIDE: True
RANDOM: False
SEED: 42
TRANSFORM:
  AUGPROB: 0.5
  COLORJITTER_B: 1
  COLORJITTER_C: 1
  COLORJITTER_H: 0.05
  COLORJITTER_S: 1
  NAME: []
USE_CUDA: True
VAL_EPOCH: 1
VERBOSE: True
WEIGHT_DECAY: 0.0005
num_workers: 8
Distributed: True, Rank: 0
[INFO] Log Path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/Standard_Pipeline+MASK_SIAM+CASS+LoRA+Fusion_Modified2/output_esdg_h100/RLDR/CASS_GDRNet_ESDG_RLDR
[08:36:21][Rank 0] Loading datasets...
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
[08:36:21][Rank 2] Loading datasets...
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
================ [Auto Config] ================
Source: ['RLDR']
Targets: ['APTOS', 'DDR', 'DEEPDR', 'FGADR', 'IDRID', 'MESSIDOR', 'EYEPACS']
Output Dir: ./output_esdg_h100/RLDR
===============================================
[08:36:21][Rank 1] Loading datasets...
[08:36:21][Rank 3] Loading datasets...
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
========================================================
[ResNet] Ê≠£Âú®‰ªéÊú¨Âú∞Âä†ËΩΩÊùÉÈáç: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/resnet50-19c8e357.pth
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
[ResNet] ‚úÖ ÊùÉÈáçÂä†ËΩΩÊàêÂäüÔºÅ
[ResNet] Êú™Âä†ËΩΩÁöÑÂ±Ç (È¢ÑÊúüÂÜÖ): []
========================================================
üöÄ [DINOv3+LoRA] Loading from local path: /datasets/work/hb-nhmrc-dhcp/work/liu275/DGDR/checkpoints/dinov3_vitb16
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.2.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
Injecting LoRA into: layer.11.attention.o_proj
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.11.mlp.down_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
üßä [DINOv3] All base parameters frozen.
Injecting LoRA into: layer.0.attention.k_proj
Injecting LoRA into: layer.0.attention.v_proj
Injecting LoRA into: layer.0.attention.q_proj
Injecting LoRA into: layer.0.attention.o_proj
Injecting LoRA into: layer.0.mlp.up_proj
Injecting LoRA into: layer.0.mlp.down_proj
Injecting LoRA into: layer.1.attention.k_proj
Injecting LoRA into: layer.1.attention.v_proj
Injecting LoRA into: layer.1.attention.q_proj
Injecting LoRA into: layer.1.attention.o_proj
Injecting LoRA into: layer.1.mlp.up_proj
Injecting LoRA into: layer.1.mlp.down_proj
Injecting LoRA into: layer.2.attention.k_proj
Injecting LoRA into: layer.2.attention.v_proj
Injecting LoRA into: layer.2.attention.q_proj
Injecting LoRA into: layer.2.attention.o_proj
Injecting LoRA into: layer.2.mlp.up_proj
Injecting LoRA into: layer.2.mlp.down_proj
Injecting LoRA into: layer.3.attention.k_proj
Injecting LoRA into: layer.3.attention.v_proj
Injecting LoRA into: layer.3.attention.q_proj
Injecting LoRA into: layer.3.attention.o_proj
Injecting LoRA into: layer.3.mlp.up_proj
Injecting LoRA into: layer.3.mlp.down_proj
Injecting LoRA into: layer.4.attention.k_proj
Injecting LoRA into: layer.4.attention.v_proj
Injecting LoRA into: layer.4.attention.q_proj
Injecting LoRA into: layer.4.attention.o_proj
Injecting LoRA into: layer.4.mlp.up_proj
Injecting LoRA into: layer.4.mlp.down_proj
Injecting LoRA into: layer.5.attention.k_proj
Injecting LoRA into: layer.5.attention.v_proj
Injecting LoRA into: layer.5.attention.q_proj
Injecting LoRA into: layer.5.attention.o_proj
Injecting LoRA into: layer.5.mlp.up_proj
Injecting LoRA into: layer.5.mlp.down_proj
Injecting LoRA into: layer.6.attention.k_proj
Injecting LoRA into: layer.6.attention.v_proj
Injecting LoRA into: layer.6.attention.q_proj
Injecting LoRA into: layer.6.attention.o_proj
Injecting LoRA into: layer.6.mlp.up_proj
Injecting LoRA into: layer.6.mlp.down_proj
Injecting LoRA into: layer.7.attention.k_proj
Injecting LoRA into: layer.7.attention.v_proj
Injecting LoRA into: layer.7.attention.q_proj
Injecting LoRA into: layer.7.attention.o_proj
Injecting LoRA into: layer.7.mlp.up_proj
Injecting LoRA into: layer.7.mlp.down_proj
Injecting LoRA into: layer.8.attention.k_proj
Injecting LoRA into: layer.8.attention.v_proj
Injecting LoRA into: layer.8.attention.q_proj
Injecting LoRA into: layer.8.attention.o_proj
Injecting LoRA into: layer.8.mlp.up_proj
Injecting LoRA into: layer.8.mlp.down_proj
Injecting LoRA into: layer.9.attention.k_proj
Injecting LoRA into: layer.9.attention.v_proj
Injecting LoRA into: layer.9.attention.q_proj
Injecting LoRA into: layer.9.attention.o_proj
Injecting LoRA into: layer.9.mlp.up_proj
Injecting LoRA into: layer.9.mlp.down_proj
Injecting LoRA into: layer.10.attention.k_proj
Injecting LoRA into: layer.10.attention.v_proj
Injecting LoRA into: layer.10.attention.q_proj
Injecting LoRA into: layer.10.attention.o_proj
Injecting LoRA into: layer.10.mlp.up_proj
Injecting LoRA into: layer.10.mlp.down_proj
Injecting LoRA into: layer.11.attention.k_proj
Injecting LoRA into: layer.11.attention.v_proj
Injecting LoRA into: layer.11.attention.q_proj
Injecting LoRA into: layer.11.attention.o_proj
Injecting LoRA into: layer.11.mlp.up_proj
Injecting LoRA into: layer.11.mlp.down_proj
üî• [LoRA Injected] Trainable parameters: 144
‚úÖ ÊàêÂäüÊ≥®ÂÖ• 3 ‰∏™ HookÔºöÂ∑≤Á≤æÂáÜÊã¶Êà™ DINOv3 Á¨¨ [3, 6, 9] Â±ÇÁöÑ Attention Map„ÄÇ
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
Total Params: 123.25M, Trainable (LoRA+CNN): 37.59M
epoch: 1, total loss: 3.5318054497241973
Epoch 1 Validation...
[CNN] val - Epoch: 1, Loss: 9.7070, Acc: 0.5723, AUC: 0.5000, F1: 0.1456
[VIT] val - Epoch: 1, Loss: 1.2059, Acc: 0.5723, AUC: 0.5512, F1: 0.1456
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5000
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5512
Saving vit model...
epoch: 2, total loss: 3.1746255338191984
Epoch 2 Validation...
[CNN] val - Epoch: 2, Loss: 2.1889, Acc: 0.5723, AUC: 0.5360, F1: 0.1456
[VIT] val - Epoch: 2, Loss: 1.2748, Acc: 0.5629, AUC: 0.5808, F1: 0.1769
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5360
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.5808
Saving vit model...
epoch: 3, total loss: 3.2207018375396728
Epoch 3 Validation...
[CNN] val - Epoch: 3, Loss: 3.4398, Acc: 0.5063, AUC: 0.5501, F1: 0.1931
[VIT] val - Epoch: 3, Loss: 1.2401, Acc: 0.5881, AUC: 0.6136, F1: 0.1880
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5501
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6136
Saving vit model...
epoch: 4, total loss: 3.184795892238617
Epoch 4 Validation...
[CNN] val - Epoch: 4, Loss: 1.2198, Acc: 0.5723, AUC: 0.5574, F1: 0.1456
[VIT] val - Epoch: 4, Loss: 1.2362, Acc: 0.5881, AUC: 0.6550, F1: 0.2068
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.5574
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6550
Saving vit model...
epoch: 5, total loss: 2.942684441804886
Epoch 5 Validation...
[CNN] val - Epoch: 5, Loss: 1.2582, Acc: 0.5723, AUC: 0.5376, F1: 0.1459
[VIT] val - Epoch: 5, Loss: 1.2196, Acc: 0.5755, AUC: 0.6980, F1: 0.2003
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.6980
Saving vit model...
epoch: 6, total loss: 3.0632494568824766
Epoch 6 Validation...
[CNN] val - Epoch: 6, Loss: 1.2132, Acc: 0.5849, AUC: 0.6617, F1: 0.2090
[VIT] val - Epoch: 6, Loss: 1.2156, Acc: 0.5912, AUC: 0.6571, F1: 0.2533
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6617
Saving cnn model...
epoch: 7, total loss: 2.8019954562187195
Epoch 7 Validation...
[CNN] val - Epoch: 7, Loss: 1.2673, Acc: 0.5252, AUC: 0.6718, F1: 0.2440
[VIT] val - Epoch: 7, Loss: 1.1915, Acc: 0.6006, AUC: 0.7156, F1: 0.2274
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6718
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7156
Saving vit model...
epoch: 8, total loss: 2.983328342437744
Epoch 8 Validation...
[CNN] val - Epoch: 8, Loss: 1.2408, Acc: 0.4969, AUC: 0.6902, F1: 0.2487
[VIT] val - Epoch: 8, Loss: 1.2102, Acc: 0.5943, AUC: 0.6701, F1: 0.2497
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.6902
Saving cnn model...
epoch: 9, total loss: 3.0942198991775514
Epoch 9 Validation...
[CNN] val - Epoch: 9, Loss: 1.1508, Acc: 0.5629, AUC: 0.7010, F1: 0.2274
[VIT] val - Epoch: 9, Loss: 1.1909, Acc: 0.5912, AUC: 0.7312, F1: 0.2708
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7010
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7312
Saving vit model...
epoch: 10, total loss: 2.6770216047763826
Epoch 10 Validation...
[CNN] val - Epoch: 10, Loss: 1.3590, Acc: 0.5377, AUC: 0.6910, F1: 0.2392
[VIT] val - Epoch: 10, Loss: 1.1789, Acc: 0.5755, AUC: 0.7451, F1: 0.2867
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7451
Saving vit model...
epoch: 11, total loss: 2.727811813354492
Epoch 11 Validation...
[CNN] val - Epoch: 11, Loss: 1.4265, Acc: 0.5849, AUC: 0.7021, F1: 0.2736
[VIT] val - Epoch: 11, Loss: 1.1661, Acc: 0.5912, AUC: 0.7321, F1: 0.2696
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7021
Saving cnn model...
epoch: 12, total loss: 2.765576183795929
Epoch 12 Validation...
[CNN] val - Epoch: 12, Loss: 1.0301, Acc: 0.5849, AUC: 0.7653, F1: 0.2708
[VIT] val - Epoch: 12, Loss: 1.1682, Acc: 0.5849, AUC: 0.7266, F1: 0.2935
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7653
Saving cnn model...
epoch: 13, total loss: 2.5492894768714907
Epoch 13 Validation...
[CNN] val - Epoch: 13, Loss: 1.0388, Acc: 0.5881, AUC: 0.7539, F1: 0.2866
[VIT] val - Epoch: 13, Loss: 1.1593, Acc: 0.5629, AUC: 0.7429, F1: 0.2726
epoch: 14, total loss: 2.7133923053741453
Epoch 14 Validation...
[CNN] val - Epoch: 14, Loss: 5.2085, Acc: 0.3899, AUC: 0.6208, F1: 0.2663
[VIT] val - Epoch: 14, Loss: 1.1807, Acc: 0.5440, AUC: 0.7638, F1: 0.2850
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7638
Saving vit model...
epoch: 15, total loss: 2.6417823135852814
Epoch 15 Validation...
[CNN] val - Epoch: 15, Loss: 1.1806, Acc: 0.5283, AUC: 0.7444, F1: 0.2457
[VIT] val - Epoch: 15, Loss: 1.1503, Acc: 0.5912, AUC: 0.7694, F1: 0.2950
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7694
Saving vit model...
epoch: 16, total loss: 2.985103487968445
Epoch 16 Validation...
[CNN] val - Epoch: 16, Loss: 1.0951, Acc: 0.5943, AUC: 0.7485, F1: 0.2967
[VIT] val - Epoch: 16, Loss: 1.1558, Acc: 0.5786, AUC: 0.7420, F1: 0.2677
epoch: 17, total loss: 2.8801737666130065
Epoch 17 Validation...
[CNN] val - Epoch: 17, Loss: 1.1079, Acc: 0.5597, AUC: 0.7386, F1: 0.2566
[VIT] val - Epoch: 17, Loss: 1.1829, Acc: 0.5283, AUC: 0.7233, F1: 0.3035
epoch: 18, total loss: 2.8000323355197905
Epoch 18 Validation...
[CNN] val - Epoch: 18, Loss: 1.2064, Acc: 0.5472, AUC: 0.7317, F1: 0.3179
[VIT] val - Epoch: 18, Loss: 1.1368, Acc: 0.6069, AUC: 0.7466, F1: 0.3361
epoch: 19, total loss: 2.8335206270217896
Epoch 19 Validation...
[CNN] val - Epoch: 19, Loss: 1.5583, Acc: 0.5692, AUC: 0.7287, F1: 0.3033
[VIT] val - Epoch: 19, Loss: 1.1473, Acc: 0.5786, AUC: 0.7598, F1: 0.3136
epoch: 20, total loss: 2.73296959400177
Epoch 20 Validation...
[CNN] val - Epoch: 20, Loss: 1.3865, Acc: 0.4560, AUC: 0.7157, F1: 0.2746
[VIT] val - Epoch: 20, Loss: 1.1278, Acc: 0.5755, AUC: 0.7680, F1: 0.3136
epoch: 21, total loss: 2.6117392003536226
Epoch 21 Validation...
[CNN] val - Epoch: 21, Loss: 1.0215, Acc: 0.5692, AUC: 0.7808, F1: 0.3260
[VIT] val - Epoch: 21, Loss: 1.1197, Acc: 0.5597, AUC: 0.7774, F1: 0.3065
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7808
Saving cnn model...
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7774
Saving vit model...
epoch: 22, total loss: 2.8221845626831055
Epoch 22 Validation...
[CNN] val - Epoch: 22, Loss: 1.0377, Acc: 0.5786, AUC: 0.7708, F1: 0.3211
[VIT] val - Epoch: 22, Loss: 1.1339, Acc: 0.5786, AUC: 0.7773, F1: 0.3432
epoch: 23, total loss: 2.7995985984802245
Epoch 23 Validation...
[CNN] val - Epoch: 23, Loss: 1.0170, Acc: 0.5849, AUC: 0.7839, F1: 0.3576
[VIT] val - Epoch: 23, Loss: 1.1184, Acc: 0.5692, AUC: 0.7706, F1: 0.3187
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.7839
Saving cnn model...
epoch: 24, total loss: 2.780407130718231
Epoch 24 Validation...
[CNN] val - Epoch: 24, Loss: 1.5356, Acc: 0.5346, AUC: 0.7252, F1: 0.3027
[VIT] val - Epoch: 24, Loss: 1.1387, Acc: 0.5818, AUC: 0.7856, F1: 0.3753
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7856
Saving vit model...
epoch: 25, total loss: 2.802813637256622
Epoch 25 Validation...
[CNN] val - Epoch: 25, Loss: 1.0680, Acc: 0.6101, AUC: 0.7572, F1: 0.3574
[VIT] val - Epoch: 25, Loss: 1.1061, Acc: 0.5943, AUC: 0.7635, F1: 0.3646
epoch: 26, total loss: 2.4611762344837187
Epoch 26 Validation...
[CNN] val - Epoch: 26, Loss: 1.0904, Acc: 0.5629, AUC: 0.7742, F1: 0.3012
[VIT] val - Epoch: 26, Loss: 1.1105, Acc: 0.6038, AUC: 0.7892, F1: 0.4174
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7892
Saving vit model...
epoch: 27, total loss: 2.7102465450763704
Epoch 27 Validation...
[CNN] val - Epoch: 27, Loss: 2.0180, Acc: 0.5881, AUC: 0.7583, F1: 0.3922
[VIT] val - Epoch: 27, Loss: 1.1047, Acc: 0.5597, AUC: 0.7979, F1: 0.3592
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.7979
Saving vit model...
epoch: 28, total loss: 2.8523258566856384
Epoch 28 Validation...
[CNN] val - Epoch: 28, Loss: 1.0177, Acc: 0.5818, AUC: 0.8009, F1: 0.3790
[VIT] val - Epoch: 28, Loss: 1.1264, Acc: 0.6069, AUC: 0.7811, F1: 0.4246
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8009
Saving cnn model...
epoch: 29, total loss: 2.6366610765457152
Epoch 29 Validation...
[CNN] val - Epoch: 29, Loss: 0.9817, Acc: 0.5912, AUC: 0.7908, F1: 0.3764
[VIT] val - Epoch: 29, Loss: 1.0986, Acc: 0.6132, AUC: 0.7743, F1: 0.4181
epoch: 30, total loss: 2.599637120962143
Epoch 30 Validation...
[CNN] val - Epoch: 30, Loss: 1.0236, Acc: 0.5755, AUC: 0.8008, F1: 0.3684
[VIT] val - Epoch: 30, Loss: 1.0878, Acc: 0.5943, AUC: 0.8050, F1: 0.3870
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8050
Saving vit model...
epoch: 31, total loss: 2.6010310351848602
Epoch 31 Validation...
[CNN] val - Epoch: 31, Loss: 1.1057, Acc: 0.5346, AUC: 0.7904, F1: 0.3389
[VIT] val - Epoch: 31, Loss: 1.1025, Acc: 0.5881, AUC: 0.8051, F1: 0.3959
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8051
Saving vit model...
epoch: 32, total loss: 2.799921399354935
Epoch 32 Validation...
[CNN] val - Epoch: 32, Loss: 1.2512, Acc: 0.5503, AUC: 0.7030, F1: 0.3210
[VIT] val - Epoch: 32, Loss: 1.0872, Acc: 0.5597, AUC: 0.7864, F1: 0.3475
epoch: 33, total loss: 2.621235501766205
Epoch 33 Validation...
[CNN] val - Epoch: 33, Loss: 1.0366, Acc: 0.6226, AUC: 0.7863, F1: 0.3811
[VIT] val - Epoch: 33, Loss: 1.0882, Acc: 0.5755, AUC: 0.7886, F1: 0.3693
epoch: 34, total loss: 2.5650332629680634
Epoch 34 Validation...
[CNN] val - Epoch: 34, Loss: 0.9853, Acc: 0.6038, AUC: 0.8021, F1: 0.3530
[VIT] val - Epoch: 34, Loss: 1.1040, Acc: 0.6006, AUC: 0.7752, F1: 0.4354
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8021
Saving cnn model...
epoch: 35, total loss: 2.5228072941303252
Epoch 35 Validation...
[CNN] val - Epoch: 35, Loss: 1.0046, Acc: 0.5912, AUC: 0.7937, F1: 0.3817
[VIT] val - Epoch: 35, Loss: 1.0982, Acc: 0.5881, AUC: 0.7914, F1: 0.4084
epoch: 36, total loss: 2.796381002664566
Epoch 36 Validation...
[CNN] val - Epoch: 36, Loss: 1.0229, Acc: 0.5912, AUC: 0.7911, F1: 0.4117
[VIT] val - Epoch: 36, Loss: 1.1001, Acc: 0.5723, AUC: 0.8000, F1: 0.3425
epoch: 37, total loss: 2.6168804824352265
Epoch 37 Validation...
[CNN] val - Epoch: 37, Loss: 1.0531, Acc: 0.5597, AUC: 0.7805, F1: 0.3418
[VIT] val - Epoch: 37, Loss: 1.0984, Acc: 0.5755, AUC: 0.7799, F1: 0.3765
epoch: 38, total loss: 2.5688047051429748
Epoch 38 Validation...
[CNN] val - Epoch: 38, Loss: 1.4352, Acc: 0.5629, AUC: 0.7252, F1: 0.3353
[VIT] val - Epoch: 38, Loss: 1.1152, Acc: 0.5818, AUC: 0.7892, F1: 0.4085
epoch: 39, total loss: 2.763832741975784
Epoch 39 Validation...
[CNN] val - Epoch: 39, Loss: 1.4263, Acc: 0.5409, AUC: 0.7275, F1: 0.3224
[VIT] val - Epoch: 39, Loss: 1.1190, Acc: 0.5849, AUC: 0.7864, F1: 0.4155
epoch: 40, total loss: 2.512841504812241
Epoch 40 Validation...
[CNN] val - Epoch: 40, Loss: 1.0617, Acc: 0.5660, AUC: 0.8205, F1: 0.3959
[VIT] val - Epoch: 40, Loss: 1.0893, Acc: 0.5786, AUC: 0.7989, F1: 0.4104
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8205
Saving cnn model...
epoch: 41, total loss: 2.5260011196136474
Epoch 41 Validation...
[CNN] val - Epoch: 41, Loss: 1.0881, Acc: 0.5818, AUC: 0.7963, F1: 0.4158
[VIT] val - Epoch: 41, Loss: 1.0838, Acc: 0.5943, AUC: 0.7935, F1: 0.4068
epoch: 42, total loss: 2.6816445350646974
Epoch 42 Validation...
[CNN] val - Epoch: 42, Loss: 1.2353, Acc: 0.5409, AUC: 0.7641, F1: 0.3635
[VIT] val - Epoch: 42, Loss: 1.0784, Acc: 0.5881, AUC: 0.7952, F1: 0.3931
epoch: 43, total loss: 2.4053686916828156
Epoch 43 Validation...
[CNN] val - Epoch: 43, Loss: 1.1007, Acc: 0.5503, AUC: 0.7693, F1: 0.3499
[VIT] val - Epoch: 43, Loss: 1.0679, Acc: 0.5849, AUC: 0.8078, F1: 0.3483
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8078
Saving vit model...
epoch: 44, total loss: 2.4309872329235076
Epoch 44 Validation...
[CNN] val - Epoch: 44, Loss: 1.0532, Acc: 0.5472, AUC: 0.8117, F1: 0.3782
[VIT] val - Epoch: 44, Loss: 1.0998, Acc: 0.5692, AUC: 0.8042, F1: 0.3579
epoch: 45, total loss: 2.446005016565323
Epoch 45 Validation...
[CNN] val - Epoch: 45, Loss: 0.9827, Acc: 0.6038, AUC: 0.8056, F1: 0.3972
[VIT] val - Epoch: 45, Loss: 1.0783, Acc: 0.5849, AUC: 0.8056, F1: 0.3793
epoch: 46, total loss: 2.334921735525131
Epoch 46 Validation...
[CNN] val - Epoch: 46, Loss: 0.9468, Acc: 0.6258, AUC: 0.8068, F1: 0.4297
[VIT] val - Epoch: 46, Loss: 1.0696, Acc: 0.6038, AUC: 0.8000, F1: 0.4123
epoch: 47, total loss: 2.744247007369995
Epoch 47 Validation...
[CNN] val - Epoch: 47, Loss: 1.0825, Acc: 0.5660, AUC: 0.7923, F1: 0.3987
[VIT] val - Epoch: 47, Loss: 1.0955, Acc: 0.5881, AUC: 0.7845, F1: 0.4243
epoch: 48, total loss: 2.5194768369197846
Epoch 48 Validation...
[CNN] val - Epoch: 48, Loss: 1.0268, Acc: 0.5660, AUC: 0.8038, F1: 0.3415
[VIT] val - Epoch: 48, Loss: 1.0844, Acc: 0.5849, AUC: 0.8072, F1: 0.3898
epoch: 49, total loss: 2.4967727422714234
Epoch 49 Validation...
[CNN] val - Epoch: 49, Loss: 1.1301, Acc: 0.5346, AUC: 0.7511, F1: 0.3321
[VIT] val - Epoch: 49, Loss: 1.0742, Acc: 0.5975, AUC: 0.7995, F1: 0.4250
epoch: 50, total loss: 2.530237430334091
Epoch 50 Validation...
[CNN] val - Epoch: 50, Loss: 0.9927, Acc: 0.5975, AUC: 0.7808, F1: 0.3953
[VIT] val - Epoch: 50, Loss: 1.0751, Acc: 0.5786, AUC: 0.8077, F1: 0.3995
epoch: 51, total loss: 2.8287226617336274
Epoch 51 Validation...
[CNN] val - Epoch: 51, Loss: 0.9779, Acc: 0.6038, AUC: 0.8002, F1: 0.4198
[VIT] val - Epoch: 51, Loss: 1.0753, Acc: 0.5975, AUC: 0.7914, F1: 0.4023
epoch: 52, total loss: 2.764374577999115
Epoch 52 Validation...
[CNN] val - Epoch: 52, Loss: 1.0275, Acc: 0.5660, AUC: 0.7981, F1: 0.3850
[VIT] val - Epoch: 52, Loss: 1.0757, Acc: 0.5849, AUC: 0.7980, F1: 0.3738
epoch: 53, total loss: 2.335138255357742
Epoch 53 Validation...
[CNN] val - Epoch: 53, Loss: 1.0341, Acc: 0.5660, AUC: 0.8081, F1: 0.4035
[VIT] val - Epoch: 53, Loss: 1.0715, Acc: 0.6006, AUC: 0.7941, F1: 0.4226
epoch: 54, total loss: 2.52217099070549
Epoch 54 Validation...
[CNN] val - Epoch: 54, Loss: 1.1323, Acc: 0.5975, AUC: 0.7745, F1: 0.3939
[VIT] val - Epoch: 54, Loss: 1.0894, Acc: 0.5881, AUC: 0.7916, F1: 0.4167
epoch: 55, total loss: 2.3807738363742827
Epoch 55 Validation...
[CNN] val - Epoch: 55, Loss: 0.9479, Acc: 0.6195, AUC: 0.8392, F1: 0.4073
[VIT] val - Epoch: 55, Loss: 1.0602, Acc: 0.5818, AUC: 0.8038, F1: 0.3986
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8392
Saving cnn model...
epoch: 56, total loss: 2.721446007490158
Epoch 56 Validation...
[CNN] val - Epoch: 56, Loss: 0.9347, Acc: 0.6258, AUC: 0.8310, F1: 0.4368
[VIT] val - Epoch: 56, Loss: 1.0573, Acc: 0.5786, AUC: 0.8132, F1: 0.4081
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8132
Saving vit model...
epoch: 57, total loss: 2.267060339450836
Epoch 57 Validation...
[CNN] val - Epoch: 57, Loss: 1.2414, Acc: 0.5346, AUC: 0.7901, F1: 0.3898
[VIT] val - Epoch: 57, Loss: 1.0537, Acc: 0.5912, AUC: 0.8058, F1: 0.3993
epoch: 58, total loss: 2.363886445760727
Epoch 58 Validation...
[CNN] val - Epoch: 58, Loss: 0.9981, Acc: 0.6132, AUC: 0.8135, F1: 0.4275
[VIT] val - Epoch: 58, Loss: 1.0682, Acc: 0.5849, AUC: 0.8044, F1: 0.3833
epoch: 59, total loss: 2.313831490278244
Epoch 59 Validation...
[CNN] val - Epoch: 59, Loss: 1.0145, Acc: 0.6006, AUC: 0.8116, F1: 0.4346
[VIT] val - Epoch: 59, Loss: 1.0826, Acc: 0.5786, AUC: 0.7957, F1: 0.3841
epoch: 60, total loss: 2.224601846933365
Epoch 60 Validation...
[CNN] val - Epoch: 60, Loss: 0.9680, Acc: 0.5881, AUC: 0.8257, F1: 0.3832
[VIT] val - Epoch: 60, Loss: 1.0588, Acc: 0.5912, AUC: 0.8212, F1: 0.3865
‚≠êÔ∏è [ViT] New Best! Val AUC: 0.8212
Saving vit model...
epoch: 61, total loss: 2.2940430760383608
Epoch 61 Validation...
[CNN] val - Epoch: 61, Loss: 1.0090, Acc: 0.5881, AUC: 0.8154, F1: 0.4111
[VIT] val - Epoch: 61, Loss: 1.0667, Acc: 0.5849, AUC: 0.8156, F1: 0.4022
epoch: 62, total loss: 2.4648850679397585
Epoch 62 Validation...
[CNN] val - Epoch: 62, Loss: 0.9285, Acc: 0.6289, AUC: 0.8324, F1: 0.4519
[VIT] val - Epoch: 62, Loss: 1.0523, Acc: 0.5943, AUC: 0.8065, F1: 0.4337
epoch: 63, total loss: 2.380696052312851
Epoch 63 Validation...
[CNN] val - Epoch: 63, Loss: 0.9321, Acc: 0.6321, AUC: 0.8386, F1: 0.4517
[VIT] val - Epoch: 63, Loss: 1.0536, Acc: 0.5943, AUC: 0.8067, F1: 0.4414
epoch: 64, total loss: 2.2991261541843415
Epoch 64 Validation...
[CNN] val - Epoch: 64, Loss: 0.9348, Acc: 0.6101, AUC: 0.8336, F1: 0.3704
[VIT] val - Epoch: 64, Loss: 1.0564, Acc: 0.6132, AUC: 0.8121, F1: 0.4468
epoch: 65, total loss: 2.3406165063381197
Epoch 65 Validation...
[CNN] val - Epoch: 65, Loss: 0.9481, Acc: 0.6069, AUC: 0.8395, F1: 0.3977
[VIT] val - Epoch: 65, Loss: 1.0694, Acc: 0.5881, AUC: 0.8151, F1: 0.4191
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8395
Saving cnn model...
epoch: 66, total loss: 2.1746088564395905
Epoch 66 Validation...
[CNN] val - Epoch: 66, Loss: 0.9048, Acc: 0.6289, AUC: 0.8402, F1: 0.4533
[VIT] val - Epoch: 66, Loss: 1.0532, Acc: 0.5912, AUC: 0.8127, F1: 0.4249
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8402
Saving cnn model...
epoch: 67, total loss: 2.510596638917923
Epoch 67 Validation...
[CNN] val - Epoch: 67, Loss: 0.9805, Acc: 0.6006, AUC: 0.8325, F1: 0.4145
[VIT] val - Epoch: 67, Loss: 1.0623, Acc: 0.6038, AUC: 0.8102, F1: 0.4388
epoch: 68, total loss: 2.345859032869339
Epoch 68 Validation...
[CNN] val - Epoch: 68, Loss: 0.9714, Acc: 0.6164, AUC: 0.8197, F1: 0.4126
[VIT] val - Epoch: 68, Loss: 1.0662, Acc: 0.5786, AUC: 0.7993, F1: 0.4074
epoch: 69, total loss: 2.4387851655483246
Epoch 69 Validation...
[CNN] val - Epoch: 69, Loss: 1.0382, Acc: 0.5629, AUC: 0.7896, F1: 0.3712
[VIT] val - Epoch: 69, Loss: 1.0702, Acc: 0.5943, AUC: 0.7941, F1: 0.4193
epoch: 70, total loss: 2.4499245047569276
Epoch 70 Validation...
[CNN] val - Epoch: 70, Loss: 0.9006, Acc: 0.6289, AUC: 0.8458, F1: 0.4064
[VIT] val - Epoch: 70, Loss: 1.0541, Acc: 0.5818, AUC: 0.8111, F1: 0.3840
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8458
Saving cnn model...
epoch: 71, total loss: 2.68933385014534
Epoch 71 Validation...
[CNN] val - Epoch: 71, Loss: 0.9084, Acc: 0.6415, AUC: 0.8316, F1: 0.4365
[VIT] val - Epoch: 71, Loss: 1.0593, Acc: 0.5943, AUC: 0.8017, F1: 0.4165
epoch: 72, total loss: 2.41960808634758
Epoch 72 Validation...
[CNN] val - Epoch: 72, Loss: 0.9254, Acc: 0.6321, AUC: 0.8440, F1: 0.4262
[VIT] val - Epoch: 72, Loss: 1.0492, Acc: 0.5849, AUC: 0.8063, F1: 0.3574
epoch: 73, total loss: 2.4177570760250093
Epoch 73 Validation...
[CNN] val - Epoch: 73, Loss: 0.9105, Acc: 0.6195, AUC: 0.8505, F1: 0.4408
[VIT] val - Epoch: 73, Loss: 1.0456, Acc: 0.5818, AUC: 0.8103, F1: 0.3941
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8505
Saving cnn model...
epoch: 74, total loss: 2.2559230089187623
Epoch 74 Validation...
[CNN] val - Epoch: 74, Loss: 0.9177, Acc: 0.6226, AUC: 0.8347, F1: 0.4075
[VIT] val - Epoch: 74, Loss: 1.0509, Acc: 0.6006, AUC: 0.8092, F1: 0.4346
epoch: 75, total loss: 2.3712162733078004
Epoch 75 Validation...
[CNN] val - Epoch: 75, Loss: 0.9264, Acc: 0.6164, AUC: 0.8331, F1: 0.3897
[VIT] val - Epoch: 75, Loss: 1.0514, Acc: 0.6038, AUC: 0.8028, F1: 0.4649
epoch: 76, total loss: 2.5268304467201235
Epoch 76 Validation...
[CNN] val - Epoch: 76, Loss: 0.9167, Acc: 0.6289, AUC: 0.8446, F1: 0.4203
[VIT] val - Epoch: 76, Loss: 1.0316, Acc: 0.6006, AUC: 0.8088, F1: 0.4304
epoch: 77, total loss: 2.5992873430252077
Epoch 77 Validation...
[CNN] val - Epoch: 77, Loss: 0.9392, Acc: 0.6069, AUC: 0.8290, F1: 0.4054
[VIT] val - Epoch: 77, Loss: 1.0547, Acc: 0.5912, AUC: 0.8038, F1: 0.4295
epoch: 78, total loss: 2.531710410118103
Epoch 78 Validation...
[CNN] val - Epoch: 78, Loss: 0.9094, Acc: 0.6258, AUC: 0.8431, F1: 0.4473
[VIT] val - Epoch: 78, Loss: 1.0439, Acc: 0.6101, AUC: 0.8072, F1: 0.4614
epoch: 79, total loss: 2.4146970093250273
Epoch 79 Validation...
[CNN] val - Epoch: 79, Loss: 0.9061, Acc: 0.6258, AUC: 0.8434, F1: 0.4154
[VIT] val - Epoch: 79, Loss: 1.0468, Acc: 0.5912, AUC: 0.8081, F1: 0.4165
epoch: 80, total loss: 2.4710805773735047
Epoch 80 Validation...
[CNN] val - Epoch: 80, Loss: 0.9757, Acc: 0.6069, AUC: 0.8296, F1: 0.4010
[VIT] val - Epoch: 80, Loss: 1.0483, Acc: 0.5881, AUC: 0.8018, F1: 0.4108
epoch: 81, total loss: 2.333494371175766
Epoch 81 Validation...
[CNN] val - Epoch: 81, Loss: 0.8934, Acc: 0.6415, AUC: 0.8418, F1: 0.4148
[VIT] val - Epoch: 81, Loss: 1.0484, Acc: 0.6038, AUC: 0.8056, F1: 0.4325
epoch: 82, total loss: 2.3012226998806
Epoch 82 Validation...
[CNN] val - Epoch: 82, Loss: 0.9311, Acc: 0.6226, AUC: 0.8124, F1: 0.4352
[VIT] val - Epoch: 82, Loss: 1.0552, Acc: 0.5912, AUC: 0.8128, F1: 0.4361
epoch: 83, total loss: 2.384924900531769
Epoch 83 Validation...
[CNN] val - Epoch: 83, Loss: 0.9223, Acc: 0.6132, AUC: 0.8456, F1: 0.4153
[VIT] val - Epoch: 83, Loss: 1.0393, Acc: 0.5975, AUC: 0.8200, F1: 0.4444
epoch: 84, total loss: 2.364794892072678
Epoch 84 Validation...
[CNN] val - Epoch: 84, Loss: 0.9204, Acc: 0.6352, AUC: 0.8474, F1: 0.4414
[VIT] val - Epoch: 84, Loss: 1.0352, Acc: 0.5975, AUC: 0.8133, F1: 0.4390
epoch: 85, total loss: 2.267416387796402
Epoch 85 Validation...
[CNN] val - Epoch: 85, Loss: 0.9285, Acc: 0.6226, AUC: 0.8375, F1: 0.4103
[VIT] val - Epoch: 85, Loss: 1.0335, Acc: 0.6006, AUC: 0.8149, F1: 0.4356
epoch: 86, total loss: 2.4516184091567994
Epoch 86 Validation...
[CNN] val - Epoch: 86, Loss: 0.9075, Acc: 0.6384, AUC: 0.8465, F1: 0.4569
[VIT] val - Epoch: 86, Loss: 1.0452, Acc: 0.6038, AUC: 0.8120, F1: 0.4562
epoch: 87, total loss: 2.226165509223938
Epoch 87 Validation...
[CNN] val - Epoch: 87, Loss: 0.9727, Acc: 0.6038, AUC: 0.8197, F1: 0.4003
[VIT] val - Epoch: 87, Loss: 1.0399, Acc: 0.6038, AUC: 0.8040, F1: 0.4362
epoch: 88, total loss: 2.237986320257187
Epoch 88 Validation...
[CNN] val - Epoch: 88, Loss: 0.9697, Acc: 0.6132, AUC: 0.8126, F1: 0.4050
[VIT] val - Epoch: 88, Loss: 1.0388, Acc: 0.5975, AUC: 0.8117, F1: 0.4231
epoch: 89, total loss: 2.5961470186710356
Epoch 89 Validation...
[CNN] val - Epoch: 89, Loss: 0.8746, Acc: 0.6447, AUC: 0.8469, F1: 0.4696
[VIT] val - Epoch: 89, Loss: 1.0579, Acc: 0.5975, AUC: 0.8086, F1: 0.4421
epoch: 90, total loss: 2.490137654542923
Epoch 90 Validation...
[CNN] val - Epoch: 90, Loss: 0.8444, Acc: 0.6478, AUC: 0.8621, F1: 0.4339
[VIT] val - Epoch: 90, Loss: 1.0415, Acc: 0.6006, AUC: 0.8049, F1: 0.4390
‚≠êÔ∏è [CNN] New Best! Val AUC: 0.8621
Saving cnn model...
epoch: 91, total loss: 2.314674288034439
Epoch 91 Validation...
[CNN] val - Epoch: 91, Loss: 0.8815, Acc: 0.6101, AUC: 0.8588, F1: 0.4110
[VIT] val - Epoch: 91, Loss: 1.0477, Acc: 0.5975, AUC: 0.8099, F1: 0.4469
epoch: 92, total loss: 2.4357842922210695
Epoch 92 Validation...
[CNN] val - Epoch: 92, Loss: 0.8752, Acc: 0.6415, AUC: 0.8536, F1: 0.4563
[VIT] val - Epoch: 92, Loss: 1.0430, Acc: 0.5849, AUC: 0.8143, F1: 0.4354
epoch: 93, total loss: 2.320930951833725
Epoch 93 Validation...
[CNN] val - Epoch: 93, Loss: 0.8474, Acc: 0.6541, AUC: 0.8540, F1: 0.4565
[VIT] val - Epoch: 93, Loss: 1.0355, Acc: 0.6006, AUC: 0.8200, F1: 0.4389
epoch: 94, total loss: 2.4639900028705597
Epoch 94 Validation...
[CNN] val - Epoch: 94, Loss: 0.8934, Acc: 0.6006, AUC: 0.8446, F1: 0.4131
[VIT] val - Epoch: 94, Loss: 1.0501, Acc: 0.5881, AUC: 0.8121, F1: 0.4325
epoch: 95, total loss: 2.3448949635028837
Epoch 95 Validation...
[CNN] val - Epoch: 95, Loss: 0.9250, Acc: 0.6478, AUC: 0.8388, F1: 0.4580
[VIT] val - Epoch: 95, Loss: 1.0432, Acc: 0.5881, AUC: 0.8071, F1: 0.4287
epoch: 96, total loss: 2.294061726331711
Epoch 96 Validation...
[CNN] val - Epoch: 96, Loss: 0.8610, Acc: 0.6321, AUC: 0.8547, F1: 0.4423
[VIT] val - Epoch: 96, Loss: 1.0406, Acc: 0.6006, AUC: 0.8044, F1: 0.4351
epoch: 97, total loss: 2.279782462120056
Epoch 97 Validation...
[CNN] val - Epoch: 97, Loss: 0.9250, Acc: 0.6321, AUC: 0.8348, F1: 0.4119
[VIT] val - Epoch: 97, Loss: 1.0396, Acc: 0.6101, AUC: 0.8129, F1: 0.4516
epoch: 98, total loss: 2.171737140417099
Epoch 98 Validation...
[CNN] val - Epoch: 98, Loss: 0.9124, Acc: 0.6258, AUC: 0.8436, F1: 0.4151
[VIT] val - Epoch: 98, Loss: 1.0399, Acc: 0.5975, AUC: 0.8154, F1: 0.4410
epoch: 99, total loss: 2.485764855146408
Epoch 99 Validation...
[CNN] val - Epoch: 99, Loss: 0.8976, Acc: 0.6258, AUC: 0.8428, F1: 0.4135
[VIT] val - Epoch: 99, Loss: 1.0397, Acc: 0.5912, AUC: 0.8144, F1: 0.4338
epoch: 100, total loss: 2.293922036886215
Epoch 100 Validation...
[CNN] val - Epoch: 100, Loss: 0.9497, Acc: 0.5943, AUC: 0.8204, F1: 0.3898
[VIT] val - Epoch: 100, Loss: 1.0466, Acc: 0.5912, AUC: 0.8120, F1: 0.4370
[08:51:19][Rank 1] Training Finished. Starting Final Testing...[08:51:19][Rank 3] Training Finished. Starting Final Testing...

[08:51:19][Rank 0] Training Finished. Starting Final Testing...
[08:51:19][Rank 2] Training Finished. Starting Final Testing...
üîÑ Loading Best CNN Model for Testing...
‚úÖ Model renewed from best_model_cnn.pth
[CNN] test - Epoch: 100, Loss: 1.2312, Acc: 0.2089, AUC: 0.8086, F1: 0.3090
[VIT] test - Epoch: 100, Loss: 1.3721, Acc: 0.2576, AUC: 0.7919, F1: 0.3012
üîÑ Loading Best VIT Model for Testing...
‚úÖ Model renewed from best_model_vit.pth
[CNN] test - Epoch: 100, Loss: 1.6937, Acc: 0.2308, AUC: 0.7910, F1: 0.2525
[VIT] test - Epoch: 100, Loss: 1.5922, Acc: 0.2285, AUC: 0.7873, F1: 0.2495
‚úÖ [ÂÆåÊàê] Ê∫êÂüü RLDR ËÆ≠ÁªÉÁªìÊùü„ÄÇ

########################################################
üìä ÊúÄÁªàÁªìÊûúÊ±áÊÄª (Running collect_results.py)
########################################################

----------------------------------------------------------------------
Domain           | Test AUC   | Test Acc   | Test F1   
----------------------------------------------------------------------
APTOS (CNN)      | 0.7180     | 0.6018     | 0.3843
APTOS (ViT)      | 0.7072     | 0.6508     | 0.3527
DEEPDR (CNN)     | 0.8056     | 0.3036     | 0.3276
DEEPDR (ViT)     | 0.7604     | 0.3982     | 0.3397
FGADR (CNN)      | 0.6914     | 0.0839     | 0.0923
FGADR (ViT)      | 0.7202     | 0.2017     | 0.1337
IDRID (CNN)      | 0.7911     | 0.6275     | 0.3990
IDRID (ViT)      | 0.7301     | 0.6399     | 0.3469
MESSIDOR (CNN)   | 0.7777     | 0.6264     | 0.3682
MESSIDOR (ViT)   | 0.7647     | 0.6261     | 0.3827
RLDR (CNN)       | 0.8086     | 0.2089     | 0.3090
RLDR (ViT)       | 0.7873     | 0.2285     | 0.2495
----------------------------------------------------------------------
AVERAGE          | 0.7552     | 0.4331     | 0.3071
----------------------------------------------------------------------

üìù Ê±áÊÄªÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: ./output_esdg_h100/final_summary.txt
========================================================
üéâ ÊâÄÊúâ‰ªªÂä°ÊâßË°åÂÆåÊØï
========================================================
Python script finished. Checking checkpoints for status...
Checking directory: ./output_esdg_h100/GDRNet_ESDG_MESSIDOR
‚ùå Neither final nor latest checkpoint found.
   This implies the job failed before the first epoch or path is wrong.
